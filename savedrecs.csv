Publication Type,Authors,Book Authors,Book Editors,Book Group Authors,Author Full Names,Book Author Full Names,Group Authors,Article Title,Source Title,Book Series Title,Book Series Subtitle,Language,Document Type,Conference Title,Conference Date,Conference Location,Conference Sponsor,Conference Host,Author Keywords,Keywords Plus,Abstract,Addresses,Affiliations,Reprint Addresses,Email Addresses,Researcher Ids,ORCIDs,Funding Orgs,Funding Name Preferred,Funding Text,Cited References,Cited Reference Count,"Times Cited, WoS Core","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,Publisher,Publisher City,Publisher Address,ISSN,eISSN,ISBN,Journal Abbreviation,Journal ISO Abbreviation,Publication Date,Publication Year,Volume,Issue,Part Number,Supplement,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,Book DOI,Early Access Date,Number of Pages,WoS Categories,Web of Science Index,Research Areas,IDS Number,Pubmed Id,Open Access Designations,Highly Cited Status,Hot Paper Status,Date of Export,UT (Unique WOS ID)
J,"Bai, L; Cui, LX; Jiao, YH; Rossi, L; Hancock, ER",,,,"Bai, Lu; Cui, Lixin; Jiao, Yuhang; Rossi, Luca; Hancock, Edwin R.",,,Learning Backtrackless Aligned-Spatial Graph Convolutional Networks for Graph Classification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Convolution; Adaptation models; Transforms; Convolutional neural networks; Standards; Feature extraction; Kernel; Graph convolutional networks; transitive vertex alignment; backtrackless walk,,"In this paper, we develop a novel backtrackless aligned-spatial graph convolutional network (BASGCN) model to learn effective features for graph classification. Our idea is to transform arbitrary-sized graphs into fixed-sized backtrackless aligned grid structures and define a new spatial graph convolution operation associated with the grid structures. We show that the proposed BASGCN model not only reduces the problems of information loss and imprecise information representation arising in existing spatially-based graph convolutional network (GCN) models, but also bridges the theoretical gap between traditional convolutional neural network (CNN) models and spatially-based GCN models. Furthermore, the proposed BASGCN model can both adaptively discriminate the importance between specified vertices during the convolution process and reduce the notorious tottering problem of existing spatially-based GCNs related to the Weisfeiler-Lehman algorithm, explaining the effectiveness of the proposed model. Experiments on standard graph datasets demonstrate the effectiveness of the proposed model.","[Bai, Lu; Jiao, Yuhang] Cent Univ Finance & Econ, Beijing 100081, Peoples R China; [Cui, Lixin] Cent Univ Finance & Econ, Sch Informat, Beijing 100081, Peoples R China; [Rossi, Luca] Queen Mary Univ London, London E1 4NS, England; [Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England",,"Cui, LX (通讯作者)，Cent Univ Finance & Econ, Sch Informat, Beijing 100081, Peoples R China.",bailucs@cufe.edu.cn; cuilixin@cufe.edu.cn; jiaoyuhang@email.cufe.edu.cn; luca.rossi@qmul.ac.uk; edwin.hancock@york.ac.uk,"Rossi, Luca/V-1459-2018; Hancock, Edwin/C-6071-2008","Rossi, Luca/0000-0002-6116-9761; Hancock, Edwin/0000-0003-4496-2028","National Natural Science Foundation of China [61976235, 61602535]; program for innovation research in Central University of Finance and Economics; Youth Talent Development Support Program by Central University of Finance and Economics [QYP1908]",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); program for innovation research in Central University of Finance and Economics; Youth Talent Development Support Program by Central University of Finance and Economics,"This work was supported by the National Natural Science Foundation of China (Grants 61976235 and 61602535), the program for innovation research in Central University of Finance and Economics, and the Youth Talent Development Support Program by Central University of Finance and Economics, No. QYP1908. The third author Mr. Yuhang Jiao mainly participated partial coding and experiment works.",,52,16,16,16,16,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,783,798,,10.1109/TPAMI.2020.3011866,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32750832,"Green Accepted, Green Published, Green Submitted",Y,N,2022-05-23,WOS:000740006100019
J,"Bolya, D; Zhou, C; Xiao, FY; Lee, YJ",,,,"Bolya, Daniel; Zhou, Chong; Xiao, Fanyi; Lee, Yong Jae",,,YOLACT plus plus Better Real-Time Instance Segmentation,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Instance segmentation; real time,,"We present a simple, fully-convolutional model for real-time ( > 30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.","[Bolya, Daniel] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Zhou, Chong; Xiao, Fanyi; Lee, Yong Jae] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA",,"Zhou, C (通讯作者)，Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.",dbolya3@gatech.edu; cczhou@ucdavis.edu; fyxiao@ucdavis.edu; yongjaeleel@ucdavis.edu,,"Zhou, Chong/0000-0002-9776-7739; Bolya, Daniel/0000-0003-0223-3599; Xiao, Fanyi/0000-0002-9839-1139",ARO YIP [W911NF17-1-0410]; NSF CAREER [IIS-1751206]; NSF [IIS-1812850]; AWS ML Research Award; Google Cloud Platform research credits; XSEDE [IRI180001],ARO YIP; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); AWS ML Research Award; Google Cloud Platform research credits(Google Incorporated); XSEDE,"This work was supported in part by ARO YIP W911NF17-1-0410, NSF CAREER IIS-1751206, NSF IIS-1812850, AWS ML Research Award, Google Cloud Platform research credits, and XSEDE IRI180001. Daniel Bolya and Chong Zhou contributed equally to this work.",,55,35,35,25,49,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,1108,1121,,10.1109/TPAMI.2020.3014297,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32755851,Green Submitted,Y,Y,2022-05-23,WOS:000740006100040
J,"Chakraborty, R; Bouza, J; Manton, J; Vemuri, BC",,,,"Chakraborty, Rudrasis; Bouza, Jose; Manton, Jonathan; Vemuri, Baba C.",,,ManifoldNet: A Deep Neural Network for Manifold-Valued Data With Applications,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Weighted frechet mean; equivariance; group action; riemannian manifolds,,"Geometric deep learning is a relatively nascent field that has attracted significant attention in the past few years. This is partly due to the availability of data acquired from non-euclidean domains or features extracted from euclidean-space data that reside on smooth manifolds. For instance, pose data commonly encountered in computer vision reside in Lie groups, while covariance matrices that are ubiquitous in many fields and diffusion tensors encountered in medical imaging domain reside on the manifold of symmetric positive definite matrices. Much of this data is naturally represented as a grid of manifold-valued data. In this paper we present a novel theoretical framework for developing deep neural networks to cope with these grids of manifold-valued data inputs. We also present a novel architecture to realize this theory and call it the ManifoldNet. Analogous to vector spaces where convolutions are equivalent to computing weighted sums, manifold-valued data 'convolutions' can be defined using the weighted Frechet Mean (wFM). (This requires endowing the manifold with a Riemannian structure if it did not already come with one.) The hidden layers of ManifoldNet compute wFMs of their inputs, where the weights are to be learnt. This means the data remain manifold-valued as they propagate through the hidden layers. To reduce computational complexity, we present a provably convergent recursive algorithm for computing the wFM. Further, we prove that on non-constant sectional curvature manifolds, each wFM layer is a contraction mapping and provide constructive evidence for its non-collapsibility when stacked in layers. This captures the two fundamental properties of deep network layers. Analogous to the equivariance of convolution in euclidean space to translations, we prove that the wFM is equivariant to the action of the group of isometries admitted by the Riemannian manifold on which the data reside. To showcase the performance of ManifoldNet, we present several experiments using both computer vision and medical imaging data sets.","[Chakraborty, Rudrasis] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Bouza, Jose; Vemuri, Baba C.] Univ Florida, Gainesville, FL 32611 USA; [Manton, Jonathan] Univ Melbourne, Parkville, Vic 013, Australia",,"Vemuri, BC (通讯作者)，Univ Florida, Gainesville, FL 32611 USA.",rudrasischa@gmail.com; josejbouw@gmail.com; j.manton@ieee.org; vemuri@cise.ufl.edu,,"Vemuri, Baba/0000-0002-1400-5844",NSF [IIS-1724174],NSF(National Science Foundation (NSF)),This work was supported in part by the NSF Grant IIS-1724174 to BCV. The authors would like to thank Dr. David Vaillancourt of UFL for providing the diffusion MRI data.,,52,4,4,8,12,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,799,810,,10.1109/TPAMI.2020.3003846,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32750791,,Y,N,2022-05-23,WOS:000740006100020
J,"Han, JW; Yao, XW; Cheng, G; Feng, XX; Xu, D",,,,"Han, Junwei; Yao, Xiwen; Cheng, Gong; Feng, Xiaoxu; Xu, Dong",,,P-CNN: Part-Based Convolutional Neural Networks for Fine-Grained Visual Categorization,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Visualization; Training; Detectors; Streaming media; Measurement; Feature extraction; Convolutional neural networks; Part localization network; part classification network; duplex focal loss; fine-grained visual categorization,,"This paper proposes an end-to-end fine-grained visual categorization system, termed Part-based Convolutional Neural Network (P-CNN), which consists of three modules. The first module is a Squeeze-and-Excitation (SE) block, which learns to recalibrate channel-wise feature responses by emphasizing informative channels and suppressing less useful ones. The second module is a Part Localization Network (PLN) used to locate distinctive object parts, through which a bank of convolutional filters are learned as discriminative part detectors. Thus, a group of informative parts can be discovered by convolving the feature maps with each part detector. The third module is a Part Classification Network (PCN) that has two streams. The first stream classifies each individual object part into image-level categories. The second stream concatenates part features and global feature into a joint feature for the final classification. In order to learn powerful part features and boost the joint feature capability, we propose a Duplex Focal Loss used for metric learning and part classification, which focuses on training hard examples. We further merge PLN and PCN into a unified network for an end-to-end training process via a simple training technique. Comprehensive experiments and comparisons with state-of-the-art methods on three benchmark datasets demonstrate the effectiveness of our proposed method.","[Han, Junwei; Yao, Xiwen; Cheng, Gong; Feng, Xiaoxu] Northwestern Polytech Univ, Sch Automat, Xian 710072, Shaanxi, Peoples R China; [Xu, Dong] Univ Sydney, Sch Elect & Informat Engn, Camperdown, NSW 2006, Australia",,"Cheng, G (通讯作者)，Northwestern Polytech Univ, Sch Automat, Xian 710072, Shaanxi, Peoples R China.",junweihan2010@gmail.com; yaoxiwen@nwpu.edu.cn; gcheng@nwpu.edu.cn; fengxiaox@mail.nwpu.edu.cn; dong.xu@sydney.edu.au,,"Yao, Xiwen/0000-0002-7466-7428; Feng, Xiaoxu/0000-0003-2639-2447","National Key Research and Development Plan of China [2018YFB1402600]; National Science Foundation of China (NSFC) [61701415, 61772425, 61773315, 61790552]; Young Star of Science and Technology in Shaanxi Province [2018KJXX-029]; Fundamental Research Funds for the Central Universities [3102018zy023, 3102019AX09]; Australian Research Council Future Fellowship [FT180100116]",National Key Research and Development Plan of China; National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Young Star of Science and Technology in Shaanxi Province; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Australian Research Council Future Fellowship(Australian Research Council),"This work was supported in part by the National Key Research and Development Plan of China under Grant 2018YFB1402600, in part by the National Science Foundation of China (NSFC) under Grants 61701415, 61772425, 61773315, and 61790552, in part by the Young Star of Science and Technology in Shaanxi Province under grant 2018KJXX-029, in part by the Fundamental Research Funds for the Central Universities under Grant 3102018zy023 and 3102019AX09, and in part by the Australian Research Council Future Fellowship under Grant FT180100116. J. Han and X. Yao contributed equally to this work.",,59,40,41,24,24,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,579,590,,10.1109/TPAMI.2019.2933510,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,31398107,,Y,Y,2022-05-23,WOS:000740006100004
J,"Li, JN; Zhang, SL; Tian, Q; Wang, M; Gao, W",,,,"Li, Jianing; Zhang, Shiliang; Tian, Qi; Wang, Meng; Gao, Wen",,,Pose-Guided Representation Learning for Person Re-Identification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Feature extraction; Training; Pose estimation; Robustness; Image segmentation; Body regions; Measurement; Person re-identification; pose variation; misalignment; pose-guided representation,ATTENTION; NETWORK,"The large pose variations and misalignment errors exhibited by person images significantly increase the difficulty of person Re-Identification (ReID). Existing works commonly apply extra operations like pose estimation, part segmentation, etc., to alleviate those issues and improve the robustness of pedestrian representations. While boosting the ReID accuracy, those operations introduce considerable computational overheads and make the deep models complex and hard to tune. To chase a more efficient solution, we propose a Part-Guided Representation (PGR) composed of Pose Invariant Feature (PIF) and Local Descriptive Feature (LDF), respectively. We call PGR Part-Guided because it is trained and supervised by local part cues. Specifically, PIF approximates a pose invariant representation inferred by pose estimation and pose normalization. LDF focuses on discriminative body parts by approximating a representation learned with body region segmentation. In this way, extra pose extraction is only introduced during the training stage to supervise the learning of PGR, but is not required during the testing stage for feature extraction. Extensive comparisons with recent works on five widely used datasets demonstrate the competitive accuracy and efficiency of PGR.","[Li, Jianing; Zhang, Shiliang; Gao, Wen] Peking Univ, Dept Elect Engn & Comp Sci, Beijing 100871, Peoples R China; [Tian, Qi] Huawei Noahs Ark Lab, Beijing, Peoples R China; [Wang, Meng] Hefei Univ Technol, Sch Comp Sci, Hefei 230009, Anhui, Peoples R China; [Wang, Meng] Hefei Univ Technol, Sch Artificial Intelligence, Hefei 230009, Anhui, Peoples R China; [Gao, Wen] Peng Cheng Lab, Shenzhen 518000, Peoples R China",,"Zhang, SL (通讯作者)，Peking Univ, Dept Elect Engn & Comp Sci, Beijing 100871, Peoples R China.",ljn-vmc@pku.edu.cn; slzhang.jdl@pku.edu.cn; tian.qi1@huawei.com; eric.mengwang@gmail.com; wgao@pku.edu.cn,,,"Peng Cheng Laboratory; Beijing Natural Science Foundation [JQ18012]; Natural Science Foundation of China [61620106009, 61572050, 91538111]",Peng Cheng Laboratory; Beijing Natural Science Foundation(Beijing Natural Science Foundation); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work is supported in part by Peng Cheng Laboratory, in part by Beijing Natural Science Foundation under Grant No. JQ18012, in part by Natural Science Foundation of China under Grant No. 61620106009, 61572050, 91538111.",,80,16,16,10,15,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,622,635,,10.1109/TPAMI.2019.2929036,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,,,Y,N,2022-05-23,WOS:000740006100007
J,"Otberdout, N; Daoudi, M; Kacem, A; Ballihi, L; Berretti, S",,,,"Otberdout, Naima; Daoudi, Mohammed; Kacem, Anis; Ballihi, Lahoucine; Berretti, Stefano",,,Dynamic Facial Expression Generation on Hilbert Hypersphere With Conditional Wasserstein Generative Adversarial Nets,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Face; Gallium nitride; Generative adversarial networks; Videos; Dynamics; Geometry; Training; Facial expression generation; conditional manifold-valued wasserstein generative adversarial networks; facial landmarks; Riemannian geometry,RECOGNITION; POSE,"In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral face image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a hypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion generation on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize new facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by editing the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first work that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We evaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic appearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation, dynamic facial expression transfer and data augmentation for training improved emotion recognition models.","[Otberdout, Naima; Ballihi, Lahoucine] Mohammed V Univ Rabat, LRIT CNRST URAC 29, Fac Sci, Rabat 10000, Morocco; [Daoudi, Mohammed] Univ Lille, UMR 9189, CRIStAL, IMT Lille Douai,CNRS, F-59000 Lille, France; [Kacem, Anis] Univ Luxembourg, SnT Interdisciplinary Ctr Secur Reliabil & Trust, F-59000 Lille, France; [Berretti, Stefano] Univ Florence, Dept Informat Engn, I-50121 Florence, Italy",,"Otberdout, N (通讯作者)，Mohammed V Univ Rabat, LRIT CNRST URAC 29, Fac Sci, Rabat 10000, Morocco.",naima.otberdout@um5s.net.ma; mohamed.daoudi@imt-lille-douai.fr; anis.kacem@uni.lu; lahoucine.ballihi@um5.ac.ma; stefano.berretti@unifi.it,"Daoudi, Mohammed/H-5935-2013","Daoudi, Mohammed/0000-0003-4219-7860; Otberdout, naima/0000-0002-5694-0128; Berretti, Stefano/0000-0003-1219-4386; kacem, anis/0000-0003-0640-9862; lahoucine, Ballihi/0000-0002-4307-7468",CNRST's Scholarship of Excellence (Morocco); CAMPUS FRANCE [41539RH]; National Agency for Research (ANR) under the Investments for the future program [ANR-16-IDEX-0004 ULNE],CNRST's Scholarship of Excellence (Morocco); CAMPUS FRANCE; National Agency for Research (ANR) under the Investments for the future program(French National Research Agency (ANR)),"This work was supported by the CNRST's Scholarship of Excellence (Morocco), and by CAMPUS FRANCE [PHC TOUBKAL 2019 (French-Morocco Bilateral Program)] under Grant 41539RH. This work was also supported in part by the French State, managed by the National Agency for Research (ANR) under the Investments for the future program with reference ANR-16-IDEX-0004 ULNE. This work was partially done when A. Kacemwas a Ph.Dstudent at IMT Lille-Douai.",,62,9,9,17,23,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,848,863,,10.1109/TPAMI.2020.3002500,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32750786,Green Submitted,Y,N,2022-05-23,WOS:000740006100024
J,"Tang, C; Liu, XW; Zheng, X; Li, WQ; Xiong, J; Wang, LZ; Zomaya, A; Longo, A",,,,"Tang, Chang; Liu, Xinwang; Zheng, Xiao; Li, Wanqing; Xiong, Jian; Wang, Lizhe; Zomaya, Albert; Longo, Antonella",,,DeFusionNET: Defocus Blur Detection via Recurrently Fusing and Refining Discriminative Multi-Scale Deep Features,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Feature extraction; Neural networks; Semantics; Image edge detection; Fuses; Task analysis; Machine learning; Defocus blur detection; multi-scale features; feature fusing; channel attention,MAP ESTIMATION; SINGLE-IMAGE; NETWORK,"Albeit great success has been achieved in image defocus blur detection, there are still several unsolved challenges, e.g., interference of background clutter, scale sensitivity and missing boundary details of blur regions. To deal with these issues, we propose a deep neural network which recurrently fuses and refines multi-scale deep features (DeFusionNet) for defocus blur detection. We first fuse the features from different layers of FCN as shallow features and semantic features, respectively. Then, the fused shallow features are propagated to deep layers for refining the details of detected defocus blur regions, and the fused semantic features are propagated to shallow layers to assist in better locating blur regions. The fusion and refinement are carried out recurrently. In order to narrow the gap between low-level and high-level features, we embed a feature adaptation module before feature propagating to exploit the complementary information as well as reduce the contradictory response of different feature layers. Since different feature channels are with different extents of discrimination for detecting blur regions, we design a channel attention module to select discriminative features for feature refinement. Finally, the output of each layer at last recurrent step are fused to obtain the final result. We collect a new dataset consists of various challenging images and their pixel-wise annotations for promoting further study. Extensive experiments on two commonly used datasets and our newly collected one are conducted to demonstrate both the efficacy and efficiency of DeFusionNet.","[Tang, Chang; Wang, Lizhe] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China; [Liu, Xinwang; Zheng, Xiao] Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China; [Zheng, Xiao] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Li, Wanqing] Univ Wollongong, Sch Comp & Informat Technol, Keiraville, NSW 2500, Australia; [Xiong, Jian] Southwestern Univ Finance & Econ, Sch Business Adm, Chengdu 611130, Sichuan, Peoples R China; [Zomaya, Albert] Univ Sydney, Sch Informat Technol, Camperdown, NSW 2006, Australia; [Longo, Antonella] Univ Salento, Dept Innovat Engn, I-73100 Lecce, Italy",,"Liu, XW; Zheng, X (通讯作者)，Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China.",tangchang@cug.edu.cn; xinwangliu@nudt.edu.cn; endozheng@gmail.com; wanqing@uow.edu.au; xiongjian2017@swufe.edu.cn; Lizhe.Wang@gmail.com; albert.zomaya@sydney.edu.au; antonella.longo@unisalento.it,"Tang, Chang/AAU-8995-2020; Longo, Antonella/A-5335-2009; Zomaya, Albert Y./G-9697-2017; Wang, Lizhe/L-7453-2014","Tang, Chang/0000-0002-6515-7696; Longo, Antonella/0000-0002-6902-0160; Zomaya, Albert Y./0000-0002-3090-1059; Wang, Lizhe/0000-0003-2766-0845; Li, Wanqing/0000-0002-4427-2687; LIU, Xinwang/0000-0001-9066-1475","National Natural Science Foundation of China [61701451, 61773392, 61901205, U1711266, 41925007]; Opening Fund of Key Laboratory of Geological Survey and Evaluation of Ministry of Education [GLAB2020ZR18]; Fundamental Research Funds for the Central Universities",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Opening Fund of Key Laboratory of Geological Survey and Evaluation of Ministry of Education; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities),"The authors would like to thank the anonymous reviewers for their constructive comments for improving this paper and also like to thank NVIDIA Corporation for the donation of a Titan Xp GPU card and a Titan V GPU card used for this research.. This work was supported by the National Natural Science Foundation of China (No. 61701451, 61773392, 61901205, U1711266, and 41925007) and the Opening Fund of Key Laboratory of Geological Survey and Evaluation of Ministry of Education (No. GLAB2020ZR18) and the Fundamental Research Funds for the Central Universities.",,61,18,18,12,18,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,955,968,,10.1109/TPAMI.2020.3014629,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32759080,,Y,N,2022-05-23,WOS:000740006100030
J,"Tang, JH; Shu, XB; Yan, R; Zhang, LY",,,,"Tang, Jinhui; Shu, Xiangbo; Yan, Rui; Zhang, Liyan",,,Coherence Constrained Graph LSTM for Group Activity Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Coherence; Activity recognition; Logic gates; Motion measurement; Time measurement; Recurrent neural networks; Games; Group activity recognition; long short-term memory; fine-grained motion; deep learning,HISTOGRAMS; ALGORITHM,"This work aims to address the group activity recognition problem by exploring human motion characteristics. Traditional methods hold that the motions of all persons contribute equally to the group activity, which suppresses the contributions of some relevant motions to the whole activity while overstating some irrelevant motions. To address this problem, we present a Spatio-Temporal Context Coherence (STCC) constraint and a Global Context Coherence (GCC) constraint to capture the relevant motions and quantify their contributions to the group activity, respectively. Based on this, we propose a novel Coherence Constrained Graph LSTM (CCG-LSTM) with STCC and GCC to effectively recognize group activity, by modeling the relevant motions of individuals while suppressing the irrelevant motions. Specifically, to capture the relevant motions, we build the CCG-LSTM with a temporal confidence gate and a spatial confidence gate to control the memory state updating in terms of the temporally previous state and the spatially neighboring states, respectively. In addition, an attention mechanism is employed to quantify the contribution of a certain motion by measuring the consistency between itself and the whole activity at each time step. Finally, we conduct experiments on two widely-used datasets to illustrate the effectiveness of the proposed CCG-LSTM compared with the state-of-the-art methods.","[Tang, Jinhui; Shu, Xiangbo; Yan, Rui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China; [Zhang, Liyan] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China",,"Shu, XB (通讯作者)，Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.",jinhuitang@njust.edu.cn; shuxb@njust.edu.cn; ruiyan@njust.edu.cn; zhangliyan@nuaa.edu.cn,"Shu, Xiangbo/AAC-6245-2022","Shu, Xiangbo/0000-0003-4902-4663; Zhang, Liyan/0000-0002-1549-3317","National Key Research and Development Program of China [2016YFB1001001]; National Natural Science Foundation of China [61732007, 61702265, 61572252, 61772268, 61720106006]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001001, in part by the National Natural Science Foundation of China under Grant 61732007, Grant 61702265, Grant 61572252, Grant 61772268 and Grant 61720106006",,60,37,37,14,26,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,636,647,,10.1109/TPAMI.2019.2928540,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,31329548,,Y,Y,2022-05-23,WOS:000740006100008
J,"Tian, ZT; Zhao, HS; Shu, M; Yang, ZC; Li, RY; Jia, JY",,,,"Tian, Zhuotao; Zhao, Hengshuang; Shu, Michelle; Yang, Zhicheng; Li, Ruiyu; Jia, Jiaya",,,Prior Guided Feature Enrichment Network for Few-Shot Segmentation,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Few-shot segmentation; few-shot learning; semantic segmentation; scene understanding,,"State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hardly work on unseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly adapts to new classes with a few labeled support samples. Theses frameworks still face the challenge of generalization ability reduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency between query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It consists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also improves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching query features with support features and prior masks. Extensive experiments on PASCAL-5(i) and COCO prove that the proposed prior generation method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by a large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples.","[Tian, Zhuotao; Zhao, Hengshuang; Jia, Jiaya] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Shu, Michelle] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Yang, Zhicheng; Li, Ruiyu] SmartMore, Shenzhen 518057, Guangdong, Peoples R China",,"Zhao, HS (通讯作者)，Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.",tianzhuotao@gmail.com; hengshuangzhao@gmail.com; mshu1@jhu.edu; cosnozc@gmail.com; royliruiyu@gmail.com; leojia@cse.cuhk.edu.hk,,,,,,,54,19,19,15,25,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,1050,1065,,10.1109/TPAMI.2020.3013717,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32750843,Green Submitted,Y,N,2022-05-23,WOS:000740006100036
J,"Ye, M; Shen, JB; Zhang, X; Yuen, PC; Chang, SF",,,,"Ye, Mang; Shen, Jianbing; Zhang, Xu; Yuen, Pong C.; Chang, Shih-Fu",,,Augmentation Invariant and Instance Spreading Feature for Softmax Embedding,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Task analysis; Visualization; Testing; Training; Unsupervised learning; Data mining; Unsupervised learning; instance feature; softmax embedding; embedding learning; data augmentation,,"Deep embedding learning plays a key role in learning discriminative feature representations, where the visually similar samples are pulled closer and dissimilar samples are pushed away in the low-dimensional embedding space. This paper studies the unsupervised embedding learning problem by learning such a representation without using any category labels. This task faces two primary challenges: mining reliable positive supervision from highly similar fine-grained classes, and generalizing to unseen testing categories. To approximate the positive concentration and negative separation properties in category-wise supervised learning, we introduce a data augmentation invariant and instance spreading feature using the instance-wise supervision. We also design two novel domain-agnostic augmentation strategies to further extend the supervision in feature space, which simulates the large batch training using a small batch size and the augmented features. To learn such a representation, we propose a novel instance-wise softmax embedding, which directly perform the optimization over the augmented instance features with the binary discrmination softmax encoding. It significantly accelerates the learning speed with much higher accuracy than existing methods, under both seen and unseen testing categories. The unsupervised embedding performs well even without pre-trained network over samples from fine-grained categories. We also develop a variant using category-wise supervision, namely category-wise softmax embedding, which achieves competitive performance over the state-of-of-the-arts, without using any auxiliary information or restrict sample mining.","[Ye, Mang; Shen, Jianbing] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Ye, Mang; Yuen, Pong C.] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China; [Zhang, Xu; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA",,"Shen, JB (通讯作者)，Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.",mangye16@gmail.com; shenjianbingcg@gmail.com; xu.zhang@columbia.edu; pcyuen@comp.hkbu.edu.hk; sc250@columbia.edu,,"Yuen, Pong Chi/0000-0002-9343-2202",,,,,88,38,39,10,11,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,924,939,,10.1109/TPAMI.2020.3013379,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,32750841,,Y,N,2022-05-23,WOS:000740006100028
J,"Yu, J; Tan, M; Zhang, HY; Tao, DC; Rui, Y",,,,"Yu, Jun; Tan, Min; Zhang, Hongyuan; Tao, Dacheng; Rui, Yong",,,Hierarchical Deep Click Feature Prediction for Fine-Grained Image Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Visualization; Feature extraction; Image recognition; Semantics; Predictive models; Vocabulary; Task analysis; Click prediction; hierarchical model; word embedding; deep neural network; transfer learning,CNNS,"The click feature of an image, defined as the user click frequency vector of the image on a predefined word vocabulary, is known to effectively reduce the semantic gap for fine-grained image recognition. Unfortunately, user click frequency data are usually absent in practice. It remains challenging to predict the click feature from the visual feature, because the user click frequency vector of an image is always noisy and sparse. In this paper, we devise a Hierarchical Deep Word Embedding (HDWE) model by integrating sparse constraints and an improved RELU operator to address click feature prediction from visual features. HDWE is a coarse-to-fine click feature predictor that is learned with the help of an auxiliary image dataset containing click information. It can therefore discover the hierarchy of word semantics. We evaluate HDWE on three dog and one bird image datasets, in which Clickture-Dog and Clickture-Bird are utilized as auxiliary datasets to provide click data, respectively. Our empirical studies show that HDWE has 1) higher recognition accuracy, 2) a larger compression ratio, and 3) good one-shot learning ability and scalability to unseen categories.","[Yu, Jun; Tan, Min; Zhang, Hongyuan] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China; [Rui, Yong] Lenovo, 6 Shang Di West Rd, Beijing 100085, Peoples R China; [Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney Artificial Intelligence Ctr, 6 Cleveland St, Darlington, NSW 2008, Australia",,"Tan, M (通讯作者)，Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.",yujun@hdu.edu.cn; tanmin@hdu.edu.cn; zhhy1994226@163.com; dacheng.tao@sydney.edu.au; yongrui@lenovo.com,,,"National Natural Science Foundation of China [61836002, 61602136, 61622205, 61601158]; Zhejiang Provincial Natural Science Foundation of China [LY19F020038]; Australian Research Council [FL-170100117, DP-180103424]; Zhejiang Provincial Key Science and Technology Project Foundation [2018C01012]",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); Australian Research Council(Australian Research Council); Zhejiang Provincial Key Science and Technology Project Foundation,"This work was supported by National Natural Science Foundation of China No.61836002, Zhejiang Provincial Natural Science Foundation of China (No.LY19F020038), National Natural Science Foundation of China (No.61602136, No.61622205, and No.61601158), Australian Research Council Projects (FL-170100117 and DP-180103424), and Zhejiang Provincial Key Science and Technology Project Foundation (No.2018C01012).",,52,124,124,24,31,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,563,578,,10.1109/TPAMI.2019.2932058,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,,,Y,Y,2022-05-23,WOS:000740006100003
J,"Zha, ZJ; Liu, DQ; Zhang, HW; Zhang, YD; Wu, F",,,,"Zha, Zheng-Jun; Liu, Daqing; Zhang, Hanwang; Zhang, Yongdong; Wu, Feng",,,Context-Aware Visual Policy Network for Fine-Grained Image Captioning,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Visualization; Task analysis; Cognition; Decision making; Training; Natural languages; Reinforcement learning; Image captioning; reinforcement learning; visual context; policy network,LANGUAGE,"With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., man riding horse) and visual comparisons (e.g., small(er) cat). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model-CAVP and its subsequent language policy network-can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.","[Zha, Zheng-Jun; Liu, Daqing; Zhang, Yongdong; Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China; [Zhang, Hanwang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore",,"Zha, ZJ (通讯作者)，Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.",zhazj@ustc.edu.cn; liudq@mail.ustc.edu.cn; hanwangzhang@ntu.edu.sg; zhyd73@ustc.edu.cn; fengwu@ustc.edu.cn,"Zha, Zheng-Jun/AAE-8408-2020","Zha, Zheng-Jun/0000-0003-2510-8993; Zhang, Hanwang/0000-0001-7374-8739","National Key R&D Program of China [2017YFB1300201]; National Natural Science Foundation of China (NSFC) [61622211, 61620106009, 61525206]; Fundamental Research Funds for the Central Universities [WK2100100030]",National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities),"This work was supported by the National Key R&D Program of China under Grant 2017YFB1300201, the National Natural Science Foundation of China (NSFC) under Grants 61622211, 61620106009 and 61525206 as well as the Fundamental Research Funds for the Central Universities under Grant WK2100100030.",,49,20,20,19,27,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2022,44,2,,,,,710,722,,10.1109/TPAMI.2019.2909864,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,YC9LS,30969916,Green Submitted,Y,N,2022-05-23,WOS:000740006100013
J,"Dai, WC; Zhang, Y; Li, P; Fang, Z; Scherer, S",,,,"Dai, Weichen; Zhang, Yu; Li, Ping; Fang, Zheng; Scherer, Sebastian",,,RGB-D SLAM in Dynamic Environments Using Point Correlations,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,SLAM; motion estimation; dynamic environments,VISUAL ODOMETRY; TRACKING; MOTION; ALGORITHM,"In this paper, a simultaneous localization and mapping (SLAM) method that eliminates the influence of moving objects in dynamic environments is proposed. This method utilizes the correlation between map points to separate points that are part of the static scene and points that are part of different moving objects into different groups. A sparse graph is first created using Delaunay triangulation from all map points. In this graph, the vertices represent map points, and each edge represents the correlation between adjacent points. If the relative position between two points remains consistent over time, there is correlation between them, and they are considered to be moving together rigidly. If not, they are considered to have no correlation and to be in separate groups. After the edges between the uncorrelated points are removed during point-correlation optimization, the remaining graph separates the map points of the moving objects from the map points of the static scene. The largest group is assumed to be the group of reliable static map points. Finally, motion estimation is performed using only these points. The proposed method was implemented for RGB-D sensors, evaluated with a public RGB-D benchmark, and tested in several additional challenging environments. The experimental results demonstrate that robust and accurate performance can be achieved by the proposed SLAM method in both slightly and highly dynamic environments. Compared with other state-of-the-art methods, the proposed method can provide competitive accuracy with good real-time performance.","[Dai, Weichen] Zhejiang Univ, Coll Control Sci & Engn, Hangzhou 310027, Zhejiang, Peoples R China; [Zhang, Yu; Li, Ping] Zhejiang Univ, State Key Lab Ind Control Technol, Coll Control Sci & Engn, Hangzhou 310027, Zhejiang, Peoples R China; [Fang, Zheng] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110032, Peoples R China; [Scherer, Sebastian] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA",,"Zhang, Y (通讯作者)，Zhejiang Univ, State Key Lab Ind Control Technol, Coll Control Sci & Engn, Hangzhou 310027, Zhejiang, Peoples R China.",weichendai@zju.edu.cn; zhangyu80@zju.edu.cn; pli@iipc.zju.edu.cn; fangzheng@mail.neu.edu.cn; basti@andrew.cmu.edu,,"Scherer, Sebastian/0000-0002-8373-4688; Zhang, Yu/0000-0002-0043-4904","National Natural Science Foundation of China [61673341, 61573091]; National Key R&D Program of China [2016YFD0200701-3]; China's Double First-class Initiative; Project of State Key Laboratory of Industrial Control Technology, Zhejiang University, China [ICT1913]; Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University, China [ICT1900312, ICT20037]","National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; China's Double First-class Initiative; Project of State Key Laboratory of Industrial Control Technology, Zhejiang University, China; Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University, China","This work was supported by the National Natural Science Foundation of China (Grant Nos. 61673341, and 61573091), National Key R&D Program of China (2016YFD0200701-3), China's Double First-class Initiative, the Project of State Key Laboratory of Industrial Control Technology, Zhejiang University, China (No. ICT1913) and the Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University, China (No. ICT1900312, No. ICT20037). Weichen Dai and Yu Zhang contributed equally to thiswork.",,60,11,11,51,68,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,373,389,,10.1109/TPAMI.2020.3010942,,,17,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750826,Green Submitted,Y,N,2022-05-23,WOS:000728561300027
J,"Fu, Y; Zhang, T; Zheng, YQ; Zhang, DB; Huang, H",,,,"Fu, Ying; Zhang, Tao; Zheng, Yinqiang; Zhang, Debing; Huang, Hua",,,Joint Camera Spectral Response Selection and Hyperspectral Image Recovery,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Camera spectral response selection; hyperspectral image recovery; spectral nonlinear mapping; spatial similarity; and classification,REFLECTANCE RECOVERY; SPECTROMETER; SENSITIVITY; DESIGN; SYSTEM,"Hyperspectral image (HSI) recovery from a single RGB image has attracted much attention, whose performance has recently been shown to be sensitive to the camera spectral response (CSR). In this paper, we present an efficient convolutional neural network (CNN) based method, which can jointly select the optimal CSR from a candidate dataset and learn a mapping to recover HSI from a single RGB image captured with this algorithmically selected camera under multi-chip or single-chip setups. Given a specific CSR, we first present a HSI recovery network, which accounts for the underlying characteristics of the HSI, including spectral nonlinear mapping and spatial similarity. Later, we append a CSR selection layer onto the recovery network, and the optimal CSR under both multi-chip and single-chip setups can thus be automatically determined from the network weights under the nonnegative sparse constraint. Experimental results on three hyperspectral datasets and two camera spectral response datasets demonstrate that our HSI recovery network outperforms state-of-the-art methods in terms of both quantitative metrics and perceptive quality, and the selection layer always returns a CSR consistent to the best one determined by exhaustive search. Finally, we show that our method can also perform well in the real capture system, and collect a hyperspectral flower dataset to evaluate the effect from HSI recovery on classification problem.","[Fu, Ying; Zhang, Tao; Huang, Hua] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China; [Zheng, Yinqiang] Natl Inst Informat, Tokyo 1018430, Japan; [Zhang, Debing] DeepGlint, Beijing 100091, Peoples R China",,"Huang, H (通讯作者)，Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.",fuying@bit.edu.cn; tzhang@bit.edu.cn; yqzheng@nii.ac.jp; debingzhang@deepglint.com; huahuang@bit.edu.cn,,"tao, zhang/0000-0002-7358-0603",National Natural Science Foundation of China [61672096]; JSPS KAKENHI [19K20307],"National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))",This work was supported by the National Natural Science Foundation of China under Grant No. 61672096 and the JSPS KAKENHI under Grant No. 19K20307.,,72,7,7,14,19,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,256,272,,10.1109/TPAMI.2020.3009999,,,17,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750820,,Y,N,2022-05-23,WOS:000728561300019
J,"Gallego, G; Delbruck, T; Orchard, GM; Bartolozzi, C; Taba, B; Censi, A; Leutenegger, S; Davison, A; Conradt, J; Daniilidis, K; Scaramuzza, D",,,,"Gallego, Guillermo; Delbruck, Tobi; Orchard, Garrick Michael; Bartolozzi, Chiara; Taba, Brian; Censi, Andrea; Leutenegger, Stefan; Davison, Andrew; Conradt, Jorg; Daniilidis, Kostas; Scaramuzza, Davide",,,Event-Based Vision: A Survey,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Event cameras; bio-inspired vision; asynchronous sensor; low latency; high dynamic range; low power,DRIVEN; TRACKING; SENSORS; RECONSTRUCTION; ARCHITECTURE; TEMPERATURE; CAMERAS; SYSTEMS; DESIGN,"Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of mu s), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.","[Gallego, Guillermo] Tech Univ Berlin, D-10623 Berlin, Germany; [Gallego, Guillermo] Einstein Ctr Digital Future, D-10117 Berlin, Germany; [Delbruck, Tobi] Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland; [Delbruck, Tobi] Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland; [Delbruck, Tobi] Swiss Fed Inst Technol, CH-8057 Zurich, Switzerland; [Orchard, Garrick Michael] Intel Labs, Santa Clara, CA 95054 USA; [Bartolozzi, Chiara] Ist Italiano Tecnol, I-16163 Genoa, Italy; [Taba, Brian] IBM Res, San Jose, CA 95120 USA; [Censi, Andrea] Swiss Fed Inst Technol, Dept Mech & Proc Engn, CH-8092 Zurich, Switzerland; [Leutenegger, Stefan; Davison, Andrew] Imperial Coll London, London SW7 2BU, England; [Conradt, Jorg] KTH Royal Inst Technol, S-11428 Stockholm, Sweden; [Daniilidis, Kostas] Univ Penn, Philadelphia, PA 19104 USA; [Scaramuzza, Davide] Univ Zurich, CH-8050 Zurich, Switzerland",,"Gallego, G (通讯作者)，Tech Univ Berlin, D-10623 Berlin, Germany.",guillermo.gallego@tu-berlin.de; tobi@ini.uzh.ch; garrick.orchard@intel.com; chiara.bartolozzi@iit.it; btaba@us.ibm.com; acensi@ethz.ch; s.leutenegger@imperial.ac.uk; ajd@doc.ic.ac.uk; jconradt@kth.se; kostas@cis.upenn.edu; sdavide@ifi.uzh.ch,"Gallego, Guillermo/I-7131-2012","Gallego, Guillermo/0000-0002-2672-9241; Delbruck, Tobi/0000-0001-5479-1141; Leutenegger, Stefan/0000-0002-7998-3737; Bartolozzi, Chiara/0000-0003-3465-6449; Daniilidis, Kostas/0000-0003-0498-0758; Conradt, Jorg/0000-0001-5998-9640",SNSF-ERC Starting Grant; Swiss National Science Foundation through the National Center of Competence in Research (NCCR) Robotics,SNSF-ERC Starting Grant; Swiss National Science Foundation through the National Center of Competence in Research (NCCR) Robotics,"The work of G. Gallego and D. Scaramuzza was supported by the SNSF-ERC Starting Grant and the Swiss National Science Foundation through the National Center of Competence in Research (NCCR) Robotics. The authors would like to thank all the people who contributed to this paper. The authors would like to thank the event camera manufacturers for providing the values in Table 1 and for discussing the difficulties in their comparison due to the lack of a common testbed. In particular, we thank Hyunsurk Eric Ryu (Samsung Electronics), Chenghan Li (iniVation), Davide Migliore (Prophesee), Marc Osswald (Insightness) and Prof. Chen (CelePixel). We are also thankful to all members of our research laboratories, for discussion and comments on early versions of this document. We thank the Editors and anonymous reviewers of IEEE TPAMI for their suggestions, which led us to improve the paper.",,241,184,184,57,65,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,154,180,,10.1109/TPAMI.2020.3008413,,,27,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750812,"Bronze, Green Published, Green Submitted",Y,Y,2022-05-23,WOS:000728561300013
J,"He, JZ; Zhang, SL; Yang, M; Shan, YH; Huang, TJ",,,,"He, Jianzhong; Zhang, Shiliang; Yang, Ming; Shan, Yanhu; Huang, Tiejun",,,BDCN: Bi-Directional Cascade Network for Perceptual Edge Detection,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Edge detection; bi-directional cascade network; scale enhancement; convolutional neural network,BOUNDARIES; FEATURES,"Exploiting multi-scale representations is critical to improve edge detection for objects at different scales. To extract edges at dramatically different scales, we propose a bi-directional cascade network (BDCN) architecture, where an individual layer is supervised by labeled edges at its specific scale, rather than directly applying the same supervision to different layers. Furthermore, to enrich multi-scale representations learned by each layer of BDCN, we introduce a scale enhancement module (SEM), which utilizes dilated convolution to generate multi-scale features, instead of using deeper CNNs. These new approaches encourage the learning of multi-scale representations in different layers and detect edges that are well delineated by their scales. Learning scale dedicated layers also results in a compact network with a fraction of parameters. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS F-measure of 0.832, 2.7 percent higher than current state-of-the-art on the BSDS500 dataset. We also applied our edge detection result to other vision tasks. Experimental results show that, our method further boosts the performance of image segmentation, optical flow estimation, and object proposal generation.","[He, Jianzhong; Zhang, Shiliang; Huang, Tiejun] Peking Univ, Sch EECS, Dept Comp Sci, Beijing 100871, Peoples R China; [Yang, Ming; Shan, Yanhu] Horizon Robot Inc, Beijing 100800, Peoples R China; [Huang, Tiejun] Peng Cheng Lab, Shenzhen 518066, Peoples R China",,"Zhang, SL (通讯作者)，Peking Univ, Sch EECS, Dept Comp Sci, Beijing 100871, Peoples R China.",jianzhonghe@pku.edu.cn; slzhang.jdl@pku.edu.cn; m-yang4@u.northwestern.edu; yanhu.shan@gmail.com; tjhuang@pku.edu.cn,,"Yang, Ming/0000-0003-1691-6817","National Key Research and Development Program of China [2018YFE0118400]; Beijing Natural Science Foundation [JQ18012]; Natural Science Foundation of China [61936011, 61425025, 61620106009, 61572050, 91538111]",National Key Research and Development Program of China; Beijing Natural Science Foundation(Beijing Natural Science Foundation); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFE0118400, in part by Beijing Natural Science Foundation under Grant No. JQ18012, in part by the Natural Science Foundation of China under Grant No. 61936011, 61425025, 61620106009, 61572050, and 91538111.",,76,19,21,29,34,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,100,113,,10.1109/TPAMI.2020.3007074,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750803,,Y,N,2022-05-23,WOS:000728561300009
J,"Huang, ZL; Wei, YC; Wang, XG; Liu, WY; Huang, TS; Shi, H",,,,"Huang, Zilong; Wei, Yunchao; Wang, Xinggang; Liu, Wenyu; Huang, Thomas S.; Shi, Humphrey",,,AlignSeg: Feature-Aligned Segmentation Networks,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Semantic segmentation; feature alignment; context alignment,,"Aggregating features in terms of different convolutional blocks or contextual embeddings has been proven to be an effective way to strengthen feature representations for semantic segmentation. However, most of the current popular network architectures tend to ignore the misalignment issues during the feature aggregation process caused by step-by-step downsampling operations and indiscriminate contextual information fusion. In this paper, we explore the principles in addressing such feature misalignment issues and inventively propose Feature-Aligned Segmentation Networks (AlignSeg). AlignSeg consists of two primary modules, i.e., the Aligned Feature Aggregation (AlignFA) module and the Aligned Context Modeling (AlignCM) module. First, AlignFA adopts a simple learnable interpolation strategy to learn transformation offsets of pixels, which can effectively relieve the feature misalignment issue caused by multi-resolution feature aggregation. Second, with the contextual embeddings in hand, AlignCM enables each pixel to choose private custom contextual information adaptively, making the contextual embeddings be better aligned. We validate the effectiveness of our AlignSeg network with extensive experiments on Cityscapes and ADE20K, achieving new state-of-the-art mIoU scores of 82.6 and 45.95 percent, respectively.","[Huang, Zilong; Wang, Xinggang; Liu, Wenyu] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China; [Wei, Yunchao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China; [Wei, Yunchao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China; [Huang, Thomas S.] Univ Illinois, Beckman Inst, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Shi, Humphrey] Univ Oregon, Comp Sci, Eugene, OR 97403 USA; [Shi, Humphrey] Univ Illinois, Elect & Informat Engn, Urbana, IL 61801 USA; [Shi, Humphrey] Picsart AI Res, San Francisco, CA 94105 USA",,"Wang, XG (通讯作者)，Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.",hzl@hust.edu.cn; wychao1987@gmail.com; xgwang@hust.edu.cn; liuwy@hust.edu.cn; t-huang1@illinois.edu; hshi10@illinois.edu,"Liu, Wenyu/AAG-1426-2019","Liu, Wenyu/0000-0002-4582-7488; Wang, Xinggang/0000-0001-6732-7823","NSFC [61876212, 61733007]; China Scholarship Council; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network",NSFC(National Natural Science Foundation of China (NSFC)); China Scholarship Council(China Scholarship Council); IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network(International Business Machines (IBM)),"This work was supported in part by the NSFC (No. 61876212 and No. 61733007), China Scholarship Council, and IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network.",,54,7,7,9,11,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,550,557,,10.1109/TPAMI.2021.3062772,,,8,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,33646946,Green Submitted,Y,N,2022-05-23,WOS:000728561300039
J,"Kim, UH; Kim, SH; Kim, JH",,,,"Kim, Ue-Hwan; Kim, Se-Ho; Kim, Jong-Hwan",,,"SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation",IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Visual odometry (VO); data-driven VO; visual SLAM; semantic VO; semantic SLAM; semantic mapping; monocular video; depth map prediction; depth estimation; ego-motion estimation; unsupervised learning; deep convolutional neural network (CNN),,"Intelligent agents need to understand the surrounding environment to provide meaningful services to or interact intelligently with humans. The agents should perceive geometric features as well as semantic entities inherent in the environment. Contemporary methods in general provide one type of information regarding the environment at a time, making it difficult to conduct high-level tasks. Moreover, running two types of methods and associating two resultant information requires a lot of computation and complicates the software architecture. To overcome these limitations, we propose a neural architecture that simultaneously performs both geometric and semantic tasks in a single thread: simultaneous visual odometry, object detection, and instance segmentation (SimVODIS). SimVODIS is built on top of Mask-RCNN which is trained in a supervised manner. Training the pose and depth branches of SimVODIS requires unlabeled video sequences and the photometric consistency between input image frames generates self-supervision signals. The performance of SimVODIS outperforms or matches the state-of-the-art performance in pose estimation, depth map prediction, object detection, and instance segmentation tasks while completing all the tasks in a single thread. We expect SimVODIS would enhance the autonomy of intelligent agents and let the agents provide effective services to humans.","[Kim, Ue-Hwan; Kim, Se-Ho; Kim, Jong-Hwan] KAIST Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea",,"Kim, JH (通讯作者)，KAIST Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.",uhkim@rit.kaist.ac.kr; shkim@rit.kaist.ac.kr; johkim@rit.kaist.ac.kr,,,Institute for Information & communications Technology Promotion (IITP) - Korea Government (MSIT) [2020-0-00440],Institute for Information & communications Technology Promotion (IITP) - Korea Government (MSIT),"This work was supported by the Institute for Information & communications Technology Promotion (IITP) Grant funded by the Korea Government (MSIT) (No.2020-0-00440, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the RealWorld).",,52,4,4,23,28,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,428,441,,10.1109/TPAMI.2020.3007546,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750805,Green Submitted,Y,N,2022-05-23,WOS:000728561300031
J,"Li, XL; Zhang, H; Wang, R; Nie, FP",,,,"Li, Xuelong; Zhang, Han; Wang, Rong; Nie, Feiping",,,Multiview Clustering: A Scalable and Parameter-Free Bipartite Graph Fusion Method,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Multiview clustering; scalable and parameter-free; graph fusion; connectivity constraint; initialization-independent,,"Multiview clustering partitions data into different groups according to their heterogeneous features. Most existing methods degenerate the applicability of models due to their intractable hyper-parameters triggered by various regularization terms. Moreover, traditional spectral based methods always encounter the expensive time overheads and fail in exploring the explicit clusters from graphs. In this paper, we present a scalable and parameter-free graph fusion framework for multiview clustering, seeking for a joint graph compatible across multiple views in a self-supervised weighting manner. Our formulation coalesces multiple view-wise graphs straightforward and learns the weights as well as the joint graph interactively, which could actively release the model from any weight-related hyper-parameters. Meanwhile, we manipulate the joint graph by a connectivity constraint such that the connected components indicate clusters directly. The designed algorithmis initialization-independent and time-economical which obtains the stable performance and scales well with the data size. Substantial experiments on toy data as well as real datasets are conducted that verify the superiority of the proposed method compared to the state-of-the-arts over the clustering performance and time expenditure.","[Li, Xuelong; Zhang, Han; Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China; [Li, Xuelong; Zhang, Han; Wang, Rong; Nie, Feiping] Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China; [Wang, Rong] Northwestern Polytech Univ, Sch Cybersecur, Xian 710072, Shaanxi, Peoples R China",,"Nie, FP (通讯作者)，Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.; Nie, FP (通讯作者)，Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.",xuelong_li@nwpu.edu.cn; zhanghan0805@mail.nwpu.edu.cn; wangrong@nwpu.edu.cn; feipingnie@gmail.com,,"Nie, Feiping/0000-0002-0871-6519; Wang, Rong/0000-0001-9240-6726","National Key Research and Development Program of China [2018AAA0101902]; National Natural Science Foundation of China [61871470, 61761130079, 61751202, 61772427, 61936014]; Inno vation Foundation for Doctor Dissertation of Northwestern Polytechnical University [CX201918]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Inno vation Foundation for Doctor Dissertation of Northwestern Polytechnical University,"This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0101902, in part by the National Natural Science Foundation of China under Grant 61871470, Grant 61761130079, Grant 61751202, Grant 61772427, and Grant 61936014, and in part by the Inno vation Foundation for Doctor Dissertation of Northwestern Polytechnical University under Grant CX201918.",,59,14,14,17,30,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,330,344,,10.1109/TPAMI.2020.3011148,,,15,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750830,,Y,N,2022-05-23,WOS:000728561300024
J,"Li, Y; Zeng, JB; Shan, SG",,,,"Li, Yong; Zeng, Jiabei; Shan, Shiguang",,,Learning Representations for Facial Actions From Unlabeled Videos,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Facial action unit detection; self-supervised learning; representation learning; feature disentanglement; encoder-decoder structure,,"Facial actions are usually encoded as anatomy-based action units (AUs), the labelling of which demands expertise and thus is time-consuming and expensive. To alleviate the labelling demand, we propose to leverage the large number of unlabelled videos by proposing a twin-cycle autoencoder (TAE) to learn discriminative representations for facial actions. TAE is inspired by the fact that facial actions are embedded in the pixel-wise displacements between two sequential face images (hereinafter, source and target) in the video. Therefore, learning the representations of facial actions can be achieved by learning the representations of the displacements. However, the displacements induced by facial actions are entangled with those induced by head motions. TAE is thus trained to disentangle the two kinds of movements by evaluating the quality of the synthesized images when either the facial actions or head pose is changed, aiming to reconstruct the target image. Experiments on AU detection show that TAE can achieve accuracy comparable to other existing AU detection methods including some supervised methods, thus validating the discriminant capacity of the representations learned by TAE. TAE's ability in decoupling the action-induced and pose-induced movements is also validated by visualizing the generated images and analyzing the facial image retrieval results qualitatively and quantitatively.","[Li, Yong; Zeng, Jiabei; Shan, Shiguang] Chinese Acad Sci, Inst Comp Technol, CAS, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Li, Yong; Shan, Shiguang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Shan, Shiguang] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China",,"Shan, SG (通讯作者)，Chinese Acad Sci, Inst Comp Technol, CAS, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.",yong.li@vipl.ict.ac.cn; jiabei.zeng@vipl.ict.ac.cn; sgshan@ict.ac.cn,"; Zeng, Jiabei/J-3865-2016","Shan, Shiguang/0000-0002-8348-392X; Zeng, Jiabei/0000-0003-3256-4524","National Key R&D Program of China [2017YFA0700800]; National Natural Science Foundation of China [61702481, 61976203]",National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported in part by the National Key R&D Program of China under contracts NO.2017YFA0700800, National Natural Science Foundation of China (Grants 61702481 and 61976203). The authors would like to thank Prof. Xilin Chen's contributions in the conference paper of this work. Yong Li and Jiabei Zeng contributed equally to this work.",,65,8,8,14,19,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,302,317,,10.1109/TPAMI.2020.3011063,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750828,,Y,N,2022-05-23,WOS:000728561300022
J,"Lu, H; Dai, YT; Shen, CH; Xu, SC",,,,"Lu, Hao; Dai, Yutong; Shen, Chunhua; Xu, Songcen",,,Index Networks,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Upsampling operators; dynamic networks; image denoising; semantic segmentation; image matting; depth estimation,,"We show that existing upsampling operators in convolutional networks can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are learned adaptively from data and are used to guide downsampling and upsampling stages, without extra training supervision. At the core of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map. IndexNet can be used as a plug-in, applicable to almost all convolutional networks that have coupled downsampling and upsampling stages, enabling the networks to dynamically capture variations of local patterns. In particular, we instantiate and investigate five families of IndexNet. We highlight their superiority in delivering spatial information over other upsampling operators with experiments on synthetic data, and demonstrate their effectiveness on four dense prediction tasks, including image matting, image denoising, semantic segmentation, and monocular depth estimation. Code and models are available at https://git.io/IndexNet","[Lu, Hao; Dai, Yutong] Univ Adelaide, Adelaide, SA 5005, Australia; [Shen, Chunhua] Univ Adelaide, Comp Sci, Adelaide, SA 5005, Australia; [Xu, Songcen] Huawei Technol, Noahs Ark Lab, Shenzhen 518129, Peoples R China",,"Shen, CH (通讯作者)，Univ Adelaide, Comp Sci, Adelaide, SA 5005, Australia.",hao.lu@adelaide.edu.au; yutong.dai@adelaide.edu.au; chunhua.shen@adelaide.edu.au; xusongcen@huawei.com,"Lu, Hao/AFL-6512-2022","Shen, Chunhua/0000-0002-8648-8718",,,,,51,9,9,6,10,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,242,255,,10.1109/TPAMI.2020.3004474,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750793,,Y,N,2022-05-23,WOS:000728561300018
J,"Peng, L; Yang, Y; Wang, Z; Huang, Z; Shen, HT",,,,"Peng, Liang; Yang, Yang; Wang, Zheng; Huang, Zi; Shen, Heng Tao",,,MRA-Net: Improving VQA Via Multi-Modal Relation Attention Network,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Visual question answering; visual relation; attention mechanism; relation attention,,"Visual Question Answering (VQA) is a task to answer natural language questions tied to the content of visual images. Most recent VQA approaches usually apply attention mechanism to focus on the relevant visual objects and/or consider the relations between objects via off-the-shelf methods in visual relation reasoning. However, they still suffer from several drawbacks. First, they mostly model the simple relations between objects, which results in many complicated questions cannot be answered correctly, because of failing to provide sufficient knowledge. Second, they seldom leverage the harmony cooperation of visual appearance feature and relation feature. To solve these problems, we propose a novel end-to-end VQA model, termed Multi-modal Relation Attention Network (MRA-Net). The proposed model explores both textual and visual relations to improve performance and interpretability. In specific, we devise 1) a self-guided word relation attention scheme, which explore the latent semantic relations between words; 2) two question-adaptive visual relation attention modules that can extract not only the fine-grained and precise binary relations between objects but also the more sophisticated trinary relations. Both kinds of question-related visual relations provide more and deeper visual semantics, thereby improving the visual reasoning ability of question answering. Furthermore, the proposed model also combines appearance feature with relation feature to reconcile the two types of features effectively. Extensive experiments on five large benchmark datasets, VQA-1.0, VQA-2.0, COCO-QA, VQA-CP v2, and TDIUC, demonstrate that our proposed model outperforms state-of-the-art approaches.","[Peng, Liang; Yang, Yang; Wang, Zheng; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China; [Peng, Liang; Yang, Yang; Wang, Zheng; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China; [Yang, Yang] Univ Elect Sci & Technol China UESTC, Inst Elect & Informat Engn, Dongguan 523808, Guangdong, Peoples R China; [Huang, Zi] Univ Queensland, St Lucia, Qld 4072, Australia",,"Yang, Y (通讯作者)，Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.",pliang951125@outlook.com; dlyyang@gmail.com; zh_wang@hotmail.com; huang@itee.uq.edu.au; shenhengtao@hotmail.com,,"Liang, Peng/0000-0002-0576-1429","National Key Research and Development Program of China [2018AAA0102200]; Sichuan Science and Technology Program, China [2018GZDZX0032, 2020YFS0057]; Fundamental Research Funds for the Central Universities [ZYGX2019Z015]; National Natural Science Foundation of China [61632007]; Dongguan Songshan Lake Introduction Programof Leading Innovative and Entrepreneurial Talents","National Key Research and Development Program of China; Sichuan Science and Technology Program, China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Dongguan Songshan Lake Introduction Programof Leading Innovative and Entrepreneurial Talents","This work was supported in part by National Key Research and Development Program of China under Grant No. 2018AAA0102200, the Sichuan Science and Technology Program, China, under Grant 2018GZDZX0032 and 2020YFS0057, the Fundamental Research Funds for the Central Universities under Project ZYGX2019Z015, the National Natural Science Foundation of China under Grants 61632007 and Dongguan Songshan Lake Introduction Programof Leading Innovative and Entrepreneurial Talents.",,56,11,11,19,26,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,318,329,,10.1109/TPAMI.2020.3004830,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750794,,Y,Y,2022-05-23,WOS:000728561300023
J,"Wang, Q; He, X; Jiang, X; Li, XL",,,,"Wang, Qi; He, Xiang; Jiang, Xu; Li, Xuelong",,,Robust Bi-Stochastic Graph Regularized Matrix Factorization for Data Clustering,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Matrix factorization; bi-stochastic graph; data clustering; robustness,TUTORIAL,"Data clustering, which is to partition the given data into different groups, has attracted much attention. Recently various effective algorithms have been developed to tackle the task. Among these methods, non-negative matrix factorization (NMF) has been demonstrated to be a powerful tool. However, there are still some problems. First, the standard NMF is sensitive to noises and outliers. Although l(2,1) norm based NMF improves the robustness, it is still affected easily by large noises. Second, for most graph regularized NMF, the performance highly depends on the initial similarity graph. Third, many graph-based NMF models perform the graph construction and matrix factorization in two separated steps. Thus the learned graph structure may not be optimal. To overcome the above drawbacks, we propose a robust bi-stochastic graph regularized matrix factorization (RBSMF) framework for data clustering. Specifically, we present a general loss function, which is more robust than the commonly used L-2 and L-1 functions. Besides, instead of keeping the graph fixed, we learn an adaptive similarity graph. Furthermore, the graph updating and matrix factorization are processed simultaneously, which can make the learned graph more appropriate for clustering. Extensive experiments have shown the proposed RBSMF outperforms other state-of-the-art methods.","[Wang, Qi; He, Xiang; Jiang, Xu; Li, Xuelong] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China; [Wang, Qi; He, Xiang; Jiang, Xu; Li, Xuelong] Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China",,"Li, XL (通讯作者)，Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.; Li, XL (通讯作者)，Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.",crabwq@gmail.com; xianghe@mail.nwpu.edu.cn; jx19961023@mail.nwpu.edu.cn; xuelong_li@nwpu.edu.cn,,,"National Key R&D Program of China [2017YFB1002202]; National Natural Science Foundation of China [U1864204, 61773316, U1801262, 61871470]",National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported by the National Key R&D Program of China under Grant 2017YFB1002202, National Natural Science Foundation of China under Grant U1864204, 61773316, U1801262, and 61871470.",,45,19,19,7,7,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,390,403,,10.1109/TPAMI.2020.3007673,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750807,,Y,N,2022-05-23,WOS:000728561300028
J,"Xia, SY; Peng, DW; Meng, DY; Zhang, CQ; Wang, GY; Giem, E; Wei, W; Chen, ZZ",,,,"Xia, Shuyin; Peng, Daowan; Meng, Deyu; Zhang, Changqing; Wang, Guoyin; Giem, Elisabeth; Wei, Wei; Chen, Zizhong",,,Ball k-Means: Fast Adaptive Clustering With No Bounds,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Ball k-means; k-means; ball cluster; stable area; active area; neighbor cluster,EFFICIENT,"This paper presents a novel accelerated exact k-means called as Ball k-means by using the ball to describe each cluster, which focus on reducing the point-centroid distance computation. The Ball k-means can exactly find its neighbor clusters for each cluster, resulting distance computations only between a point and its neighbor clusters' centroids instead of all centroids. What's more, each cluster can be divided into stable area and active area, and the latter one is further divided into some exact annular area. The assignment of the points in the stable area is not changed while the points in each annular area will be adjusted within a few neighbor clusters. There are no upper or lower bounds in the whole process. Moreover, ball k-means uses ball clusters and neighbor searching along with multiple novel stratagems for reducing centroid distance computations. In comparison with the current state-of-the art accelerated exact bounded methods, the Yinyang algorithm and the Exponion algorithm, as well as other top-of-the-line tree-based and bounded methods, the ball k-means attains both higher performance and performs fewer distance calculations, especially for large-k problems. The faster speed, no extra parameters and simpler design of Ball k-means make it an all-around replacement of the naive k-means.","[Xia, Shuyin; Peng, Daowan; Wang, Guoyin] Chongqing Univ Posts & Telecommun, Dept Chongqing Key Lab Computat Intelligence, Chongqing 400065, Peoples R China; [Meng, Deyu] Macau Univ Sci & Technol, Macau Inst Syst Engn, Taipa, Macau, Peoples R China; [Meng, Deyu] Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Peoples R China; [Zhang, Changqing] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China; [Giem, Elisabeth; Chen, Zizhong] Univ Calif Riverside, Dept Comp Sci & Engn, 900 Univ Ave, Riverside, CA 92521 USA; [Wei, Wei] Xian Univ Technol, Sch Comp Sci & Engn, Xian 710048, Peoples R China",,"Xia, SY; Wang, GY (通讯作者)，Chongqing Univ Posts & Telecommun, Dept Chongqing Key Lab Computat Intelligence, Chongqing 400065, Peoples R China.",xiasy@cqupt.edu.cn; daowan_peng@qq.com; dymeng@xjtu.edu.cn; zhangchangqing@tju.edu.cn; wanggy@cqupt.edu.cn; chen@cs.ucr.edu; weiwei@xaut.edu.cn; gieme01@ucr.edu,,"Xia, Shuyin/0000-0001-5993-9563","National Natural Science Foundation of China [61806030, 61936001, 11690011, 61721002, U1811461]; Natural Science Foundation of Chongqing [cstc2019jcyj-msxmX0485, cstc2019jcyj-cxttX0002]; National Key Research and Development Program of China [2019QY(Y)0301, 2016QY01W0200]; NICE: NRT for Integrated Computational Entomology, US National Natural Science [1631776]","National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Chongqing(Natural Science Foundation of Chongqing); National Key Research and Development Program of China; NICE: NRT for Integrated Computational Entomology, US National Natural Science","The authors greatly thank the handling associate editor and all anonymous reviewers for their valuable comments. This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61806030, 61936001, 11690011, 61721002, and U1811461, the Natural Science Foundation of Chongqing Nos. cstc2019jcyj-msxmX0485 and cstc2019jcyj-cxttX0002, the National Key Research and Development Program of China under Grant Nos. 2019QY(Y)0301 and 2016QY01W0200, and NICE: NRT for Integrated Computational Entomology, US National Natural Science award 1631776.",,40,4,4,6,6,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,87,99,,10.1109/TPAMI.2020.3008694,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750814,,Y,N,2022-05-23,WOS:000728561300008
J,"Xu, H; Ma, JY; Jiang, JJ; Guo, XJ; Ling, HB",,,,"Xu, Han; Ma, Jiayi; Jiang, Junjun; Guo, Xiaojie; Ling, Haibin",,,U2Fusion: A Unified Unsupervised Image Fusion Network,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image fusion; unified model; unsupervised learning; continual learning,MULTI-FOCUS IMAGE; FRAMEWORK,"This study proposes a novel unified and unsupervised end-to-end image fusion network, termed as U2Fusion, which is capable of solving different fusion problems, including multi-modal, multi-exposure, and multi-focus cases. Using feature extraction and information measurement, U2Fusion automatically estimates the importance of corresponding source images and comes up with adaptive information preservation degrees. Hence, different fusion tasks are unified in the same framework. Based on the adaptive degrees, a network is trained to preserve the adaptive similarity between the fusion result and source images. Therefore, the stumbling blocks in applying deep learning for image fusion, e.g., the requirement of ground-truth and specifically designed metrics, are greatly mitigated. By avoiding the loss of previous fusion capabilities when training a single model for different tasks sequentially, we obtain a unified model that is applicable to multiple fusion tasks. Moreover, a new aligned infrared and visible image dataset, RoadScene (available at https://github.com/hanna-xu/RoadScene), is released to provide a new option for benchmark evaluation. Qualitative and quantitative experimental results on three typical image fusion tasks validate the effectiveness and universality of U2Fusion. Our code is publicly available at https://github.com/hanna-xu/U2Fusion.","[Xu, Han; Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China; [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China; [Guo, Xiaojie] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China; [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA",,"Ma, JY (通讯作者)，Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.",xu_han@whu.edu.cn; jyma2010@gmail.com; jiangjunjun@hit.edu.cn; xj.max.guo@gmail.com; haibin.ling@gmail.com,"Guo, Xiaojie/AAC-3114-2022","Ma, Jiayi/0000-0003-3264-3265; Xu, Han/0000-0002-6291-2924","National Natural Science Foundation of China [61773295, 61971165, 61772512]; Natural Science Foundation of Hubei Province [2019CFA037]",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Hubei Province(Natural Science Foundation of Hubei Province),"This work was supported by the National Natural Science Foundation of China under Grants 61773295, 61971165 and 61772512, and the Natural Science Foundation of Hubei Province under Grant no. 2019CFA037.This work was supported by the National Natural Science Foundation of China under Grants 61773295, 61971165 and 61772512, and the Natural Science Foundation of Hubei Province under Grant no. 2019CFA037.",,50,100,101,78,115,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,502,518,,10.1109/TPAMI.2020.3012548,,,17,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750838,,Y,Y,2022-05-23,WOS:000728561300036
J,"Yang, H; Yan, D; Zhang, L; Sun, YD; Li, D; Maybank, SJ",,,,"Yang, Hao; Yan, Dan; Zhang, Li; Sun, Yunda; Li, Dong; Maybank, Stephen J.",,,Feedback Graph Convolutional Network for Skeleton-Based Action Recognition,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Skeleton; Feature extraction; Joints; Semantics; Predictive models; Data models; Convolution; Feedback mechanism; graph convolutional network; skeleton; action recognition,,"Skeleton-based action recognition has attracted considerable attention since the skeleton data is more robust to the dynamic circumstances and complicated backgrounds than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks for which it is impossible for the shallower layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces a feedback mechanism into GCNs for action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) A multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse to fine process; (2) A Feedback Graph Convolutional Block (FGCB) is proposed to introduce dense feedback connections into the GCNs. It transmits the high-level semantic features to the shallower layers and conveys temporal information stage by stage to model video level spatial-temporal features for action recognition; (3) The FGCN model provides predictions on-the-fly. In the early stages, its predictions are relatively coarse. These coarse predictions are treated as priors to guide the feature learning in later stages, to obtain more accurate predictions. Extensive experiments on three datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on all three datasets.","[Yang, Hao; Yan, Dan; Sun, Yunda] Nuctech Co Ltd, R&D Ctr Artificial Intelligent, Beijing 100190, Peoples R China; [Zhang, Li; Li, Dong] Tsinghua Univ, Dept Engn Phys, Beijing 100084, Peoples R China; [Maybank, Stephen J.] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HU, England",,"Yang, H (通讯作者)，Nuctech Co Ltd, R&D Ctr Artificial Intelligent, Beijing 100190, Peoples R China.",yanghao1@nuctech.com; yandan@nuctech.com; zli@mailisinghua.edu.cn; sunyunda@nuctech.com; lid19@mail.tsinghua.edu.cn; steve.maybank@bbk.ac.uk,,"Yang, Hao/0000-0003-3590-4491",,,,,71,5,5,21,21,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2022,31,,,,,,164,175,,10.1109/TIP.2021.3129117,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XI0CQ,34818190,"Green Accepted, Green Submitted",Y,N,2022-05-23,WOS:000725791700001
J,"Zhao, J; Yan, SC; Feng, JS",,,,"Zhao, Jian; Yan, Shuicheng; Feng, Jiashi",,,Towards Age-Invariant Face Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Age-invariant face recognition; age-invariant model; generative adversarial networks; benchmark dataset,VERIFICATION; PATTERNS; SHAPE,"Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, effective and novel training strategies are developed for end-to-end learning of the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we construct a new large-scaleCross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR dataset and several other cross-age datasets (MORPH, CACD, and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on the popular unconstrained face recognition datasets YTFand IJB-C additionally verifies its promising generalization ability in recognizing faces in the wild.","[Zhao, Jian] Inst North Elect Equipment, Beijing 100191, Peoples R China; [Yan, Shuicheng] Yitu Technol, Beijing 100000, Peoples R China; [Feng, Jiashi] Natl Univ Singapore, Singapore 119077, Singapore",,"Zhao, J (通讯作者)，Inst North Elect Equipment, Beijing 100191, Peoples R China.",zhaojian90@u.nus.edu; shuicheng.yan@gmail.com; elefjia@nus.edu.sg,"Feng, Jiashi/AGX-6209-2022","Zhao, Jian/0000-0002-3508-756X",NUS [IDS R-263-000-C67-646]; MOE [R-263-000-D17-112]; ECRA [R-263-000-C87-133],NUS(National University of Singapore); MOE(Ministry of Higher Education & Scientific Research (MHESR)); ECRA,"The work of Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.",,81,14,14,13,17,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,474,487,,10.1109/TPAMI.2020.3011426,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750831,,Y,N,2022-05-23,WOS:000728561300034
J,"Zhu, LC; Yang, Y",,,,"Zhu, Linchao; Yang, Yi",,,Label Independent Memory for Semi-Supervised Few-Shot Video Classification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Few-shot video classification; semi-supervised learning; memory-augmented neural networks; compound memory networks,,"In this paper, we propose to leverage freely available unlabeled video data to facilitate few-shot video classification. In this semi-supervised few-shot video classification task, millions of unlabeled data are available for each episode during training. These videos can be extremely imbalanced, while they have profound visual and motion dynamics. To tackle the semi-supervised few-shot video classification problem, we make the following contributions. First, we propose a label independent memory (LIM) to cache label related features, which enables a similarity search over a large set of videos. LIM produces a class prototype for few-shot training. This prototype is an aggregated embedding for each class, which is more robust to noisy video features. Second, we integrate a multi-modality compound memory network to capture both RGB and flow information. We propose to store the RGB and flow representation in two separate memory networks, but they are jointly optimized via a unified loss. In this way, mutual communications between the two modalities are leveraged to achieve better classification performance. Third, we conduct extensive experiments on the few-shot Kinetics-100, Something-Something-100 datasets, which validates the effectiveness of leveraging the accessible unlabeled data for few-shot classification.","[Zhu, Linchao; Yang, Yi] Univ Technol Sydney, Ultimo, NSW 2007, Australia",,"Yang, Y (通讯作者)，Univ Technol Sydney, Ultimo, NSW 2007, Australia.",linchao.zhu@uts.edu.au; yi.yang@uts.edu.au,"Yang, Yi/B-9273-2017","Yang, Yi/0000-0002-0512-880X; Zhu, Linchao/0000-0002-4093-7557",ARC [DP200100938],ARC(Australian Research Council),This work was partially supported by ARC DP200100938.,,50,17,17,9,12,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2022,44,1,,,,,273,285,,10.1109/TPAMI.2020.3007511,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,XM0XY,32750804,,Y,N,2022-05-23,WOS:000728561300020
J,"Guo, YL; Wang, HY; Hu, QY; Liu, H; Liu, L; Bennamoun, M",,,,"Guo, Yulan; Wang, Hanyun; Hu, Qingyong; Liu, Hao; Liu, Li; Bennamoun, Mohammed",,,Deep Learning for 3D Point Clouds: A Survey,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Three-dimensional displays; Shape; Deep learning; Object detection; Laser radar; Task analysis; Sensors; Deep learning; point clouds; 3D data; shape classification; shape retrieval; object detection; object tracking; scene flow; instance segmentation; semantic segmentation; part segmentation,OBJECT DETECTION; NETWORKS; SEGMENTATION; CLASSIFICATION; RECOGNITION,"Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.","[Guo, Yulan; Liu, Hao] Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510275, Peoples R China; [Guo, Yulan] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha, Peoples R China; [Wang, Hanyun] Informat Engn Univ, Sch Surveying & Mapping, Zhengzhou 450001, Peoples R China; [Hu, Qingyong] Univ Oxford, Dept Comp Sci, Oxford OX1 3PP, England; [Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China; [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90570, Finland; [Bennamoun, Mohammed] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia",,"Guo, YL (通讯作者)，Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510275, Peoples R China.",yulan.guo@nudt.edu.cn; why.scholar@gmail.com; qingyong.hu@cs.ox.ac.uk; liuh327@mail2.sysu.edu.cn; lilyliu_nudt@163.com; mohammed.bennamoun@uwa.edu.au,"; Bennamoun, Mohammed/C-2789-2013; /E-7102-2014","Hanyun, Wang/0000-0002-8320-4230; Liu, li/0000-0002-2011-2873; Bennamoun, Mohammed/0000-0002-6603-3257; hu, qing yong/0000-0003-0337-9207; /0000-0001-7051-841X","National Natural Science Foundation of China [61972435, 61602499, 61872379]; Natural Science Foundation of Guangdong Province [2019A1515011271]; Science and Technology Innovation Committee of Shenzhen Municipality [JCYJ20190807152209394]; Australian Research Council [DP150100294, DP150104251]; China Scholarship Council (CSC); Academy of Finland",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Science and Technology Innovation Committee of Shenzhen Municipality; Australian Research Council(Australian Research Council); China Scholarship Council (CSC)(China Scholarship Council); Academy of Finland(Academy of Finland),"This work was partially supported by the National Natural Science Foundation of China (No. 61972435, 61602499, 61872379), the Natural Science Foundation of Guangdong Province (2019A1515011271), the Science and Technology Innovation Committee of Shenzhen Municipality (JCYJ20190807152209394), the Australian Research Council (Grants DP150100294 and DP150104251), the China Scholarship Council (CSC) and the Academy of Finland. Yulan Guo, Hanyun Wang, Qingyong Hu and Hao Liu have equal contribution to this work and are co-first authors.",,254,256,256,288,396,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,DEC 1,2021,43,12,,,,,4338,4364,,10.1109/TPAMI.2020.3005434,,,27,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,WR0MQ,32750799,"Green Accepted, Green Submitted",Y,Y,2022-05-23,WOS:000714203900015
J,"Jing, LL; Tian, YL",,,,"Jing, Longlong; Tian, Yingli",,,Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Task analysis; Visualization; Videos; Training; Learning systems; Feature extraction; Annotations; Self-supervised learning; unsupervised learning; convolutional neural network; transfer learning; deep learning,CLASSIFICATION; MACHINE; SCENES,"Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.","[Jing, Longlong] CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10016 USA; [Tian, Yingli] CUNY City Coll, Dept Elect Engn, New York, NY 10031 USA; [Tian, Yingli] CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10031 USA",,"Tian, YL (通讯作者)，CUNY City Coll, Dept Elect Engn, New York, NY 10031 USA.; Tian, YL (通讯作者)，CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10031 USA.",ljing@gradcenter.cuny.edu; ytian@ccny.cuny.edu,,,National Science Foundation [IIS-1400802],National Science Foundation(National Science Foundation (NSF)),This material is based upon work supported by the National Science Foundation under award number IIS-1400802.,,186,159,160,15,15,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,NOV 1,2021,43,11,,,,,4037,4058,,10.1109/TPAMI.2020.2992393,,,22,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,WA1JH,32386141,Green Submitted,Y,N,2022-05-23,WOS:000702649700025
J,"Kobyzev, I; Prince, SJD; Brubaker, MA",,,,"Kobyzev, Ivan; Prince, Simon J. D.; Brubaker, Marcus A.",,,Normalizing Flows: An Introduction and Review of Current Methods,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Review,,,,,,Estimation; Jacobian matrices; Mathematical model; Training; Computational modeling; Context modeling; Random variables; Generative models; normalizing flows; density estimation; variational inference; invertible neural networks,DENSITY-ESTIMATION,"Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.","[Kobyzev, Ivan; Prince, Simon J. D.; Brubaker, Marcus A.] Borealis AI, Montreal, PQ H2S 3H1, Canada",,"Kobyzev, I (通讯作者)，Borealis AI, Montreal, PQ H2S 3H1, Canada.",ivan.kobyzev@borealisai.com; simon.prince@borealisai.com; mab@eecs.yorku.ca,,,,,,,108,73,73,17,23,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,NOV 1,2021,43,11,,,,,3964,3979,,10.1109/TPAMI.2020.2992934,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,WA1JH,32396070,Green Submitted,Y,Y,2022-05-23,WOS:000702649700020
J,"Li, JJ; Chen, EP; Ding, ZM; Zhu, L; Lu, K; Shen, HT",,,,"Li, Jingjing; Chen, Erpeng; Ding, Zhengming; Zhu, Lei; Lu, Ke; Shen, Heng Tao",,,Maximum Density Divergence for Domain Adaptation,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Measurement; Training; Kernel; Task analysis; Adaptation models; Benchmark testing; Games; Domain adaptation; transfer learning; adversarial learning,,"Unsupervised domain adaptation addresses the problem of transferring knowledge from a well-labeled source domain to an unlabeled target domain where the two domains have distinctive data distributions. Thus, the essence of domain adaptation is to mitigate the distribution divergence between the two domains. The state-of-the-art methods practice this very idea by either conducting adversarial training or minimizing a metric which defines the distribution gaps. In this paper, we propose a new domain adaptation method named adversarial tight match (ATM) which enjoys the benefits of both adversarial training and metric learning. Specifically, at first, we propose a novel distance loss, named maximum density divergence (MDD), to quantify the distribution divergence. MDD minimizes the inter-domain divergence (match in ATM) and maximizes the intra-class density (tight in ATM). Then, to address the equilibrium challenge issue in adversarial domain adaptation, we consider leveraging the proposed MDD into adversarial domain adaptation framework. At last, we tailor the proposed MDD as a practical learning loss and report our ATM. Both empirical evaluation and theoretical analysis are reported to verify the effectiveness of the proposed method. The experimental results on four benchmarks, both classical and large-scale, show that our method is able to achieve new state-of-the-art performance on most evaluations.","[Li, Jingjing; Chen, Erpeng; Lu, Ke; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China; [Ding, Zhengming] Indiana Univ Purdue Univ, Dept Comp Informat & Technol, Indianapolis, IN 46202 USA; [Zhu, Lei] Shandong Normal Univ, Jinan 250014, Peoples R China",,"Li, JJ (通讯作者)，Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.",lijin117@yeah.net; cep1126@163.com; zd2@iu.edu; leizhu0608@gmail.com; kel@uestc.edu.cn; shenhengtao@hotmail.com,"Shen, Heng Tao/ABD-5331-2021",,"National Key R&D Program of China [2018YFE0203900]; National Natural Science Foundation of China [61806039, 61832001]; Sichuan Department of Science and Technology [20ZDYF2771]",National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Sichuan Department of Science and Technology,"This work was supported in part by the National Key R&D Program of China under Grant 2018YFE0203900, in part by the National Natural Science Foundation of China under Grant 61806039 and Grant 61832001, and in part by the Sichuan Department of Science and Technology under Grant 20ZDYF2771.",,44,29,29,15,15,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,NOV 1,2021,43,11,,,,,3918,3930,,10.1109/TPAMI.2020.2991050,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,WA1JH,32356736,Green Submitted,Y,N,2022-05-23,WOS:000702649700017
J,"Yang, WH; Tan, RT; Wang, SQ; Fang, YM; Liu, JY",,,,"Yang, Wenhan; Tan, Robby T.; Wang, Shiqi; Fang, Yuming; Liu, Jiaying",,,Single Image Deraining: From Model-Based to Data-Driven and Beyond,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Rain; Atmospheric modeling; Videos; Image coding; Visualization; Degradation; Machine learning; Rain streak removal; single image; model-based; data-driven,QUALITY ASSESSMENT; RAIN; REMOVAL; CONTEXT; NETWORK; SPARSE,"The goal of single-image deraining is to restore the rain-free background scenes of an image degraded by rain streaks and rain accumulation. The early single-image deraining methods employ a cost function, where various priors are developed to represent the properties of rain and background layers. Since 2017, single-image deraining methods step into a deep-learning era, and exploit various types of networks, i.e., convolutional neural networks, recurrent neural networks, generative adversarial networks, etc., demonstrating impressive performance. Given the current rapid development, in this paper, we provide a comprehensive survey of deraining methods over the last decade. We summarize the rain appearance models, and discuss two categories of deraining approaches: model-based and data-driven approaches. For the former, we organize the literature based on their basic models and priors. For the latter, we discuss the developed ideas related to architectures, constraints, loss functions, and training datasets. We present milestones of single-image deraining methods, review a broad selection of previous works in different categories, and provide insights on the historical development route from the model-based to data-driven methods. We also summarize performance comparisons quantitatively and qualitatively. Beyond discussing the technicality of deraining methods, we also discuss the future possible directions.","[Yang, Wenhan; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China; [Tan, Robby T.] Yale NUS Coll, Singapore 138527, Singapore; [Tan, Robby T.] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore; [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330013, Jiangxi, Peoples R China; [Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China",,"Yang, WH (通讯作者)，City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.",wyang34@cityu.edu.hk; tanrobby@gmail.com; shiqwang@cityu.edu.hk; fa0001ng@e.ntu.edu.sg; liujiaying@pku.edu.cn,,,National Key Research and Development Program of China [2018AA A0102702]; Hong Kong RGC Early Career Scheme [9048122 (CityU 21211018)]; National Natural Science Foundation of China [61772043]; Beijing Natural Science Foundation [L182002];  [MOE2019-T2-1-130],National Key Research and Development Program of China; Hong Kong RGC Early Career Scheme; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); ,"This work was supported by the National Key Research and Development Program of China under Grant 2018AA A0102702, the Hong Kong RGC Early Career Scheme under Grant 9048122 (CityU 21211018), National Natural Science Foundation of China under contractNo.61772043, and Beijing Natural Science Foundation under contract No.L182002. The work of Robby T. Tan was supported by MOE2019-T2-1-130.",,64,49,50,22,33,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,NOV 1,2021,43,11,,,,,4059,4077,,10.1109/TPAMI.2020.2995190,,,19,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,WA1JH,32750766,Green Submitted,Y,Y,2022-05-23,WOS:000702649700026
J,"Geng, CX; Huang, SJ; Chen, SC",,,,"Geng, Chuanxing; Huang, Sheng-Jun; Chen, Songcan",,,Recent Advances in Open Set Recognition: A Survey,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Training; Testing; Task analysis; Semantics; Face recognition; Data visualization; Open set recognition; classification; open world recognition; zero-short learning; one-shot learning,SUPPORT VECTOR MACHINE; FACE RECOGNITION; SPARSE REPRESENTATION; CLASSIFICATION; CLASSIFIERS; MODELS; STEPS,"In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers to not only accurately classify the seen classes, but also effectively deal with unseen ones. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, evaluation criteria, and algorithm comparisons. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also review the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field.","[Geng, Chuanxing; Huang, Sheng-Jun; Chen, Songcan] Nanjing Univ Aeronaut & Astronaut NUAA, Coll Comp Sci & Technol, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China",,"Chen, SC (通讯作者)，Nanjing Univ Aeronaut & Astronaut NUAA, Coll Comp Sci & Technol, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.",gengchuanxing@nuaa.edu.cn; huangsj@nuaa.edu.cn; s.chen@nuaa.edu.cn,,"Geng, Chuanxing/0000-0001-6345-5385",NSFC [61672281]; Key Program of NSFC [61732006]; Postgraduate Research & Practice Innovation Program of Jiangsu Province,NSFC(National Natural Science Foundation of China (NSFC)); Key Program of NSFC(National Natural Science Foundation of China (NSFC)); Postgraduate Research & Practice Innovation Program of Jiangsu Province,"The authors would like to thank the support from NSFC under Grant No. 61672281, the Key Program of NSFC under Grant No. 61732006 and the Postgraduate Research & Practice Innovation Program of Jiangsu Province under grant no. KYCX18_0306.",,176,75,76,31,71,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,OCT 1,2021,43,10,,,,,3614,3631,,10.1109/TPAMI.2020.2981604,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UK8RG,32191881,Green Submitted,Y,Y,2022-05-23,WOS:000692232400026
J,"Oksuz, K; Cam, BC; Kalkan, S; Akbas, E",,,,"Oksuz, Kemal; Cam, Baris Can; Kalkan, Sinan; Akbas, Emre",,,Imbalance Problems in Object Detection: A Review,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Review,,,,,,Object detection; Taxonomy; Feature extraction; Deep learning; Pipelines; Neural networks; Pattern analysis; Object detection; imbalance; class imbalance; scale imbalance; spatial imbalance; objective imbalance,VEHICLE DETECTION; REGION PROPOSAL; FACE DETECTION,"In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: https://github.com/kemaloksuz/ObjectDetectionImbalance.","[Oksuz, Kemal; Cam, Baris Can; Kalkan, Sinan; Akbas, Emre] Middle East Tech Univ METU, Dept Comp Engn, TR-06800 Ankara, Turkey",,"Oksuz, K (通讯作者)，Middle East Tech Univ METU, Dept Comp Engn, TR-06800 Ankara, Turkey.",kemal.oksuz@metu.edu.tr; can.cam@metu.edu.tr; skalkan@metu.edu.tr; emre@ceng.metu.edu.tr,"Akbas, Emre/B-6857-2008","Akbas, Emre/0000-0002-3760-6722; Cam, Baris Can/0000-0001-8480-4636",Scientific and Technological Research Council of Turkey (TUBITAK) through the project titled Object Detection in Videos with Deep Neural Networks [117E054]; TUBITAK 2211-A National Scholarship Programme,Scientific and Technological Research Council of Turkey (TUBITAK) through the project titled Object Detection in Videos with Deep Neural Networks(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK)); TUBITAK 2211-A National Scholarship Programme(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK)),This work was supported in part by the Scientific and Technological Research Council of Turkey (TUBITAK) through the project titled Object Detection in Videos with Deep Neural Networks (grant number 117E054). The work of Kemal Oksuz was supported by the TUBITAK 2211-A National Scholarship Programme for Ph.D. students. Sinan Kalkan and Emre Akbas contributed equally to this work.,,135,68,70,124,188,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,OCT 1,2021,43,10,,,,,3388,3415,,10.1109/TPAMI.2020.2981890,,,28,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UK8RG,32191882,Green Submitted,Y,Y,2022-05-23,WOS:000692232400012
J,"Wang, JD; Sun, K; Cheng, TH; Jiang, BR; Deng, CR; Zhao, Y; Liu, D; Mu, YD; Tan, MK; Wang, XG; Liu, WY; Xiao, B",,,,"Wang, Jingdong; Sun, Ke; Cheng, Tianheng; Jiang, Borui; Deng, Chaorui; Zhao, Yang; Liu, Dong; Mu, Yadong; Tan, Mingkui; Wang, Xinggang; Liu, Wenyu; Xiao, Bin",,,Deep High-Resolution Representation Learning for Visual Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Spatial resolution; Semantics; Object detection; Pose estimation; Convolutional codes; Indexes; Image segmentation; HRNet; high-resolution representations; low-resolution representations; human pose estimation; semantic segmentation; object detection,NETWORK,"High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.","[Wang, Jingdong] Microsoft Res, Visual Comp Grp, Beijing 100080, Peoples R China; [Sun, Ke; Liu, Dong] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China; [Cheng, Tianheng; Wang, Xinggang; Liu, Wenyu] Huazhong Univ Sci & Technol, Wuhan 430074, Hubei, Peoples R China; [Jiang, Borui] Peking Univ, Beijing 100871, Peoples R China; [Mu, Yadong] Peking Univ, Inst Comp Sci & Technol, Machine Intelligence Lab, Beijing 100871, Peoples R China; [Deng, Chaorui] South China Univ Technol, Guangzhou 510641, Guangdong, Peoples R China; [Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China; [Zhao, Yang] Griffith Univ, Nathan, Qld 4111, Australia; [Xiao, Bin] Microsoft, Redmond, WA 98052 USA",,"Wang, JD (通讯作者)，Microsoft Res, Visual Comp Grp, Beijing 100080, Peoples R China.",jingdw@microsoft.com; sunk@mail.ustc.edu.cn; thch@hust.edu.cn; jbr@pku.edu.cn; c.r.deng2012@gmail.com; yang.zhao4@griffithuni.edu.au; dongeliu@ustc.edu.cn; muyadong@gmail.com; mingkuitan@scut.edu.cn; xgwang@hust.edu.cn; liuwy@hust.edu.cn; leoxiaobin@gmail.com,"Liu, Wenyu/AAG-1426-2019; Xiao, Bin/AAL-3936-2020","Liu, Wenyu/0000-0002-4582-7488; Xiao, Bin/0000-0001-6477-5911",,,,,156,263,264,151,151,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,OCT 1,2021,43,10,,,,,3349,3364,,10.1109/TPAMI.2020.2983686,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UK8RG,32248092,"Green Published, Green Submitted",Y,Y,2022-05-23,WOS:000692232400010
J,"Wang, ZH; Chen, J; Hoi, SCH",,,,"Wang, Zhihao; Chen, Jian; Hoi, Steven C. H.",,,Deep Learning for Image Super-Resolution: A Survey,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Deep learning; Degradation; Animals; Benchmark testing; Measurement; Image super-resolution; deep learning; convolutional neural networks (CNN); Generative adversarial nets (GAN),VIDEO SUPERRESOLUTION; QUALITY ASSESSMENT; RESOLUTION; NETWORKS; CLASSIFICATION,"Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.","[Wang, Zhihao] South China Univ Technol, Guangzhou, Guangdong, Peoples R China; [Chen, Jian] South China Univ Technol, Sch Software Engn, Guangzhou, Guangdong, Peoples R China; [Hoi, Steven C. H.] Salesforce Com, Salesforce Res Asia, Singapore 038985, Singapore; [Hoi, Steven C. H.] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore",,"Hoi, SCH (通讯作者)，Salesforce Com, Salesforce Res Asia, Singapore 038985, Singapore.; Hoi, SCH (通讯作者)，Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.",ptkin@outlook.com; ellachen@scut.edu.cn; shoi@salesforce.com,,,"Guangdong special branch plans young talent with scientific and technological innovation [2016TQ03X445]; Guangzhou science and technology planning project [201904010197]; Natural Science Foundation of Guangdong Province, China [2016A030313437]","Guangdong special branch plans young talent with scientific and technological innovation; Guangzhou science and technology planning project; Natural Science Foundation of Guangdong Province, China(National Natural Science Foundation of Guangdong Province)","Prof. Jian Chen is supported by the Guangdong special branch plans young talent with scientific and technological innovation (Grant No. 2016TQ03X445), the Guangzhou science and technology planning project (Grant No. 201904010197) and Natural Science Foundation of Guangdong Province, China (2016A030313437).",,211,163,164,153,153,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,OCT 1,2021,43,10,,,,,3365,3387,,10.1109/TPAMI.2020.2982166,,,23,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UK8RG,32217470,"Green Submitted, Green Accepted",Y,Y,2022-05-23,WOS:000692232400011
J,"Berman, D; Levy, D; Avidan, S; Treibitz, T",,,,"Berman, Dana; Levy, Deborah; Avidan, Shai; Treibitz, Tali",,,Underwater Single Image Color Restoration Using Haze-Lines and a New Quantitative Dataset,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image color analysis; Attenuation; Image restoration; Channel estimation; Three-dimensional displays; Cameras; Optical attenuators; Image processing and computer vision; image enhancement; computational photography; image restoration; image color analysis,ENHANCEMENT; VISIBILITY; LIGHT; WATER,"Underwater images suffer from color distortion and low contrast, because light is attenuated while it propagates through water. Attenuation under water varies with wavelength, unlike terrestrial images where attenuation is assumed to be spectrally uniform. The attenuation depends both on the water body and the 3D structure of the scene, making color restoration difficult. Unlike existing single underwater image enhancement techniques, our method takes into account multiple spectral profiles of different water types. By estimating just two additional global parameters: the attenuation ratios of the blue-red and blue-green color channels, the problem is reduced to single image dehazing, where all color channels have the same attenuation coefficients. Since the water type is unknown, we evaluate different parameters out of an existing library of water types. Each type leads to a different restored image and the best result is automatically chosen based on color distribution. We also contribute a dataset of 57 images taken in different locations. To obtain ground truth, we placed multiple color charts in the scenes and calculated its 3D structure using stereo imaging. This dataset enables a rigorous quantitative evaluation of restoration algorithms on natural images for the first time.","[Berman, Dana; Avidan, Shai] Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel; [Levy, Deborah; Treibitz, Tali] Univ Haifa, Charney Sch Marine Sci, Hatter Dept Marine Technol, IL-3498838 Haifa, Israel",,"Berman, D (通讯作者)，Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.",dana.menaker@gmail.com; dlrun14@gmail.com; avidan@eng.tau.ac.il; ttreibitz@univ.haifa.ac.il,,,"Leona M. and Harry B. Helmsley Charitable Trust; Maurice Hatter Foundation; Israel Science Foundation [680/18]; Ministry of Science, Technology and Space grant [3 - 12487]; Technion Ollendorff Minerva Center for Vision and Image Sciences; Mediterranean Sea Research Center of Israel; Apple Graduate Fellowship","Leona M. and Harry B. Helmsley Charitable Trust; Maurice Hatter Foundation; Israel Science Foundation(Israel Science Foundation); Ministry of Science, Technology and Space grant; Technion Ollendorff Minerva Center for Vision and Image Sciences; Mediterranean Sea Research Center of Israel; Apple Graduate Fellowship","TT was supported by the The Leona M. and Harry B. Helmsley Charitable Trust, The Maurice Hatter Foundation, Israel Science Foundation Grant #680/18, Ministry of Science, Technology and Space grant #3 - 12487, and the Technion Ollendorff Minerva Center for Vision and Image Sciences. DB was supported by The Mediterranean Sea Research Center of Israel and by Apple Graduate Fellowship.",,55,57,57,45,90,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2822,2837,,10.1109/TPAMI.2020.2977624,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32142424,Green Submitted,Y,Y,2022-05-23,WOS:000670578800022
J,"Liu, XW; Li, MM; Tang, C; Xia, JY; Xiong, J; Liu, L; Kloft, M; Zhu, E",,,,"Liu, Xinwang; Li, Miaomiao; Tang, Chang; Xia, Jingyuan; Xiong, Jian; Liu, Li; Kloft, Marius; Zhu, En",,,Efficient and Effective Regularized Incomplete Multi-View Clustering,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Multiple kernel clustering; multiple view learning; incomplete kernel learning,,"Incomplete multi-view clustering (IMVC) optimally combines multiple pre-specified incomplete views to improve clustering performance. Among various excellent solutions, the recently proposed multiple kernel k-means with incomplete kernels (MKKM-IK) forms a benchmark, which redefines IMVC as a joint optimization problem where the clustering and kernel matrix imputation tasks are alternately performed until convergence. Though demonstrating promising performance in various applications, we observe that the manner of kernel matrix imputation in MKKM-IK would incur intensive computational and storage complexities, over-complicated optimization and limitedly improved clustering performance. In this paper, we first propose an Efficient and Effective Incomplete Multi-view Clustering (EE-IMVC) algorithm to address these issues. Instead of completing the incomplete kernel matrices, EE-IMVC proposes to impute each incomplete base matrix generated by incomplete views with a learned consensus clustering matrix. Moreover, we further improve this algorithm by incorporating prior knowledge to regularize the learned consensus clustering matrix. Two three-step iterative algorithms are carefully developed to solve the resultant optimization problems with linear computational complexity, and their convergence is theoretically proven. After that, we theoretically study the generalization bound of the proposed algorithms. Furthermore, we conduct comprehensive experiments to study the proposed algorithms in terms of clustering accuracy, evolution of the learned consensus clustering matrix and the convergence. As indicated, our algorithms deliver their effectiveness by significantly and consistently outperforming some state-of-the-art ones.","[Liu, Xinwang; Zhu, En] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China; [Li, Miaomiao] Changsha Coll, Dept Comp, Changsha 410073, Hunan, Peoples R China; [Tang, Chang] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Hubei, Peoples R China; [Xia, Jingyuan] Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England; [Xiong, Jian] Southwestern Univ Finance & Econ, Sch Business Adm, Chengdu 611130, Sichuan, Peoples R China; [Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China; [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland; [Kloft, Marius] Tech Univ Kaiserslautern, Dept Comp Sci, D-67653 Kaiserslautern, Germany",,"Liu, XW (通讯作者)，Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.",xinwangliu@nudt.edu.cn; miaomiaolinudt@gmail.com; tangchang@cug.edu.cn; j.xia16@imperial.ac.uk; xiongjian2017@swufe.edu.cn; li.liu@oulu.fi; kloft@cs.uni-kl.de; enzhu@nudt.edu.cn,"Xia, Jingyuan/ABE-7885-2021; Tang, Chang/AAU-8995-2020","Tang, Chang/0000-0002-6515-7696","Natural Science Foundation of China [61773392, 61922088, 61701451]",Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported by the Natural Science Foundation of China (project no. 61773392, 61922088 and 61701451).",,34,27,27,14,30,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2634,2646,,10.1109/TPAMI.2020.2974828,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32086196,Green Accepted,Y,N,2022-05-23,WOS:000670578800009
J,"Sam, DB; Peri, SV; Sundararaman, MN; Kamath, A; Babu, RV",,,,"Sam, Deepak Babu; Peri, Skand Vishwanath; Sundararaman, Mukuntha Narayanan; Kamath, Amogh; Babu, R. Venkatesh",,,"Locate, Size, and Count: Accurately Resolving People in Dense Crowds via Detection",IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Training; Detectors; Magnetic heads; Face; Feature extraction; Task analysis; Crowd counting; head detection; deep learning,HUMANS; IMAGE,"We introduce a detection framework for dense crowd counting and eliminate the need for the prevalent density regression paradigm. Typical counting models predict crowd density for an image as opposed to detecting every person. These regression methods, in general, fail to localize persons accurate enough for most applications other than counting. Hence, we adopt an architecture that locates every person in the crowd, sizes the spotted heads with bounding box and then counts them. Compared to normal object or face detectors, there exist certain unique challenges in designing such a detection system. Some of them are direct consequences of the huge diversity in dense crowds along with the need to predict boxes contiguously. We solve these issues and develop our LSC-CNN model, which can reliably detect heads of people across sparse to dense crowds. LSC-CNN employs a multi-column architecture with top-down feature modulation to better resolve persons and produce refined predictions at multiple resolutions. Interestingly, the proposed training regime requires only point head annotation, but can estimate approximate size information of heads. We show that LSC-CNN not only has superior localization than existing density regressors, but outperforms in counting as well. The code for our approach is available at https://github.com/val-iisc/lsc-cnn.","[Sam, Deepak Babu; Peri, Skand Vishwanath; Sundararaman, Mukuntha Narayanan; Kamath, Amogh; Babu, R. Venkatesh] Indian Inst Sci, Dept Computat & Data Sci, Video Analyt Lab, Bangalore 560012, Karnataka, India",,"Sam, DB (通讯作者)，Indian Inst Sci, Dept Computat & Data Sci, Video Analyt Lab, Bangalore 560012, Karnataka, India.",deepaksam@iisc.ac.in; pvskand@pm.me; muks14x@gmail.com; amogh30@gmail.com; venky@iisc.ac.in,,,"SERB, Dept. of Science and Technology [SB/S3/EECE/0127/2015]","SERB, Dept. of Science and Technology","This work was supported by SERB, Dept. of Science and Technology, Govt. of India (Proj: SB/S3/EECE/0127/2015). Deepak Babu Sam and Skand Vishwanath Peri contributed equally to this work.",,45,29,29,7,17,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2739,2751,,10.1109/TPAMI.2020.2974830,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32086197,,Y,Y,2022-05-23,WOS:000670578800016
J,"Shi, SS; Wang, Z; Shi, JP; Wang, XG; Li, HS",,,,"Shi, Shaoshuai; Wang, Zhe; Shi, Jianping; Wang, Xiaogang; Li, Hongsheng",,,From Points to Parts: 3D Object Detection From Point Cloud With Part-Aware and Part-Aggregation Network,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,3D object detection; point cloud; part location; LiDAR; convolutional neural network; autonomous driving,,"3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-A(2) net). The whole framework consists of the part-aware stage and the part-aggregation stage. First, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-A(2) net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data.","[Shi, Shaoshuai; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Wang, Zhe; Shi, Jianping] SenseTime Res, Beijing, Peoples R China",,"Li, HS (通讯作者)，Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.",ssshi@ee.cuhk.edu.hk; wangzhe@sensetime.com; shijianping@sensetime.com; xgwang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk,"Wang, Zhe/ABG-6377-2021; Shi, Shaoshuai/AAV-3211-2021","Wang, Zhe/0000-0002-0597-4475; ",,,,,63,85,86,56,78,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2647,2664,,10.1109/TPAMI.2020.2977026,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32142423,Green Submitted,Y,N,2022-05-23,WOS:000670578800010
J,"Wang, QL; Xie, JT; Zuo, WM; Zhang, L; Li, PH",,,,"Wang, Qilong; Xie, Jiangtao; Zuo, Wangmeng; Zhang, Lei; Li, Peihua",,,Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Global covariance pooling; matrix power normalization; deep convolutional neural networks; visual recognition,SHRINKAGE,"Compared with global average pooling in existing deep convolutional neural networks (CNNs), global covariance pooling can capture richer statistics of deep features, having potential for improving representation and generalization abilities of deep CNNs. However, integration of global covariance pooling into deep CNNs brings two challenges: (1) robust covariance estimation given deep features of high dimension and small sample size; (2) appropriate usage of geometry of covariances. To address these challenges, we propose a global Matrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a robust covariance estimator, very suitable for scenario of high dimension and small sample size. It can also be regarded as Power-Euclidean metric between covariances, effectively exploiting their geometry. Furthermore, a global Gaussian embedding network is proposed to incorporate first-order statistics into MPN-COV. For fast training of MPN-COV networks, we implement an iterative matrix square root normalization, avoiding GPU unfriendly eigen-decomposition inherent in MPN-COV. Additionally, progressive 1 x 1 convolutions and group convolution are introduced to compress covariance representations. The proposed methods are highly modular, readily plugged into existing deep CNNs. Extensive experiments are conducted on large-scale object classification, scene categorization, fine-grained visual recognition and texture classification, showing our methods outperform the counterparts and obtain state-of-the-art performance.","[Wang, Qilong] Tianjin Univ, Tianjin Key Lab Machine Learning, Coll Intelligence & Comp, Tianjin 300350, Peoples R China; [Wang, Qilong; Xie, Jiangtao; Li, Peihua] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Liaoning, Peoples R China; [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China; [Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China",,"Li, PH (通讯作者)，Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Liaoning, Peoples R China.",qlwang@tju.edu.cn; jiangtaoxie@mail.dlut.edu.cn; cswmzuo@gmail.com; cslzhang@comp.polyu.edu.hk; peihuali@dlut.edu.cn,,,"National Natural Science Foundation of China [61971086, 61806140, U19A2073, 61471082, 61671182, 61732011]; China Post-doctoral Programme Foundation for Innovative Talent",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Post-doctoral Programme Foundation for Innovative Talent,"The work was supported by the National Natural Science Foundation of China (Grant No. 61971086, 61806140, U19A2073, 61471082, 61671182 and 61732011). Qilong Wang was supported by China Post-doctoral Programme Foundation for Innovative Talent. The work was done while Q. Wang was a PhD student at the School of Information and Communication Engineering, Dalian University of Technology, China.",,70,4,4,2,2,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2582,2597,,10.1109/TPAMI.2020.2974833,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32086198,Green Submitted,Y,N,2022-05-23,WOS:000670578800005
J,"Zhong, Z; Zheng, L; Luo, ZM; Li, SZ; Yang, Y",,,,"Zhong, Zhun; Zheng, Liang; Luo, Zhiming; Li, Shaozi; Yang, Yi",,,Learning to Adapt Invariance in Memory for Person Re-Identification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Training; Cameras; Adaptation models; Reliability; Australia; Memory modules; Task analysis; Person re-identification; domain adaptation; invariance learning; exemplar memory; graph-based positive prediction,NETWORKS,"This work considers the problem of unsupervised domain adaptation in person re-identification (re-ID), which aims to transfer knowledge from the source domain to the target domain. Existing methods are primary to reduce the inter-domain shift between the domains, which however usually overlook the relations among target samples. This paper investigates into the intra-domain variations of the target domain and proposes a novel adaptation framework w.r.t three types of underlying invariance, i.e., Exemplar-Invariance, Camera-Invariance, and Neighborhood-Invariance. Specifically, an exemplar memory is introduced to store features of samples, which can effectively and efficiently enforce the invariance constraints over the global dataset. We further present the Graph-based Positive Prediction (GPP) method to explore reliable neighbors for the target domain, which is built upon the memory and is trained on the source samples. Experiments demonstrate that 1) the three invariance properties are complementary and indispensable for effective domain adaptation, 2) the memory plays a key role in implementing invariance learning and improves the performance with limited extra computation cost, 3) GPP can facilitate the invariance learning and thus significantly improves the results, and 4) our approach produces new state-of-the-art adaptation accuracy on three re-ID large-scale benchmarks.","[Zhong, Zhun; Li, Shaozi] Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Fujian, Peoples R China; [Zhong, Zhun; Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo, NSW 2007, Australia; [Zheng, Liang] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia; [Luo, Zhiming] Xiamen Univ, Postdoctoral Mobile Stn Informat & Commun Engn, Xiamen 361005, Fujian, Peoples R China",,"Li, SZ (通讯作者)，Xiamen Univ, Dept Artificial Intelligence, Xiamen 361005, Fujian, Peoples R China.",zhunzhong007@gmail.com; liangzheng06@gmail.com; zhiming.luo@xmu.edu.cn; szlig@xmu.edu.cn; yee.i.yang@gmail.com,"Yang, Yi/B-9273-2017","Yang, Yi/0000-0002-0512-880X","National Nature Science Foundation of China [61876159, 61806172, 61572409, U1705286, 61571188]; National Key Research and Development Program of China [2018YFC0831402]; Australian Research Council Discovery Early Career Award - Australian Government [DE200101283]",National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Australian Research Council Discovery Early Career Award - Australian Government(Australian Research Council),"This work was supported by the National Nature Science Foundation of China (No. 61876159, 61806172, 61572409, U1705286 & 61571188), the National Key Research and Development Program of China (No. 2018YFC0831402). Dr. Liang Zheng is the recipient of an Australian Research Council Discovery Early Career Award (DE200101283) funded by the Australian Government. Zhun Zhong thanks Wenjing Li for encouragement.",,71,23,23,6,15,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,AUG 1,2021,43,8,,,,,2723,2738,,10.1109/TPAMI.2020.2976933,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TF2YV,32142418,Green Submitted,Y,N,2022-05-23,WOS:000670578800015
J,"Pan, JS; Dong, JX; Liu, Y; Zhang, JW; Ren, JM; Tang, JH; Tai, YW; Yang, MH",,,,"Pan, Jinshan; Dong, Jiangxin; Liu, Yang; Zhang, Jiawei; Ren, Jimmy; Tang, Jinhui; Tai, Yu-Wing; Yang, Ming-Hsuan",,,Physics-Based Generative Adversarial Models for Image Restoration and Beyond,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image restoration; Generative adversarial networks; Gallium nitride; Physics; Task analysis; Mathematical model; Degradation; Generative adversarial network; physics model; low-level vision; image restoration,REMOVAL,"We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, and image deraining). These problems are ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we show that these problems can be solved by generative models with adversarial learning. However, a straightforward formulation based on a straightforward generative adversarial network (GAN) does not perform well in these tasks, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose an algorithm that guides the estimation process of a specific task within the GAN framework. The proposed model is trained in an end-to-end fashion and can be applied to a variety of image restoration and low-level vision problems. Extensive experiments demonstrate that the proposed method performs favorably against state-of-the-art algorithms.","[Pan, Jinshan; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China; [Dong, Jiangxin; Liu, Yang] Dalian Univ Technol, Sch Math & Sci, Dalian 116024, Peoples R China; [Zhang, Jiawei; Ren, Jimmy] SenseTime Res, Shenzhen 518000, Peoples R China; [Tai, Yu-Wing] Tencent, Shenzhen 518054, Peoples R China; [Yang, Ming-Hsuan] Univ Calif, Sch Engn, Merced, CA 95344 USA",,"Yang, MH (通讯作者)，Univ Calif, Sch Engn, Merced, CA 95344 USA.",sdluran@gmail.com; dongjxjx@gmail.com; lewisyangliu@gmail.com; zhjw1988@gmail.com; rensijie@sensetime.com; jinhuitang@njust.edu.cn; yuwingtai@tencent.com; mhyang@ucmerced.edu,"Yang, Ming-Hsuan/T-9533-2019; Tai, Yuwing/C-2047-2011","Yang, Ming-Hsuan/0000-0003-4848-2304; Tai, Yuwing/0000-0002-3148-0380; Ren, Jimmy/0000-0002-5888-3083","National Natural Science Foundation of China [61922043, 61872421, 61732007]; Natural Science Foundation of Jiangsu Province [BK20180471]; National Science Foundation CAREER [1149783]",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province); National Science Foundation CAREER(National Science Foundation (NSF)),"This work was supported in part by the National Natural Science Foundation of China (Nos. 61922043, 61872421, 61732007), the Natural Science Foundation of Jiangsu Province (No. BK20180471), and National Science Foundation CAREER (No. 1149783).",,65,30,30,30,46,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JUL 1,2021,43,7,,,,,2449,2462,,10.1109/TPAMI.2020.2969348,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UL3FK,31995475,Green Submitted,Y,N,2022-05-23,WOS:000692540900020
J,"Zhang, YL; Tian, YP; Kong, Y; Zhong, BN; Fu, Y",,,,"Zhang, Yulun; Tian, Yapeng; Kong, Yu; Zhong, Bineng; Fu, Yun",,,Residual Dense Network for Image Restoration,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Feature extraction; Image restoration; Training; Task analysis; Image coding; Image denoising; Residual dense network; hierarchical features; image restoration; image super-resolution; image denoising; compression artifact reduction; image deblurring,SPARSE; SUPERRESOLUTION; REPRESENTATIONS,"Recently, deep convolutional neural network (CNN) has achieved great success for image restoration (IR) and provided hierarchical features at the same time. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images; thereby, resulting in relatively-low performance. In this work, we propose a novel and efficient residual dense network (RDN) to address this problem in IR, by making a better tradeoff between efficiency and effectiveness in exploiting the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.","[Zhang, Yulun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [Tian, Yapeng] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Kong, Yu] Thomas Golisano Coll Comp & Informat Sci, Rochester Inst Technol, Rochester, NY 14623 USA; [Zhong, Bineng] Huaqiao Univ, Sch Comp Sci & Technol, Xiamen 361021, Peoples R China; [Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [Fu, Yun] Northeastern Univ, Khoury Coll, Boston, MA 02115 USA",,"Zhang, YL (通讯作者)，Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.",yulun100@gmail.com; yapengtian@rochester.edu; yu.kong@rit.edu; bnzhong@hqu.edu.cn; yunfu@ece.neu.edu,,"Fu, Yun/0000-0002-5098-2853",U.S. Army Research Office Award [W911NF-17-1-0367],U.S. Army Research Office Award,This work was supported in part by the U.S. Army Research Office Award W911NF-17-1-0367.,,75,104,106,69,129,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JUL 1,2021,43,7,,,,,2480,2495,,10.1109/TPAMI.2020.2968521,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,UL3FK,31985406,Green Submitted,Y,Y,2022-05-23,WOS:000692540900022
J,"Rebecq, H; Ranftl, R; Koltun, V; Scaramuzza, D",,,,"Rebecq, Henri; Ranftl, Rene; Koltun, Vladlen; Scaramuzza, Davide",,,High Speed and High Dynamic Range Video with an Event Camera,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Event-based vision; dynamic vision sensor; video reconstruction; high speed; high dynamic range,VISION; ODOMETRY; ROBUST; SENSOR; PIXEL,"Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous events instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (> 20%), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (> 5, 000 frames per second) of high-speed phenomena (e.g., a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.","[Rebecq, Henri; Scaramuzza, Davide] Univ Zurich, Robot & Percept Grp, Dept Informat, Zurich, Switzerland; [Rebecq, Henri; Scaramuzza, Davide] Univ Zurich, Dept Neuroinformat, Zurich, Switzerland; [Ranftl, Rene; Koltun, Vladlen] Intel Labs, Intelligent Syst Lab, Santa Clara, CA 95054 USA",,"Rebecq, H (通讯作者)，Univ Zurich, Robot & Percept Grp, Dept Informat, Zurich, Switzerland.; Rebecq, H (通讯作者)，Univ Zurich, Dept Neuroinformat, Zurich, Switzerland.",rebecq@ifi.uzh.ch; rene.ranftl@intel.com; vkoltun@gmail.com; sdavide@ifi.uzh.ch,,"Rebecq, Henri/0000-0002-6577-9735",Swiss National Center of Competence Research Robotics (NCCR); SNSF-ERC Starting Grant andQualcomm(QualcommInnovation Fellowship 2018),Swiss National Center of Competence Research Robotics (NCCR); SNSF-ERC Starting Grant andQualcomm(QualcommInnovation Fellowship 2018),"The authors would like to thank Cedric Scheerlinck, Matthias Faessler and his family for valuable discussions and for help recording data, as well as iniVation and Samsung for providing the event cameras that were used for this research. This work was supported by the Swiss National Center of Competence Research Robotics (NCCR), the SNSF-ERC Starting Grant andQualcomm(through the QualcommInnovation Fellowship 2018).",,63,30,30,19,34,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JUN 1,2021,43,6,,,,,1964,1980,,10.1109/TPAMI.2019.2963386,,,17,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,SA8YQ,31902754,Green Submitted,Y,N,2022-05-23,WOS:000649590200011
J,"Wang, Q; Gao, JY; Lin, W; Li, XL",,,,"Wang, Qi; Gao, Junyu; Lin, Wei; Li, Xuelong",,,NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Crowd counting; crowd localization; crowd analysis; benchmark website,,"In the last decade, crowd counting and localization attract much attention of researchers due to its wide-spread applications, including crowd monitoring, public safety, space design, etc. Many convolutional neural networks (CNN) are designed for tackling this task. However, currently released datasets are so small-scale that they can not meet the needs of the supervised CNN-based algorithms. To remedy this problem, we construct a large-scale congested crowd counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0 similar to 20; 033). Besides, a benchmark website is developed for impartially evaluating the different methods, which allows researchers to submit the results of the test set. Based on the proposed dataset, we further describe the data characteristics, evaluate the performance of some mainstream state-of-the-art (SOTA) methods, and analyze the new problems that arise on the new data. What's more, the benchmark is deployed at https://www.crowdbenchmark.com/, and the dataset/code/models/results are available at https://gjy3035.github.io/ NWPU-Crowd-Sample-Code/","[Wang, Qi; Gao, Junyu; Lin, Wei; Li, Xuelong] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China; [Wang, Qi; Gao, Junyu; Lin, Wei; Li, Xuelong] Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China",,"Li, XL (通讯作者)，Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.; Li, XL (通讯作者)，Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.",crabwq@gmail.com; gjy3035@gmail.com; elonlin24@gmail.com; li@nwpu.edu.cn,,"Lin, Wei/0000-0001-8425-956X; Wang, Qi/0000-0002-7028-4956","National Key R&D Program of China [2017YFB1002202]; National Natural Science Foundation of China [U1864204, 61773316, U1801262, 61871470]",National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported by the National Key R&D Program of China under Grant 2017YFB1002202, National Natural Science Foundation of China under Grant U1864204, 61773316, U1801262, and 61871470.",,46,43,43,2,2,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JUN 1,2021,43,6,,,,,2141,2149,,10.1109/TPAMI.2020.3013269,,,9,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,SA8YQ,32750840,Green Submitted,Y,N,2022-05-23,WOS:000649590200024
J,"Cai, ZW; Vasconcelos, N",,,,"Cai, Zhaowei; Vasconcelos, Nuno",,,Cascade R-CNN: High Quality Object Detection and Instance Segmentation,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Detectors; Object detection; Training; Proposals; Task analysis; Computer architecture; Feature extraction; Object detection; high quality; cascade; bounding box regression; instance segmentation,,"In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN.","[Cai, Zhaowei; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92093 USA",,"Cai, ZW (通讯作者)，Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92093 USA.",zwcai@ucsd.edu; nuno@ucsd.edu,,"Vasconcelos, Nuno/0000-0002-9024-4302","NSF [IIS-1546305, IIS-1637941]",NSF(National Science Foundation (NSF)),"This work was funded by NSF Awards IIS-1546305 and IIS-1637941, and a GPU donation from NVIDIA. The authors would also like to thank Kaiming He for valuable discussions.",,63,168,174,110,175,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAY 1,2021,43,5,,,,,1483,1498,,10.1109/TPAMI.2019.2956516,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,RJ3YD,31794388,Green Submitted,Y,Y,2022-05-23,WOS:000637533800002
J,"Dong, XP; Shen, JB; Wang, WG; Shao, L; Ling, HB; Porikli, F",,,,"Dong, Xingping; Shen, Jianbing; Wang, Wenguan; Shao, Ling; Ling, Haibin; Porikli, Fatih",,,Dynamical Hyperparameter Optimization via Deep Reinforcement Learning in Tracking,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Heuristic algorithms; Learning (artificial intelligence); Object tracking; Training; Visualization; Optimization methods; Hyperparameters; continuous deep q-learning; reinforcement learning; visual object tracking,,"Hyperparameters are numerical pre-sets whose values are assigned prior to the commencement of a learning process. Selecting appropriate hyperparameters is often critical for achieving satisfactory performance in many vision problems, such as deep learning-based visual object tracking. However, it is often difficult to determine their optimal values, especially if they are specific to each video input. Most hyperparameter optimization algorithms tend to search a generic range and are imposed blindly on all sequences. In this paper, we propose a novel dynamical hyperparameter optimization method that adaptively optimizes hyperparameters for a given sequence using an action-prediction network leveraged on continuous deep Q-learning. Since the observation space for object tracking is significantly more complex than those in traditional control problems, existing continuous deep Q-learning algorithms cannot be directly applied. To overcome this challenge, we introduce an efficient heuristic strategy to handle high dimensional state space, while also accelerating the convergence behavior. The proposed algorithm is applied to improve two representative trackers, a Siamese-based one and a correlation-filter-based one, to evaluate its generalizability. Their superior performances on several popular benchmarks are clearly demonstrated. Our source code is available at https://github.com/shenjianbing/dqltracking.","[Dong, Xingping; Shen, Jianbing; Wang, Wenguan] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China; [Dong, Xingping; Shen, Jianbing; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Porikli, Fatih] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia",,"Shen, JB (通讯作者)，Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.; Shen, JB (通讯作者)，Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.",xingping.dong@gmail.com; shenjianbingcg@gmail.com; wenguanwang.ai@gmail.com; ling.shao@ieee.org; haibin.ling@stonybrook.edu; fatih.porikli@anu.edu.au,"Dong, Xingping/AAA-8107-2019; Wang, Wenguan/AAA-5782-2022","Dong, Xingping/0000-0003-1613-9288; Wang, Wenguan/0000-0002-0802-9567; Shen, Jianbing/0000-0002-4109-8353; Ling, Haibin/0000-0003-4094-8413","Beijing Natural Science Foundation [4182056]; CCF-Tencent Open Fund, Zhijiang Lab's International Talent Fund for Young Professionals; Joint Building Programof Beijing Municipal Education Commission","Beijing Natural Science Foundation(Beijing Natural Science Foundation); CCF-Tencent Open Fund, Zhijiang Lab's International Talent Fund for Young Professionals; Joint Building Programof Beijing Municipal Education Commission","This work was supported in part by the Beijing Natural Science Foundation under Grant 4182056, the CCF-Tencent Open Fund, Zhijiang Lab's International Talent Fund for Young Professionals, and the Joint Building Programof Beijing Municipal Education Commission. A preliminary version of this work has appeared in CVPR 2018 [1].",,70,49,49,51,101,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAY 1,2021,43,5,,,,,1515,1529,,10.1109/TPAMI.2019.2956703,,,15,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,RJ3YD,31796388,,Y,N,2022-05-23,WOS:000637533800004
J,"Han, XF; Laga, H; Bennamoun, M",,,,"Han, Xian-Feng; Laga, Hamid; Bennamoun, Mohammed",,,Image-Based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Three-dimensional displays; Image reconstruction; Shape; Training; Deep learning; Two dimensional displays; Australia; 3D reconstruction; depth estimation; SLAM; SfM; CNN; deep learning; LSTM; 3D face; 3D human body; 3D video,PARAMETERIZATION,"3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.","[Han, Xian-Feng] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China; [Han, Xian-Feng] Tianjin Univ, Tianjin 300072, Peoples R China; [Han, Xian-Feng; Bennamoun, Mohammed] Univ Western Australia, Perth, WA 6009, Australia; [Laga, Hamid] Murdoch Univ, Informat Technol Math & Stat Discipline, Perth, WA 6150, Australia; [Laga, Hamid] Univ South Australia, Phen & Bioinformat Res Ctr, Adelaide, SA 5001, Australia",,"Laga, H (通讯作者)，Murdoch Univ, Informat Technol Math & Stat Discipline, Perth, WA 6150, Australia.",hanxianf@163.com; H.Laga@murdoch.edu.au; mohammed.bennamoun@uwa.edu.au,"Bennamoun, Mohammed/C-2789-2013","Bennamoun, Mohammed/0000-0002-6603-3257; Han, Xianfeng/0000-0002-4869-4537","China Scholarship Council (CSC) scholarship; ARC [DP150100294, DP150104251]",China Scholarship Council (CSC) scholarship(China Scholarship Council); ARC(Australian Research Council),Xian-Feng Han is supported by a China Scholarship Council (CSC) scholarship. This work was supported in part by ARC DP150100294 and DP150104251. Xian-Feng Han and Hamid Laga are joint first authors.,,164,90,91,134,258,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAY 1,2021,43,5,,,,,1578,1604,,10.1109/TPAMI.2019.2954885,,,27,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,RJ3YD,31751229,Green Submitted,Y,Y,2022-05-23,WOS:000637533800008
J,"Huang, LH; Zhao, X; Huang, KQ",,,,"Huang, Lianghua; Zhao, Xin; Huang, Kaiqi",,,GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Training; Object tracking; Databases; Protocols; Benchmark testing; Servers; Object tracking; benchmark dataset; performance evaluation,,"We introduce here a large tracking database that offers an unprecedentedly wide coverage of common moving objects in the wild, called GOT-10k. Specifically, GOT-10k is built upon the backbone of WordNet structure [1] and it populates the majority of over 560 classes of moving objects and 87 motion patterns, magnitudes wider than the most recent similar-scale counterparts [19], [20], [23], [26]. By releasing the large high-diversity database, we aim to provide a unified training and evaluation platform for the development of class-agnostic, generic purposed short-term trackers. The features of GOT-10k and the contributions of this article are summarized in the following. (1) GOT-10k offers over 10,000 video segments with more than 1.5 million manually labeled bounding boxes, enabling unified training and stable evaluation of deep trackers. (2) GOT-10k is by far the first video trajectory dataset that uses the semantic hierarchy of WordNet to guide class population, which ensures a comprehensive and relatively unbiased coverage of diverse moving objects. (3) For the first time, GOT-10k introduces the one-shot protocol for tracker evaluation, where the training and test classes are zero-overlapped. The protocol avoids biased evaluation results towards familiar objects and it promotes generalization in tracker development. (4) GOT-10k offers additional labels such as motion classes and object visible ratios, facilitating the development of motion-aware and occlusion-aware trackers. (5) We conduct extensive tracking experiments with 39 typical tracking algorithms and their variants on GOT-10k and analyze their results in this paper. (6) Finally, we develop a comprehensive platform for the tracking community that offers full-featured evaluation toolkits, an online evaluation server, and a responsive leaderboard. The annotations of GOT-10k's test data are kept private to avoid tuning parameters on it.","[Huang, Lianghua; Zhao, Xin; Huang, Kaiqi] Chinese Acad Sci, Ctr Res Intelligent Syst & Engn, Inst Automat, Beijing 100190, Peoples R China; [Huang, Lianghua; Zhao, Xin; Huang, Kaiqi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Huang, Kaiqi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China; [Huang, Kaiqi] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China",,"Huang, LH (通讯作者)，Chinese Acad Sci, Ctr Res Intelligent Syst & Engn, Inst Automat, Beijing 100190, Peoples R China.",huanglianghua2017@ia.ac.cn; xzhao@nlpr.ia.ac.cn; kqhuang@nlpr.ia.ac.cn,,"Zhao, Xin/0000-0002-7660-9897; Huang, Lianghua/0000-0002-9686-9354","National Key Research and Development Program of China [2016YFB1001001, 2016YFB1001005]; National Natural Science Foundation of China [61602485, 61673375]; Projects of Chinese Academy of Science [QYZDB-SSW-JSC006]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Projects of Chinese Academy of Science,"This work was supported in part by the National Key Research and Development Program of China (Grant No. 2016YFB1001001 and No. 2016YFB1001005), the National Natural Science Foundation of China (Grant No. 61602485 and No. 61673375), and the Projects of Chinese Academy of Science (Grant No. QYZDB-SSW-JSC006). Lianghua Huang and Xin Zhao contributed equally to this work.",,82,187,200,21,33,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAY 1,2021,43,5,,,,,1562,1577,,10.1109/TPAMI.2019.2957464,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,RJ3YD,31804928,Green Submitted,Y,Y,2022-05-23,WOS:000637533800007
J,"Mittal, S; Tatarchenko, M; Brox, T",,,,"Mittal, Sudhanshu; Tatarchenko, Maxim; Brox, Thomas",,,Semi-Supervised Semantic Segmentation With High- and Low-Level Consistency,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image segmentation; Training; Semantics; Gallium nitride; Generators; Standards; Semisupervised learning; Computer vision; semi-supervised learning; semantic segmentation; generative adversarial networks,,"The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. The proposed approach relies on adversarial training with a feature matching loss to learn from unlabeled images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks-PASCAL VOC 2012, PASCAL-Context, and Cityscapes-the approach achieves new state-of-the-art in semi-supervised learning.","[Mittal, Sudhanshu; Tatarchenko, Maxim; Brox, Thomas] Univ Freiburg, Comp Sci Dept, D-79085 Freiburg, Germany",,"Mittal, S (通讯作者)，Univ Freiburg, Comp Sci Dept, D-79085 Freiburg, Germany.",mittal@cs.uni-freiburg.de; tatarchm@cs.uni-freiburg.de; brox@cs.uni-freiburg.de,,"Mittal, Sudhanshu/0000-0002-7809-8058; Tatarchenko, Maxim/0000-0003-1988-1488",German Federal Ministry of Education and Research; Intel Network of Intelligent Systems; Facebook,German Federal Ministry of Education and Research(Federal Ministry of Education & Research (BMBF)); Intel Network of Intelligent Systems; Facebook(Facebook Inc),This study was supported by the German Federal Ministry of Education and Research via the project Deep-PTL and by the Intel Network of Intelligent Systems. The authors would also like to thank Facebook for their P100 server donation and gift funding.,,40,36,36,29,62,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,APR 1,2021,43,4,,,,,1369,1379,,10.1109/TPAMI.2019.2960224,,,11,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QT3YJ,31869780,Green Submitted,Y,N,2022-05-23,WOS:000626525300019
J,"Xu, YC; Fu, MT; Wang, QM; Wang, YK; Chen, K; Xia, GS; Bai, X",,,,"Xu, Yongchao; Fu, Mingtao; Wang, Qimeng; Wang, Yukang; Chen, Kai; Xia, Gui-Song; Bai, Xiang",,,Gliding Vertex on the Horizontal Bounding Box for Multi-Oriented Object Detection,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Object detection; Feature extraction; Proposals; Detectors; Electronic mail; Benchmark testing; Runtime; Object detection; R-CNN; multi-oriented object; aerial image; scene text; pedestrian detection,,"Object detection has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the regression head of faster R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, pedestrian detection in fisheye images.","[Xu, Yongchao; Fu, Mingtao; Wang, Qimeng; Wang, Yukang; Bai, Xiang] Huazhong Univ Sci & Technol HUST, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China; [Chen, Kai] Wuhan Univ, LIEMARS, Wuhan 430072, Hubei, Peoples R China; [Xia, Gui-Song] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China; [Xia, Gui-Song] Onyou Inc, Shenzhen 518057, Guangdong, Peoples R China",,"Bai, X (通讯作者)，Huazhong Univ Sci & Technol HUST, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.",yongchaoxu@hust.edu.cn; mingtaofu@hust.edu.cn; qimengwang@hust.edu.cn; wangyk@hust.edu.cn; kchen@sjtu.edu.cn; guisong.xia@whu.edu.cn; xbai@hust.edu.cn,"Wang, Yukang/AAK-7775-2020","Wang, Yukang/0000-0003-0173-6880; Bai, Xiang/0000-0002-3449-5940; Doloriel, Chandler Timm/0000-0003-3820-7317; wang, qimeng/0000-0002-9715-836X; Xia, Gui-Song/0000-0001-7660-6090",Major Project for New Generation of AI [2018AAA0100400]; NSFC [61703171]; NSF of Hubei Province of China [2018CFB199]; Young Elite Scientists Sponsorship Program by CAST; Program for HUST Academic Frontier Youth Team [2017QYTD08],Major Project for New Generation of AI; NSFC(National Natural Science Foundation of China (NSFC)); NSF of Hubei Province of China; Young Elite Scientists Sponsorship Program by CAST; Program for HUST Academic Frontier Youth Team,"This work was supported in part by the Major Project for New Generation of AI under Grant no. 2018AAA0100400, in part by NSFC 61703171, and in part by NSF of Hubei Province of China under Grant 2018CFB199, Dr. Yongchao Xu was supported by the Young Elite Scientists Sponsorship Program by CAST and Dr. Xiang Bai was supported by the Program for HUST Academic Frontier Youth Team 2017QYTD08.",,48,107,113,76,146,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,APR 1,2021,43,4,,,,,1452,1459,,10.1109/TPAMI.2020.2974745,,,8,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QT3YJ,32086194,Green Submitted,Y,Y,2022-05-23,WOS:000626525300025
J,"Yan, CG; Gong, B; Wei, YX; Gao, Y",,,,"Yan, Chenggang; Gong, Biao; Wei, Yuxuan; Gao, Yue",,,Deep Multi-View Enhancement Hashing for Image Retrieval,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image retrieval; Binary codes; Machine learning; Training; Feature extraction; Neural networks; Stability analysis; Multi-view hashing; multi-view enhancement; image retrieval,ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH; NEURAL-NETWORKS; BINARY-CODES; SCALE,"Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However, large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore, we try to introduce the multi-view deep neural network into the hash learning field, and design an efficient and innovative retrieval model, which has achieved a significant improvement in retrieval performance. In this paper, we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views, which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval, we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.","[Yan, Chenggang; Gong, Biao] Hangzhou Dianzi Univ, Hangzhou 310018, Zhejiang, Peoples R China; [Yan, Chenggang] Shandong Univ, Sch Mech Elect & Informat Engn, Weihai 264209, Shandong, Peoples R China; [Wei, Yuxuan; Gao, Yue] Tsinghua Univ, Sch Software, KLISS, BNRist, Beijing 100084, Peoples R China",,"Gao, Y (通讯作者)，Tsinghua Univ, Sch Software, KLISS, BNRist, Beijing 100084, Peoples R China.",cgyan@hdu.edu.cn; a.biao.gong@gmail.com; weiyuxua19@mails.tsinghua.edu.cn; gaoyue@tsinghua.edu.cn,,"Gong, Biao/0000-0002-6156-0816","National Nature Science Foundation of China [61931008, 61671267, 61671196, 61701149, 61801157, 61971268, 61901145, 61901150, 61972123]; National Natural Science Major Foundation of Research Instrumentation of PR China [61427808]; Zhejiang Province Nature Science Foundation of China [LR17F030006, Q19F010030]; 111 Project [D17019]","National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Major Foundation of Research Instrumentation of PR China; Zhejiang Province Nature Science Foundation of China; 111 Project(Ministry of Education, China - 111 Project)","This work is supported by National Nature Science Foundation of China (61931008, 61671267, 61671196, 61701149, 61801157, 61971268, 61901145, 61901150, 61972123), National Natural Science Major Foundation of Research Instrumentation of PR China under Grants 61427808, Zhejiang Province Nature Science Foundation of China (LR17F030006,Q19F010030), 111 Project, No. D17019.",,50,461,478,90,163,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,APR 1,2021,43,4,,,,,1445,1451,,10.1109/TPAMI.2020.2975798,,,7,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QT3YJ,32091992,Green Submitted,Y,Y,2022-05-23,WOS:000626525300024
J,"Gou, JP; Yu, BS; Maybank, SJ; Tao, DC",,,,"Gou, Jianping; Yu, Baosheng; Maybank, Stephen J.; Tao, Dacheng",,,Knowledge Distillation: A Survey,INTERNATIONAL JOURNAL OF COMPUTER VISION,,,English,Article,,,,,,Deep neural networks; Model compression; Knowledge distillation; Knowledge transfer; Teacher&#8211; student architecture,,"In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.","[Gou, Jianping] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China; [Gou, Jianping] Jiangsu Univ, Jiangsu Key Lab Secur Technol Ind Cyberspace, Zhenjiang 212013, Jiangsu, Peoples R China; [Gou, Jianping; Yu, Baosheng; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Maybank, Stephen J.] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London, England",,"Tao, DC (通讯作者)，Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.",cherish.gjp@gmail.com; baosheng.yu@sydney.edu.au; sjmaybank@dcs.bbk.ac.uk; dacheng.tao@sydney.edu.au,,,"Australian Research Council [FL-170100117, IH-180100002, IC-190100 031]; National Natural Science Foundation of China [61976107]",Australian Research Council(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"The work was in part supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100 031, and National Natural Science Foundation of China 61976107.",,347,84,85,52,92,SPRINGER,DORDRECHT,"VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS",0920-5691,1573-1405,,INT J COMPUT VISION,Int. J. Comput. Vis.,JUN,2021,129,6,,,,,1789,1819,,10.1007/s11263-021-01453-z,,MAR 2021,31,"Computer Science, Artificial Intelligence",Science Citation Index Expanded (SCI-EXPANDED),Computer Science,SH0NU,,"Green Submitted, Green Accepted",Y,N,2022-05-23,WOS:000631300600001
J,"Bao, WB; Lai, WS; Zhang, XY; Gao, ZY; Yang, MH",,,,"Bao, Wenbo; Lai, Wei-Sheng; Zhang, Xiaoyun; Gao, Zhiyong; Yang, Ming-Hsuan",,,MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Interpolation; Kernel; Estimation; Motion estimation; Adaptation models; Optical imaging; Motion compensation; Motion estimation; motion compensation; convolutional neural network; adaptive warping,FRAME RATE; OPTICAL-FLOW; ALGORITHM,"Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net architecture can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.","[Bao, Wenbo; Zhang, Xiaoyun; Gao, Zhiyong] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Dept Elect Engn & Comp Sci, Merced, CA 95340 USA",,"Yang, MH (通讯作者)，Univ Calif Merced, Dept Elect Engn & Comp Sci, Merced, CA 95340 USA.",baowenbo@sjtu.edu.cn; wlai24@ucmerced.edu; xiaoyun.zhang@sjtu.edu.cn; zhiyong.gao@sjtu.edu.cn; mhyang@ucmerced.edu,"Yang, Ming-Hsuan/T-9533-2019","Yang, Ming-Hsuan/0000-0003-4848-2304",National Natural Science Foundation of China [61771306]; Natural Science Foundation of Shanghai [18ZR1418100]; Chinese National Key S&T Special Program [2013ZX01033001-002-002]; Shanghai Key Laboratory of Digital Media Processing and Transmissions [STCSM 18DZ2270700]; NSF Career Grant [1149783],National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Shanghai(Natural Science Foundation of Shanghai); Chinese National Key S&T Special Program; Shanghai Key Laboratory of Digital Media Processing and Transmissions; NSF Career Grant(National Science Foundation (NSF)NSF - Office of the Director (OD)),"W. Bao, X. Zhang, and Z. Gao are supported in part by the National Natural Science Foundation of China (61771306), the Natural Science Foundation of Shanghai (18ZR1418100), the ChineseNational Key S&T Special Program(2013ZX01033001-002-002), and the Shanghai Key Laboratory of Digital Media Processing and Transmissions (STCSM 18DZ2270700). W.-S. Lai andM.-H. Yang are supported in part by NSF Career Grant (1149783) and gifts fromAdobe, Google, and NEC.",,58,56,57,12,34,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,933,948,,10.1109/TPAMI.2019.2941941,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31722471,Green Submitted,Y,N,2022-05-23,WOS:000616309900013
J,"Guan, ZY; Xing, QL; Xu, M; Yang, R; Liu, T; Wang, ZL",,,,"Guan, Zhenyu; Xing, Qunliang; Xu, Mai; Yang, Ren; Liu, Tie; Wang, Zulin",,,MFQE 2.0: A New Approach for Multi-Frame Quality Enhancement on Compressed Video,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Transform coding; Image coding; Databases; MPEG 1 Standard; Task analysis; Video recording; Quality enhancement; compressed video; deep learning,MOTION COMPENSATION; SUPERRESOLUTION; ARTIFACTS; DCT,"The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, not considering the similarity between consecutive frames. Since heavy fluctuation exists across compressed video frames as investigated in this paper, frame similarity can be utilized for quality enhancement of low-quality frames given their neighboring high-quality frames. This task is Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as the first attempt in this direction. In our approach, we first develop a Bidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are the input. In MF-CNN, motion between the non-PQF and PQFs is compensated by a motion compensation subnet. Subsequently, a quality enhancement subnet fuses the non-PQF and compensated PQFs, and then reduces the compression artifacts of the non-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments validate the effectiveness and generalization ability of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video.","[Guan, Zhenyu; Xing, Qunliang; Xu, Mai; Yang, Ren; Liu, Tie; Wang, Zulin] Beihang Univ, Beijing 100191, Peoples R China; [Xu, Mai] Beihang Univ, Hangzhou Innovat Inst, Beijing, Peoples R China",,"Xu, M (通讯作者)，Beihang Univ, Beijing 100191, Peoples R China.",guanzhenyu@buaa.edu.cn; xingql@buaa.edu.cn; MaiXu@buaa.edu.cn; reyang@ee.ethz.ch; liutie@buaa.edu.cn; wzulin@buaa.edu.cn,"于, 于增臣/AAH-4657-2021; Xing, Qunliang/AAR-8676-2021","Xing, Qunliang/0000-0002-3007-716X; Yang, Ren/0000-0003-4124-4186; GUAN, zhenyu/0000-0002-3959-338X","NSFC [61876013, 61922009, 61573037]",NSFC(National Natural Science Foundation of China (NSFC)),"This work was supported by the NSFC projects 61876013, 61922009 and 61573037. Z. Guan and Q. Xing contributed equally to this paper.",,57,50,50,17,31,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,949,963,,10.1109/TPAMI.2019.2944806,,,15,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31581073,Green Submitted,Y,N,2022-05-23,WOS:000616309900014
J,"Kouw, WM; Loog, M",,,,"Kouw, Wouter M.; Loog, Marco",,,A Review of Domain Adaptation without Target Labels,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Review,,,,,,Sociology; Statistics; Machine learning; Pattern recognition; Hospitals; Image recognition; Pattern analysis; Machine learning; pattern recognition; domain adaptation; transfer learning; covariate shift; sample selection bias,COVARIATE SHIFT ADAPTATION; LEARNING-METHODS; KERNEL; CLASSIFICATION; ALGORITHM; BIAS,"Domain adaptation has become a prominent problem setting in machine learning and related fields. This review asks the question: How can a classifier learn from a source domain and generalize to a target domain? We present a categorization of approaches, divided into, what we refer to as, sample-based, feature-based, and inference-based methods. Sample-based methods focus on weighting individual observations during training based on their importance to the target domain. Feature-based methods revolve around on mapping, projecting, and representing features such that a source classifier performs well on the target domain and inference-based methods incorporate adaptation into the parameter estimation procedure, for instance through constraints on the optimization procedure. Additionally, we review a number of conditions that allow for formulating bounds on the cross-domain generalization error. Our categorization highlights recurring ideas and raises questions important to further research.","[Kouw, Wouter M.] Univ Copenhagen, Datal Inst, Machine Learning Grp, DK-1165 Copenhagen, Denmark; [Loog, Marco] Delft Univ Technol, Pattern Recognit Lab, NL-2628 Delft, Netherlands; [Loog, Marco] Univ Copenhagen, Datal Inst, DK-1165 Copenhagen, Denmark",,"Kouw, WM (通讯作者)，Univ Copenhagen, Datal Inst, Machine Learning Grp, DK-1165 Copenhagen, Denmark.",wmkouw@gmail.com; m.loog@tudelft.nl,"Kouw, Wouter/T-9432-2019","Kouw, Wouter/0000-0002-0547-4817",Niels Stensen Fellowship,Niels Stensen Fellowship,"This paper is made possible through the support of the Niels Stensen Fellowship. The authors thank Marcel Reinders, Tom Viering, and Soufiane Mouragui for insightful discussions, and the reviewers for their constructive feedback.",,214,67,67,71,137,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,766,785,,10.1109/TPAMI.2019.2945942,,,20,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI),Computer Science; Engineering,QE6IS,31603771,Green Submitted,Y,N,2022-05-23,WOS:000616309900002
J,"Liu, X; Hu, ZK; Ling, HB; Cheung, YM",,,,"Liu, Xin; Hu, Zhikai; Ling, Haibin; Cheung, Yiu-Ming",,,MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal Retrieval,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Lips; Semantics; Computer science; Adaptation models; Task analysis; Encoding; Correlation; Cross-modal retrieval; matrix tri-factorization hashing; varying hash length; semantic correlation matrix,BINARY-CODES; QUANTIZATION; SPARSE,"Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed. Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable. However, such unified or equal-length hash representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths. To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios. To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH), is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios. More specifically, MTFH exploits an efficient objective function to flexibly learn the modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices to semantically correlate the different hash representations for heterogeneous data comparable. As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks. Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.","[Liu, Xin; Hu, Zhikai] Huaqiao Univ, Dept Comp Sci, Xiamen 361021, Peoples R China; [Liu, Xin] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China; [Hu, Zhikai] Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, Xiamen 361021, Peoples R China; [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Cheung, Yiu-Ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China",,"Liu, X (通讯作者)，Huaqiao Univ, Dept Comp Sci, Xiamen 361021, Peoples R China.; Cheung, YM (通讯作者)，Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.",xliu@hqu.edu.cn; zkhu@hqu.edu.cn; hling@cs.stonybrook.edu; ymc@comp.hkbu.edu.hk,"Cheung, Yiu-ming/E-2050-2015","Cheung, Yiu-ming/0000-0001-7629-4648; Hu, Zhikai/0000-0001-7278-9977; Ling, Haibin/0000-0003-4094-8413","National Science Foundation of China [61673185, 61672444]; Fundamental Research Funds for the Central Universities of Huaqiao University [ZQN-PY309]; State Key Laboratory of Integrated Services Networks of Xidian University [ISN2011]; National Science Foundation of Fujian Province [2017J01112]; Quanzhou City Science & Technology Program of China [2018C107R]; ITF project of HKSAR [ITS/339/18]; Hong Kong Baptist University, Research Committee, Initiation Grant-Faculty Niche Research Areas (IG-FNRA) 2018/19 [RC-FNRA-IG/18-19/SCI/03]","National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities of Huaqiao University; State Key Laboratory of Integrated Services Networks of Xidian University; National Science Foundation of Fujian Province; Quanzhou City Science & Technology Program of China; ITF project of HKSAR; Hong Kong Baptist University, Research Committee, Initiation Grant-Faculty Niche Research Areas (IG-FNRA) 2018/19","This work was supported by the National Science Foundation of China (No. 61673185 and No. 61672444), Fundamental Research Funds for the Central Universities of Huaqiao University (No. ZQN-PY309), State Key Laboratory of Integrated Services Networks of Xidian University (No. ISN2011), National Science Foundation of Fujian Province (No. 2017J01112), Quanzhou City Science & Technology Program of China (No. 2018C107R), the ITF project of HKSAR (No. ITS/339/18), and Hong Kong Baptist University, Research Committee, Initiation Grant-Faculty Niche Research Areas (IG-FNRA) 2018/19 (No. RC-FNRA-IG/18-19/SCI/03).",,54,26,27,14,35,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,964,981,,10.1109/TPAMI.2019.2940446,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31514125,Green Submitted,Y,N,2022-05-23,WOS:000616309900015
J,"Luo, WX; Liu, W; Lian, DZ; Tang, JH; Duan, LX; Peng, X; Gao, SH",,,,"Luo, Weixin; Liu, Wen; Lian, Dongze; Tang, Jinhui; Duan, Lixin; Peng, Xi; Gao, Shenghua",,,Video Anomaly Detection with Sparse Coding Inspired Deep Neural Networks,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Anomaly detection; Encoding; Feature extraction; Training; Optimization; Dictionaries; Deep learning; Sparse coding; anomaly detection; stacked recurrent neural networks,,"This paper presents an anomaly detection method that is based on a sparse coding inspired Deep Neural Networks (DNN). Specifically, in light of the success of sparse coding based anomaly detection, we propose a Temporally-coherent Sparse Coding (TSC), where a temporally-coherent term is used to preserve the similarity between two similar frames. The optimization of sparse coefficients in TSC with the Sequential Iterative Soft-Thresholding Algorithm (SIATA) is equivalent to a special stacked Recurrent Neural Networks (sRNN) architecture. Further, to reduce the computational cost in alternatively updating the dictionary and sparse coefficients in TSC optimization and to alleviate hyperparameters selection in TSC, we stack one more layer on top of the TSC-inspired sRNN to reconstruct the inputs, and arrive at an sRNN-AE. We further improve sRNN-AE in the following aspects: i) rather than using a predefined similarity measurement between two frames, we propose to learn a data-dependent similarity measurement between neighboring frames in sRNN-AE to make it more suitable for anomaly detection; ii) to reduce computational costs in the inference stage, we reduce the depth of the sRNN in sRNN-AE and, consequently, our framework achieves real-time anomaly detection; iii) to improve computational efficiency, we conduct temporal pooling over the appearance features of several consecutive frames for summarizing information temporally, then we feed appearance features and temporally summarized features into a separate sRNN-AE for more robust anomaly detection. To facilitate anomaly detection evaluation, we also build a large-scale anomaly detection dataset which is even larger than the summation of all existing datasets for anomaly detection in terms of both the volume of data and the diversity of scenes. Extensive experiments on both a toy dataset under controlled settings and real datasets demonstrate that our method significantly outperforms existing methods, which validates the effectiveness of our sRNN-AE method for anomaly detection. Codes and data have been released at https://github.com/StevenLiuWen/sRNN_TSC_Anomaly_Detection.","[Luo, Weixin; Liu, Wen; Lian, Dongze; Gao, Shenghua] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Luo, Weixin; Liu, Wen; Lian, Dongze] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China; [Luo, Weixin; Liu, Wen; Lian, Dongze] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Tang, Jinhui] Nanjing Univ Sci & Engn, Sch Comp Sci & Engn, Nanjing 210023, Peoples R China; [Duan, Lixin] Univ Elect Sci & Technol China, Chengdu 610054, Peoples R China; [Peng, Xi] Sichuan Univ, Coll Comp Sci, Chengdu 610017, Peoples R China",,"Gao, SH (通讯作者)，ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.",luowx@shanghaitech.edu.cn; liuwen@shanghaitech.edu.cn; liandz@shanghaitech.edu.cn; jinhuitang@njust.edu.cn; lxduan@gmail.com; pengx.gm@gmail.com; gaoshh@shanghaitech.edu.cn,"; Peng, Xi/B-9002-2012","Tang, Jinhui/0000-0001-9008-222X; , Weixin/0000-0002-0754-6458; Peng, Xi/0000-0002-5727-2790; Duan, Lixin/0000-0002-0723-4016","National Key Research and Development Program of China [2016YFB1001001]; NSFC [61502304]; Fundamental Research Funds for the Central Universities [YJ201949, 2018SCUH0070]; NFSC [61806135]",National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NFSC(National Natural Science Foundation of China (NSFC)),"This work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001001 and NSFC (No. 61502304). Also, the work was supported in part by the Fundamental Research Funds for the Central Universities under Grant YJ201949 and 2018SCUH0070, in part by the NFSC under Grant 61806135. Weixin Luo and Wen Liu are the co-first authors.",,51,31,31,23,44,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,1070,1084,,10.1109/TPAMI.2019.2944377,,,15,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31567072,,Y,N,2022-05-23,WOS:000616309900022
J,"Meng, N; So, HKH; Sun, X; Lam, EY",,,,"Meng, Nan; So, Hayden K. -H.; Sun, Xing; Lam, Edmund Y.",,,High-Dimensional Dense Residual Convolutional Neural Network for Light Field Reconstruction,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image reconstruction; Convolution; Estimation; Image restoration; Feature extraction; Correlation; Light field super-resolution; 4-dimensional convolution; convolutional neural networks; deep learning,CAMERA; RESOLUTION,"We consider the problem of high-dimensional light field reconstruction and develop a learning-based framework for spatial and angular super-resolution. Many current approaches either require disparity clues or restore the spatial and angular details separately. Such methods have difficulties with non-Lambertian surfaces or occlusions. In contrast, we formulate light field super-resolution (LFSR) as tensor restoration and develop a learning framework based on a two-stage restoration with 4-dimensional (4D) convolution. This allows our model to learn the features capturing the geometry information encoded in multiple adjacent views. Such geometric features vary near the occlusion regions and indicate the foreground object border. To train a feasible network, we propose a novel normalization operation based on a group of views in the feature maps, design a stage-wise loss function, and develop the multi-range training strategy to further improve the performance. Evaluations are conducted on a number of light field datasets including real-world scenes, synthetic data, and microscope light fields. The proposed method achieves superior performance and less execution time comparing with other state-of-the-art schemes.","[Meng, Nan; So, Hayden K. -H.; Lam, Edmund Y.] Univ Hong Kong, Dept Elect & Elect Engn, Pokfulam, Hong Kong, Peoples R China; [Sun, Xing] Tencent, YouTu Lab, Shanghai 200233, Peoples R China",,"Lam, EY (通讯作者)，Univ Hong Kong, Dept Elect & Elect Engn, Pokfulam, Hong Kong, Peoples R China.",nanmeng@eee.hku.hk; hso@eee.hku.hk; winfredsun@tencent.com; elam@eee.hku.hk,"So, Hayden Kwok Hay/C-1585-2009; Lam, Edmund Yin Mun/C-1853-2009","So, Hayden Kwok Hay/0000-0002-6514-0237; Lam, Edmund Yin Mun/0000-0001-6268-950X; Meng, Nan/0000-0003-2821-7474","Hong Kong Research Grants Council [17203217, 17201818, 104005009, 104005438]",Hong Kong Research Grants Council(Hong Kong Research Grants Council),This work was supported in part by the Hong Kong Research Grants Council (17203217 and 17201818) and theUniversity of Hong Kong (104005009 and 104005438).,,61,24,24,26,51,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,873,886,,10.1109/TPAMI.2019.2945027,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31581075,Green Submitted,Y,N,2022-05-23,WOS:000616309900009
J,"Shu, XB; Tang, JH; Qi, GJ; Liu, W; Yang, J",,,,"Shu, Xiangbo; Tang, Jinhui; Qi, Guo-Jun; Liu, Wei; Yang, Jian",,,Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Dynamics; Videos; Logic gates; Deep learning; Task analysis; Pattern recognition; Feeds; Human interaction recognition; long short-term memory; activity recognition; deep learning,,"In this work, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons. Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability to capture the temporal motion information in a range. However, most existing LSTM-based methods focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole. Such methods neglect the inter-related dynamics of how human interactions change over time. To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing human interactions. Specifically, we first feed each person's static features into a Single-Person LSTM to model the single-person dynamic. Subsequently, at one time step, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate, and a new co-memory cell. In the Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively. Extensive experiments on several public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.","[Shu, Xiangbo; Tang, Jinhui; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China; [Qi, Guo-Jun] Huawei Cloud, Bellevue, WA 98004 USA; [Liu, Wei] Tencent AI Lab, Comp Vis Grp, Shenzhen 518000, Peoples R China",,"Tang, JH (通讯作者)，Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.",shuxb@njust.edu.cn; jinhuitang@njust.edu.cn; guojun.qi@huawei.com; wliu@ee.columbia.edu; csjyang@njust.edu.cn,"Shu, Xiangbo/AAC-6245-2022","Shu, Xiangbo/0000-0003-4902-4663; Tang, Jinhui/0000-0001-9008-222X; Liu, Wei/0000-0002-3865-8145","National Key Research and Development Program of China [2016YFB1001001]; National Natural Science Foundation of China [61732007, 61702265, 61672285, 61772268]; National Natural Science Foundation of Jiangsu Province [BK20170856]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province),"This work was partially supported by theNational Key Research and Development Program of China (Grant No. 2016YFB1001001), the National Natural Science Foundation of China (Grant No. 61732007, 61702265, 61672285, and 61772268), and the National Natural Science Foundation of Jiangsu Province (GrantNo. BK20170856).",,49,37,38,41,72,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,1110,1118,,10.1109/TPAMI.2019.2942030,,,9,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31545711,Green Submitted,Y,Y,2022-05-23,WOS:000616309900026
J,"Sun, YF; Zheng, L; Li, YL; Yang, Y; Tian, Q; Wang, SJ",,,,"Sun, Yifan; Zheng, Liang; Li, Yali; Yang, Yi; Tian, Qi; Wang, Shengjin",,,Learning Part-based Convolutional Features for Person Re-Identification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Pose estimation; Training; Feature extraction; Deep learning; Semantics; Sun; Labeling; Person re-identification; part-based convolutional baseline; part refinement,NETWORK,"Part-level features offer fine granularity for pedestrian image description. In this article, we generally aim to learn discriminative part-informed feature for person re-identification. Our contribution is two-fold. First, we introduce a general part-level feature learning method, named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. PCB is general in that it is able to accommodate several part partitioning strategies, including pose estimation, human parsing and uniform part partitioning. In experiment, we show that the learned descriptor has a significantly higher discriminative ability than the global descriptor. Second, based on PCB, we propose refined part pooling (RPP), which allows the parts to be more precisely located. Our idea is that pixels within a well-located part should be similar to each other while being dissimilar with pixels from other parts. We call it within-part consistency. When a pixel-wise feature vector in a part is more similar to some other part, it is then an outlier, indicating inappropriate partitioning. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. RPP requires no part labels and is trained in a weakly supervised manner. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2) percent mAP and (92.3+1.5) percent rank-1 accuracy, a competitive performance with the state of the art.","[Sun, Yifan; Li, Yali; Wang, Shengjin] Tsinghua Univ, Beijing 100084, Peoples R China; [Zheng, Liang] Australian Natl Univ, Canberra, ACT 0200, Australia; [Yang, Yi] Univ Technol Sydney, Ultimo, NSW 2007, Australia; [Tian, Qi] Univ Texas San Antonio, San Antonio, TX 78249 USA",,"Sun, YF (通讯作者)，Tsinghua Univ, Beijing 100084, Peoples R China.",sunyf15@mail.tsinghua.org.cn; liangzheng06@gmail.com; liyali13@tsinghua.edu.cn; yee.i.yang@gmail.com; Qi.Tian@utsa.edu; wgsgj@tsinghua.edu.cn,"Yang, Yi/B-9273-2017","Yang, Yi/0000-0002-0512-880X",National Natural Science Foundation of China [61771288],National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),This work was supported by the National Natural Science Foundation of China under Grant No. 61771288.,,67,26,26,8,36,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,MAR 1,2021,43,3,,,,,902,917,,10.1109/TPAMI.2019.2938523,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QE6IS,31502963,,Y,N,2022-05-23,WOS:000616309900011
J,"Borji, A",,,,"Borji, Ali",,,Saliency Prediction in the Deep Learning Era: Successes and Limitations,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Predictive models; Computational modeling; Benchmark testing; Data models; Visualization; Deep learning; Task analysis; Visual saliency; eye movement prediction; attention; video saliency; benchmark; deep learning,VISUAL-ATTENTION; EYE-MOVEMENTS; GAZE ALLOCATION; MODEL; SCENE; SPECTRUM; NETWORK,"Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data. Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy. In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets. A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets. Further, I identify factors that contribute to the gap between models and humans and discuss the remaining issues that need to be addressed to build the next generation of more powerful saliency models. Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.","[Borji, Ali] MarkableAI Inc, Brooklyn, NY 11201 USA",,"Borji, A (通讯作者)，MarkableAI Inc, Brooklyn, NY 11201 USA.",aliborji@gmail.com,,"Borji, Ali/0000-0001-8198-0335",,,,,153,38,38,18,44,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2021,43,2,,,,,679,700,,10.1109/TPAMI.2019.2935715,,,22,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI),Computer Science; Engineering,PR6ZZ,31425064,,Y,N,2022-05-23,WOS:000607383300020
J,"Gao, SH; Cheng, MM; Zhao, K; Zhang, XY; Yang, MH; Torr, P",,,,"Gao, Shang-Hua; Cheng, Ming-Ming; Zhao, Kai; Zhang, Xin-Yu; Yang, Ming-Hsuan; Torr, Philip",,,Res2Net: A New Multi-Scale Backbone Architecture,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Feature extraction; Task analysis; Object detection; Semantics; Computer architecture; Kernel; Convolution; Multi-scale; deep learning,SALIENT OBJECT DETECTION,"Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.","[Gao, Shang-Hua; Cheng, Ming-Ming; Zhao, Kai; Yang, Ming-Hsuan] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300350, Peoples R China; [Zhang, Xin-Yu] UC Merced, Merced, CA 95343 USA; [Torr, Philip] Univ Oxford, Oxford OX1 2JD, England",,"Cheng, MM (通讯作者)，Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300350, Peoples R China.",shgao@mail.nankai.edu.cn; cmm@nankai.edu.cn; kaiz.xyz@gmail.com; xinyuzhang@mail.nankai.edu.cn; mhyang@ucmerced.edu; philip.torr@eng.ox.ac.uk,"Cheng, Ming-Ming/A-2527-2009; Yang, Ming-Hsuan/T-9533-2019","Cheng, Ming-Ming/0000-0001-5550-8758; ZHAO, KAI/0000-0002-2496-0829; Yang, Ming-Hsuan/0000-0003-4848-2304","NSFC [61620106008, 61572264]; national youth talent support program; Tianjin Natural Science Foundation [17JCJQJC43700, 18ZXZNGX00110]; EPSRC [EP/N019474/1] Funding Source: UKRI",NSFC(National Natural Science Foundation of China (NSFC)); national youth talent support program; Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)),"This research was supported by NSFC (NO. 61620106008, 61572264), the national youth talent support program, and Tianjin Natural Science Foundation (17JCJQJC43700, 18ZXZNGX00110). Shang-Hua Gao and Ming-Ming Cheng contributed equally to this work.",,68,411,426,164,231,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2021,43,2,,,,,652,662,,10.1109/TPAMI.2019.2938758,,,11,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PR6ZZ,31484108,Green Submitted,Y,Y,2022-05-23,WOS:000607383300018
J,"Liao, MH; Lyu, PY; He, MH; Yao, C; Wu, WH; Bai, X",,,,"Liao, Minghui; Lyu, Pengyuan; He, Minghang; Yao, Cong; Wu, Wenhao; Bai, Xiang",,,Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Scene text spotting; scene text detection; scene text recognition; arbitrary shapes; attention; segmentation,RECOGNITION; SEQUENCE,"Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.","[Liao, Minghui; Lyu, Pengyuan; He, Minghang; Bai, Xiang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China; [Yao, Cong; Wu, Wenhao] Megvii Face Inc, Beijing 100190, Peoples R China",,"Bai, X (通讯作者)，Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.",mhliao@hust.edu.cn; lvpyuan@gmail.com; minghanghe@hust.edu.cn; yaocong2010@gmail.com; wwh@megvii.com; xbai@mail.hust.edu.cn,,"Bai, Xiang/0000-0002-3449-5940; Liao, Minghui/0000-0002-2583-4314",National Key R&D Program of China [2018YFB1004600]; National Program for Support of Topnotch Young Professionals; NSFC [61733007]; Program for HUST Academic Frontier Youth Team [2017QYTD08],National Key R&D Program of China; National Program for Support of Topnotch Young Professionals; NSFC(National Natural Science Foundation of China (NSFC)); Program for HUST Academic Frontier Youth Team,"Minghui Liao and Pengyuan Lyu contributed equally to this article. Xiang Bai is the corresponding author. This work was supported by National Key R&D Program of China No. 2018YFB1004600, NSFC 61733007. Dr. Xiang Bai was supported by the National Program for Support of Topnotch Young Professionals and the Program for HUST Academic Frontier Youth Team 2017QYTD08.",,82,60,64,26,58,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB,2021,43,2,,,,,532,548,,10.1109/TPAMI.2019.2937086,,,17,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PS2ZD,31449005,Green Submitted,Y,N,2022-05-23,WOS:000607795000002
J,"Tellez, D; Litjens, G; van der Laak, J; Ciompi, F",,,,"Tellez, David; Litjens, Geert; van der Laak, Jeroen; Ciompi, Francesco",,,Neural Image Compression for Gigapixel Histopathology Image Analysis,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Image coding; Training; Image reconstruction; Image analysis; Neural networks; Visualization; Task analysis; Gigapixel image analysis; computational pathology; convolutional neural networks; representation learning,,"We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.","[Tellez, David; Litjens, Geert; van der Laak, Jeroen; Ciompi, Francesco] Radboud Univ Nijmegen, Dept Pathol, Diagnost Image Anal Grp, Med Ctr, NL-6500 HB Nijmegen, Netherlands",,"Tellez, D (通讯作者)，Radboud Univ Nijmegen, Dept Pathol, Diagnost Image Anal Grp, Med Ctr, NL-6500 HB Nijmegen, Netherlands.",david.tellezm@gmail.com; Geert.Litjens@radboudumc.nl; Jeroen.vanderlaak@radboudumc.nl; francesco.ciompi@radboudumc.nl,"van der Laak, Jeroen AWM/D-3057-2015; Litjens, Geert JS/A-2319-2016; Ciompi, Francesco/P-5598-2015","van der Laak, Jeroen AWM/0000-0001-7982-0754; Litjens, Geert JS/0000-0003-1554-1291; Ciompi, Francesco/0000-0001-8327-9606","Junior Researcher grant from the Radboud Institute of Health Sciences (RIHS), Nijmegen, The Netherlands; Dutch Cancer Society [KUN 2015-7970]; Dutch Cancer Society; Alpe d'HuZes fund [KUN 2014-7032]; European Union's Horizon 2020 research and innovation programme [825292]","Junior Researcher grant from the Radboud Institute of Health Sciences (RIHS), Nijmegen, The Netherlands(Netherlands Government); Dutch Cancer Society(KWF Kankerbestrijding); Dutch Cancer Society(KWF Kankerbestrijding); Alpe d'HuZes fund; European Union's Horizon 2020 research and innovation programme","This study was supported by a Junior Researcher grant from the Radboud Institute of Health Sciences (RIHS), Nijmegen, The Netherlands; a grant from the Dutch Cancer Society (KUN 2015-7970); and another grant from the Dutch Cancer Society and the Alpe d'HuZes fund (KUN 2014-7032); this project has also been partially funded by the European Union's Horizon 2020 research and innovation programme under grant agreement No 825292. The authors would like to thank Dr. Mitko Veta for evaluating our predictions in the test set of the TUPAC16 dataset, and the developers of Keras [45], the open source tool that we used to run our deep learning experiments.",,45,30,31,17,40,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,FEB 1,2021,43,2,,,,,567,578,,10.1109/TPAMI.2019.2936841,,,12,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PR6ZZ,31442971,Green Submitted,Y,N,2022-05-23,WOS:000607383300012
J,"Cao, Z; Hidalgo, G; Simon, T; Wei, SE; Sheikh, Y",,,,"Cao, Zhe; Hidalgo, Gines; Simon, Tomas; Wei, Shih-En; Sheikh, Yaser",,,OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,2D human pose estimation; 2D foot keypoint estimation; real-time; multiple person; part affinity fields,PICTORIAL STRUCTURES; PEOPLE; MODELS,"Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of Open Pose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.","[Cao, Zhe] Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94709 USA; [Hidalgo, Gines; Sheikh, Yaser] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Simon, Tomas; Wei, Shih-En] Facebook Real Labs, Pittsburgh, PA 15213 USA",,"Cao, Z (通讯作者)，Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94709 USA.",zhecao@berkeley.edu; gines@alumni.cmu.edu; tomas.simon@oculus.com; shih-en.wei@oculus.com; yaser@cs.cmu.edu,"Hidalgo, Ginés/ABF-9472-2020","Hidalgo, Ginés/0000-0001-7206-0131; Fernandez Ramos, Oscar/0000-0002-4133-1872",Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00340],Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC),We acknowledge the effort from the authors of the MPII and COCO human pose datasets. These datasets make 2D human pose estimation in the wild possible. This research was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00340.,,78,810,833,154,296,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,172,186,,10.1109/TPAMI.2019.2929257,,,15,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31331883,Green Submitted,Y,Y,2022-05-23,WOS:000597206900012
J,"Chen, ZY; Cong, RM; Xu, QQ; Huang, QM",,,,"Chen, Zuyao; Cong, Runmin; Xu, Qianqian; Huang, Qingming",,,DPANet: Depth Potentiality-Aware Gated Attention Network for RGB-D Salient Object Detection,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Logic gates; Object detection; Contamination; Task analysis; Saliency detection; Computer science; Image color analysis; Salient object detection; RGB-D images; depth potentiality perception; gated multi-modality attention,FUSION,"There are two main issues in RGB-D salient object detection: (1) how to effectively integrate the complementarity from the cross-modal RGB-D data; (2) how to prevent the contamination effect from the unreliable depth map. In fact, these two problems are linked and intertwined, but the previous methods tend to focus only on the first problem and ignore the consideration of depth map quality, which may yield the model fall into the sub-optimal state. In this paper, we address these two issues in a holistic model synergistically, and propose a novel network named DPANet to explicitly model the potentiality of the depth map and effectively integrate the cross-modal complementarity. By introducing the depth potentiality perception, the network can perceive the potentiality of depth information in a learning-based manner, and guide the fusion process of two modal data to prevent the contamination occurred. The gated multi-modality attention module in the fusion process exploits the attention mechanism with a gate controller to capture long-range dependencies from a cross-modal perspective. Experimental results compared with 16 state-of-the-art methods on 8 datasets demonstrate the validity of the proposed approach both quantitatively and qualitatively. https://github.com/JosephChenHub/DPANet","[Chen, Zuyao] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100190, Peoples R China; [Cong, Runmin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China; [Cong, Runmin] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China; [Cong, Runmin] CUNY, Dept Comp Sci, Hong Kong, Peoples R China; [Xu, Qianqian] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China; [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Huang, Qingming] Peng Cheng Lab, Shenzhen 518055, Peoples R China",,"Xu, QQ (通讯作者)，Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Huang, QM (通讯作者)，Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.; Huang, QM (通讯作者)，Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China.; Huang, QM (通讯作者)，Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Huang, QM (通讯作者)，Peng Cheng Lab, Shenzhen 518055, Peoples R China.",chenzuyao17@mails.ucas.ac.cn; rmcong@bjtu.edu.cn; xuqianqian@ict.ac.cn; qmhuang@ucas.ac.cn,,"Chen, Zuyao/0000-0002-7344-1101","National Key Research and Development Program of China [2018AAA0102003]; Beijing Nova Program [Z201100006820016]; National Natural Science Foundation of China [61931008, 62002014, 61620106009, U1636214, 61836002, 61672514, 61976202]; Key Research Program of Frontier Sciences [CAS: QYZDJ-SSW-SYS013]; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; Beijing Natural Science Foundation [4182079]; Youth Innovation Promotion Association CAS; Hong Kong Scholars Program; Elite Scientist Sponsorship Program - Beijing Association for Science and Technology; China Postdoctoral Science Foundation [2020T130050, 2019M660438]; Fundamental Research Funds for the Central Universities [2019RC039]; CAAI-Huawei MindSpore Open Fund",National Key Research and Development Program of China; Beijing Nova Program(Beijing Municipal Science & Technology Commission); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research Program of Frontier Sciences; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Youth Innovation Promotion Association CAS; Hong Kong Scholars Program; Elite Scientist Sponsorship Program - Beijing Association for Science and Technology; China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); CAAI-Huawei MindSpore Open Fund,"This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0102003, in part by the Beijing Nova Program under Grant Z201100006820016, in part by the National Natural Science Foundation of China under Grant 61931008, Grant 62002014, Grant 61620106009, Grant U1636214, Grant 61836002, Grant 61672514, and Grant 61976202, in part by the Key Research Program of Frontier Sciences under Grant CAS: QYZDJ-SSW-SYS013, in part by the Strategic Priority Research Program of Chinese Academy of Sciences under Grant XDB28000000, in part by the Beijing Natural Science Foundation under Grant 4182079, in part by the Youth Innovation Promotion Association CAS, in part by Hong Kong Scholars Program, in part by Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology, in part by CAAI-Huawei MindSpore Open Fund, in part by the China Postdoctoral Science Foundation under Grant 2020T130050 and Grant 2019M660438, and in part by the Fundamental Research Funds for the Central Universities under Grant 2019RC039.",,65,26,26,11,18,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,7012,7024,,10.1109/TIP.2020.3028289,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,TY7UJ,33141667,Green Submitted,Y,N,2022-05-23,WOS:000683985500001
J,"Jiang, YF; Gong, XY; Liu, D; Cheng, Y; Fang, C; Shen, XH; Yang, JC; Zhou, P; Wang, ZY",,,,"Jiang, Yifan; Gong, Xinyu; Liu, Ding; Cheng, Yu; Fang, Chen; Shen, Xiaohui; Yang, Jianchao; Zhou, Pan; Wang, Zhangyang",,,EnlightenGAN: Deep Light Enhancement Without Paired Supervision,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Training; Visualization; Lighting; Generative adversarial networks; Gallium nitride; Adaptation models; Training data; Low-light enhancement; generative adversarial networks; unsupervised learning,RETINEX,"Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN.","[Jiang, Yifan; Gong, Xinyu; Wang, Zhangyang] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Liu, Ding; Cheng, Yu; Fang, Chen; Shen, Xiaohui; Yang, Jianchao] Bytedance Inc, Mountain View, CA 94041 USA; [Cheng, Yu] Microsoft AI & Res, Redmond, WA 98052 USA; [Zhou, Pan] Huazhong Univ Sci & Technol, Dept Elect Informat & Commun, Wuhan 430074, Peoples R China",,"Wang, ZY (通讯作者)，Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.",yifanjiang97@utexas.edu; xinyu.gong@utexas.edu; liuding@bytedance.com; yu.cheng@microsoft.com; fangchen@bytedance.com; shenxiaohui.kevin@bytedance.com; yangjianchao@bytedance.com; panzhou@hust.edu.cn; atlaswang@utexas.edu,"Jiang, Yifan/ABB-4400-2021","Jiang, Yifan/0000-0002-7113-305X",,,,,59,94,97,53,78,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,2340,2349,,10.1109/TIP.2021.3051462,,,10,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QA4GN,33481709,Green Submitted,Y,Y,2022-05-23,WOS:000613403600019
J,"Jing, XY; Zhang, XY; Zhu, XK; Wu, F; You, XG; Gao, Y; Shan, SG; Yang, JY",,,,"Jing, Xiao-Yuan; Zhang, Xinyu; Zhu, Xiaoke; Wu, Fei; You, Xinge; Gao, Yang; Shan, Shiguang; Yang, Jing-Yu",,,Multiset Feature Learning for Highly Imbalanced Data Classification,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Highly imbalanced data classification; multiset feature learning; deep metric learning; generative adversarial network; cost-sensitive factor; weighted uncorrelated constraint,DATA-SETS; FRAMEWORK; SMOTE,"With the expansion of data, increasing imbalanced data has emerged. When the imbalance ratio (IR) of data is high, most existing imbalanced learning methods decline seriously in classification performance. In this paper, we systematically investigate the highly imbalanced data classification problem, and propose an uncorrelated cost-sensitive multiset learning (UCML) approach for it. Specifically, UCML first constructs multiple balanced subsets through random partition, and then employs the multiset feature learning (MFL) to learn discriminant features from the constructed multiset. To enhance the usability of each subset and deal with the nonlinearity issue existed in each subset, we further propose a deep metric based UCML (DM-UCML) approach. DM-UCML introduces the generative adversarial network technique into the multiset constructing process, such that each subset can own similar distribution with the original dataset. To cope with the non-linearity issue, DM-UCML integrates deep metric learning with MFL, such that more favorable performance can be achieved. In addition, DM-UCML designs a new discriminant term to enhance the discriminability of learned metrics. Experiments on eight traditional highly class-imbalanced datasets and two large-scale datasets indicate that: the proposed approaches outperform state-of-the-art highly imbalanced learning methods and are more robust to high IR.","[Jing, Xiao-Yuan; Zhang, Xinyu] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China; [Jing, Xiao-Yuan] Guangdong Univ Petrochem Technol, Sch Comp, Maoming 525000, Peoples R China; [Jing, Xiao-Yuan; Wu, Fei] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210003, Peoples R China; [Zhu, Xiaoke] Henan Univ, Henan Key Lab Big Data Anal & Proc, Kaifeng 475004, Peoples R China; [You, Xinge] Huazhong Univ Sci & Technol, Dept Elect & Informat Engn, Wuhan 430074, Peoples R China; [Gao, Yang] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210094, Peoples R China; [Shan, Shiguang] Chinese Acad Sci, Inst Comp Technol, CAS, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Yang, Jing-Yu] Nanjing Univ Sci & Technol, Coll Comp Sci, Nanjing 210094, Peoples R China",,"Jing, XY (通讯作者)，Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.; Jing, XY (通讯作者)，Guangdong Univ Petrochem Technol, Sch Comp, Maoming 525000, Peoples R China.; Wu, F (通讯作者)，Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210003, Peoples R China.; Zhu, XK (通讯作者)，Henan Univ, Henan Key Lab Big Data Anal & Proc, Kaifeng 475004, Peoples R China.",jingxy_2000@126.com; zhangxinyu@whu.edu.cn; whuzxk@whu.edu.cn; wufei_8888@126.com; youxg@mail.hust.edu.cn; gaoy@nju.edu.cn; sgshan@ict.ac.cn; yangjy@mail.njust.edu.cn,,"Zhu, Xiaoke/0000-0002-0664-1832; Shan, Shiguang/0000-0002-8348-392X; Wu, Fei/0000-0001-5498-4947; Gao, Yang/0000-0002-2488-1813; zhang, xinyu/0000-0002-9109-1889; You, Xinge/0000-0002-6227-1346","NSFC-Key Project of General Technology Fundamental Research United Fund [U1736211]; National Natural Science Foundation of China [61672208, 61702280, 61772220, 61432008]; key research and development program of China [2016YFE0121200]; Key Science and Technology Innovation Program of Hubei Province [2017AAA017, 2018ACA135]; Natural Science Foundation Key Project for Innovation Group of Hubei Province [2018CFA024]; Natural Science Foundation of Jiangsu Province [BK20170900]; National Postdoctoral Program for Innovative Talents [BX20180146]; Higher Education Institution Key Research Projects of Henan Province [19A520001]",NSFC-Key Project of General Technology Fundamental Research United Fund; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); key research and development program of China; Key Science and Technology Innovation Program of Hubei Province; Natural Science Foundation Key Project for Innovation Group of Hubei Province; Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province); National Postdoctoral Program for Innovative Talents; Higher Education Institution Key Research Projects of Henan Province,"The authors would like to thank the editor, the associate editor, and anonymous reviewers for their constructive comments in helping improve our work. This work was supported by the NSFC-Key Project of General Technology Fundamental Research United Fund No. U1736211, the National Natural Science Foundation of China under Grant Nos. 61672208, 61702280, 61772220 and 61432008, the key research and development program of China under Grant No. 2016YFE0121200, the Key Science and Technology Innovation Program of Hubei Province under Grant Nos. 2017AAA017 and 2018ACA135, the Natural Science Foundation Key Project for Innovation Group of Hubei Province under Grant No. 2018CFA024, the Natural Science Foundation of Jiangsu Province under Grant No. BK20170900, the National Postdoctoral Program for Innovative Talents under Grant No. BX20180146, the Higher Education Institution Key Research Projects of Henan Province under Grant No. 19A520001. Xiao-Yuan Jing and Xinyu Zhang are cofirst authors of this paper.",,74,28,29,62,155,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,139,156,,10.1109/TPAMI.2019.2929166,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31331881,,Y,N,2022-05-23,WOS:000597206900010
J,"Pu, W",,,,"Pu, Wei",,,Deep SAR Imaging and Motion Compensation,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Synthetic aperture radar; Radar polarimetry; Backscatter; Radar imaging; Imaging; Azimuth; Two dimensional displays; Synthetic aperture radar (SAR); auto-encoder; deep learning; convolutional neural network (CNN); motion compensation,,"Compressive sensing (CS) and matrix sensing (MS) techniques have been applied to the synthetic aperture radar (SAR) imaging problem to reduce the sampling amount of SAR echo using the sparse or low-rank prior information. To further exploit the redundancy and improve sampling efficiency, we take a different approach, wherein a deep SAR imaging algorithm is proposed. The main idea is to exploit the redundancy of the backscattering coefficient using an auto-encoder structure, wherein the hidden latent layer in auto-encoder has lower dimension and less parameters than the backscattering coefficient layer. Based on the auto-encoder model, the parameters of the auto-encoder structure and the backscattering coefficient are estimated simultaneously by optimizing the reconstruction loss associated with the down-sampled SAR echo. In addition, in order to meet the practical application requirements, a deep SAR motion compensation algorithm is proposed to eliminate the effect of motion errors on imaging results. The effectiveness of the proposed algorithms is verified on both simulated and real SAR data.","[Pu, Wei] UCL, Dept Elect & Elect Engn, London WC1E 6BT, England",,"Pu, W (通讯作者)，UCL, Dept Elect & Elect Engn, London WC1E 6BT, England.",pwaitaimei@outlook.com,,,Newton International Fellowship from the Royal Society,Newton International Fellowship from the Royal Society,"This work was supported by the Newton International Fellowship from the Royal Society, U. K.",,49,25,25,15,25,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,2232,2247,,10.1109/TIP.2021.3051484,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QA4GN,33471760,,Y,N,2022-05-23,WOS:000613403600011
J,"Ren, DW; Zuo, WM; Zhang, D; Zhang, L; Yang, MH",,,,"Ren, Dongwei; Zuo, Wangmeng; Zhang, David; Zhang, Lei; Yang, Ming-Hsuan",,,Simultaneous Fidelity and Regularization Learning for Image Restoration,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,image restoration; blind deconvolution; rain streak removal; task-driven learning,MINIMIZATION; ALGORITHM,"Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known. As the degradation process can only be partially known or inaccurately modeled, images may not be well restored. Rain streak removal and image deconvolution with inaccurate blur kernels are two representative examples of such tasks. For rain streak removal, although an input image can be decomposed into a scene layer and a rain streak layer, there exists no explicit formulation for modeling rain streaks and the composition with scene layer. For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well. In this paper, we propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model. Specifically, the residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed. With a training set of degraded and ground-truth image pairs, we parameterize and learn the fidelity term for a degradation model in a task-driven manner. Furthermore, the regularization term can also be learned along with the fidelity term, thereby forming a simultaneous fidelity and regularization learning model. Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels, deconvolution with multiple degradations and rain streak removal.","[Ren, Dongwei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China; [Ren, Dongwei; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China; [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China; [Zuo, Wangmeng] Peng Cheng Lab, Shenzhen 518055, Peoples R China; [Zhang, David] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95344 USA",,"Zuo, WM (通讯作者)，Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.",rendongweihit@gmail.com; cswmzuo@gmail.com; csdzhang@comp.polyu.edu.hk; cslzhang@comp.polyu.edu.hk; mhyang@ucmerced.edu,"; Yang, Ming-Hsuan/T-9533-2019","Zhang, Lei/0000-0002-2078-4215; Zuo, Wangmeng/0000-0002-3330-783X; Ren, Dongwei/0000-0002-0965-6810; Yang, Ming-Hsuan/0000-0003-4848-2304","National Natural Scientific Foundation of China (NSFC) [61671182, 61801326]; Hong Kong RGC GRF grant [PolyU 152124/15E]; US National Science Foundation CAREER Grant [1149783]",National Natural Scientific Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Hong Kong RGC GRF grant; US National Science Foundation CAREER Grant(National Science Foundation (NSF)),"This work is supported in part by National Natural Scientific Foundation of China (NSFC) under grant (61671182 and 61801326), Hong Kong RGC GRF grant (PolyU 152124/15E), and US National Science Foundation CAREER Grant No.1149783.",,64,27,27,7,28,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,284,299,,10.1109/TPAMI.2019.2926357,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31283494,Green Submitted,Y,N,2022-05-23,WOS:000597206900019
J,"Sun, H; Zheng, XT; Lu, XQ",,,,"Sun, Hao; Zheng, Xiangtao; Lu, Xiaoqiang",,,A Supervised Segmentation Network for Hyperspectral Image Classification,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Hyperspectral imaging; Feature extraction; Training; Task analysis; Imaging; Image segmentation; Testing; Hyperspectral image (HSI) classification; fully convolutional segmentation network (FCSN); generalization,FEATURE FUSION; CNN; ARCHITECTURE,"Recently, deep learning has drawn broad attention in the hyperspectral image (HSI) classification task. Many works have focused on elaborately designing various spectral-spatial networks, where convolutional neural network (CNN) is one of the most popular structures. To explore the spatial information for HSI classification, pixels with its adjacent pixels are usually directly cropped from hyperspectral data to form HSI cubes in CNN-based methods. However, the spatial land-cover distributions of cropped HSI cubes are usually complicated. The land-cover label of a cropped HSI cube cannot simply be determined by its center pixel. In addition, the spatial land-cover distribution of a cropped HSI cube is fixed and has less diversity. For CNN-based methods, training with cropped HSI cubes will result in poor generalization to the changes of spatial land-cover distributions. In this paper, an end-to-end fully convolutional segmentation network (FCSN) is proposed to simultaneously identify land-cover labels of all pixels in a HSI cube. First, several experiments are conducted to demonstrate that recent CNN-based methods show the weak generalization capabilities. Second, a fine label style is proposed to label all pixels of HSI cubes to provide detailed spatial land-cover distributions of HSI cubes. Third, a HSI cube generation method is proposed to generate plentiful HSI cubes with fine labels to improve the diversity of spatial land-cover distributions. Finally, a FCSN is proposed to explore spectral-spatial features from finely labeled HSI cubes for HSI classification. Experimental results show that FCSN has the superior generalization capability to the changes of spatial land-cover distributions.","[Sun, Hao; Zheng, Xiangtao; Lu, Xiaoqiang] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China; [Sun, Hao] Univ Chinese Acad Sci, Beijing 100049, Peoples R China",,"Zheng, XT (通讯作者)，Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China.",xiangtaoz@gmail.com,"Sun, Hao/AAG-5676-2020","Sun, Hao/0000-0002-1314-4957; Lu, Xiaoqiang/0000-0002-7037-5188","National Science Fund for Distinguished Young Scholars [61925112]; National Natural Science Foundation of China [61806193, 61772510]; Innovation Capability Support Program of Shaanxi [2020KJXX-091, 2020TD-015]; Natural Science Basic Research Program of Shaanxi [2019JQ340, 2019JC-23]; CAAI-Huawei MindSpore Open Fund",National Science Fund for Distinguished Young Scholars(National Natural Science Foundation of China (NSFC)National Science Fund for Distinguished Young Scholars); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Capability Support Program of Shaanxi; Natural Science Basic Research Program of Shaanxi; CAAI-Huawei MindSpore Open Fund,"This work was supported in part by the National Science Fund for Distinguished Young Scholars under Grant 61925112, in part by the National Natural Science Foundation of China under Grant 61806193 and Grant 61772510, in part by the Innovation Capability Support Program of Shaanxi under Grant 2020KJXX-091 and Grant 2020TD-015, in part by the Natural Science Basic Research Program of Shaanxi under Grant 2019JQ340 and 2019JC-23, and in part by CAAI-Huawei MindSpore Open Fund. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jocelyn Chanussot.",,51,27,28,15,32,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,2810,2825,,10.1109/TIP.2021.3055613,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QG7JB,33539293,,Y,Y,2022-05-23,WOS:000617758400007
J,"Sun, SJ; Akhtar, N; Song, HS; Mian, AS; Shah, M",,,,"Sun, Shijie; Akhtar, Naveed; Song, HuanSheng; Mian, Ajmal S.; Shah, Mubarak",,,Deep Affinity Network for Multiple Object Tracking,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Multiple object tracking; deep tracking; deep affinity; tracking challenge; on-line tracking,MULTITARGET TRACKING; PERFORMANCE; ALGORITHM,"Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision. Most MOT methods employ two steps: Object Detection and Data Association. The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks. Object detection has made tremendous progress in the last few years due to deep learning. However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames. In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion. The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities. DAN also accounts for multiple objects appearing and disappearing between video frames. We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking. Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges. The open source implementation of our work is available at https://github.com/shijieS/SST.git.","[Sun, Shijie; Song, HuanSheng] Changan Univ, Sch Informat Engn, Xian 710000, Shaanxi, Peoples R China; [Akhtar, Naveed; Mian, Ajmal S.] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia; [Shah, Mubarak] Univ Cent Florida, Ctr Res Comp Vis CRCV, Dept Comp Sci, Orlando, FL 32816 USA",,"Song, HS (通讯作者)，Changan Univ, Sch Informat Engn, Xian 710000, Shaanxi, Peoples R China.",shijieSun@chd.edu.cn; naveed.akhtar@uwa.edu.au; hshsong@chd.edu.cn; ajmal.mian@uwa.edu.au; shah@crcv.ucf.edu,"AKHTAR, NAVEED/AAT-1283-2020","AKHTAR, NAVEED/0000-0003-3406-673X; Mian, Ajmal/0000-0002-5206-3842","ARC [DP160101458, DP190102443]; National Natural Science Foundation of China [61572083]; Joint Found of of Ministry of Education of China [6141A02022610]; Key projects of key R&D projects in Shaanxi [2018ZDXM-GY047]; Team cultivation project of Central University [300102248402]; China Scholarship Council; IARPA, via IARPA RD [D17PC00345]; ODNI","ARC(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Joint Found of of Ministry of Education of China(Ministry of Education, China); Key projects of key R&D projects in Shaanxi; Team cultivation project of Central University; China Scholarship Council(China Scholarship Council); IARPA, via IARPA RD; ODNI","This research was supported by ARC grant DP160101458 and DP190102443, the National Natural Science Foundation of China (Grant No. 61572083), the Joint Found of of Ministry of Education of China (Grant No. 6141A02022610), Key projects of key R&D projects in Shaanxi (Grant No. 2018ZDXM-GY047), Team cultivation project of Central University (Grant No. 300102248402) and China Scholarship Council. Mubarak Shah acknowledges the support of ODNI, and IARPA, via IARPA R&D Contract No. D17PC00345. The views, findings, opinions, and conclusions or recommendations contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose not withstanding any copyright annotation thereon. The GPU used for this work was donated by NVIDIA Corporation.",,96,64,67,40,119,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,104,119,,10.1109/TPAMI.2019.2929520,,,16,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31329110,Green Submitted,Y,N,2022-05-23,WOS:000597206900008
J,"Wang, WG; Shen, JB; Xie, JW; Cheng, MM; Ling, HB; Borji, A",,,,"Wang, Wenguan; Shen, Jianbing; Xie, Jianwen; Cheng, Ming-Ming; Ling, Haibin; Borji, Ali",,,Revisiting Video Saliency Prediction in the Deep Learning Era,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Video saliency; dynamic visual attention; benchmark; deep learning,VISUAL-ATTENTION; DETECTION MODEL,"Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interest recently. However, relatively less effort has been spent in understanding and modeling visual attention over dynamic scenes. This work makes three contributions to video saliency research. First, we introduce a new benchmark, called DHF1K (Dynamic Human Fixation 1K), for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field. DHF1K consists of 1K high-quality elaborately-selected video sequences annotated by 17 observers using an eye tracker device. The videos span a wide range of scenes, motions, object types and backgrounds. Second, we propose a novel video saliency model, called ACLNet (Attentive CNN-LSTM Network), that augments the CNN-LSTM architecture with a supervised attention mechanism to enable fast end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. Third, we perform an extensive evaluation of the state-of-the-art saliency models on three datasets : DHF1K, Hollywood-2, and UCF sports. An attribute-based analysis of previous saliency models and cross-dataset generalization are also presented. Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40 fps using a single GPU). Our code and all the results are available at https://github.com/wenguanwang/DHF1K.","[Wang, Wenguan; Shen, Jianbing] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China; [Wang, Wenguan; Shen, Jianbing] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Xie, Jianwen] Hikvis Res Inst, City Of Industry, CA 91748 USA; [Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Nankai 300071, Peoples R China; [Ling, Haibin] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA; [Borji, Ali] MarkableAI, New York, NY 11201 USA",,"Shen, JB (通讯作者)，Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.",wenguanwang.ai@gmail.com; shenjianbingcg@gmail.com; Jianwen.Xie@hikvision.com; cmm@nankai.edu.cn; hbling@temple.edu; aliborji@gmail.com,"Wang, Wenguan/AAA-5782-2022; Cheng, Ming-Ming/A-2527-2009","Wang, Wenguan/0000-0002-0802-9567; Cheng, Ming-Ming/0000-0001-5550-8758; Ling, Haibin/0000-0003-4094-8413","Beijing Natural Science Foundation [4182056]; National Natural Science Foundation of China [61572264]; national youth talent support program; Tianjin Natural Science Foundation [17JCJQJC43700, 18ZXZNGX00110]; Specialized Fund for Joint Building Program of Beijing Municipal Education Commission",Beijing Natural Science Foundation(Beijing Natural Science Foundation); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); national youth talent support program; Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin); Specialized Fund for Joint Building Program of Beijing Municipal Education Commission,"This work was supported in part by the Beijing Natural Science Foundation under Grant 4182056, the National Natural Science Foundation of China under Grant 61572264, the national youth talent support program, and the Tianjin Natural Science Foundation under grants 17JCJQJC43700 and 18ZXZNGX00110, and the Specialized Fund for Joint Building Program of Beijing Municipal Education Commission. A preliminary version of this work has appeared in CVPR 2018 [1].",,97,79,80,20,44,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,220,237,,10.1109/TPAMI.2019.2924417,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31247542,,Y,N,2022-05-23,WOS:000597206900015
J,"Wu, TY; Tang, S; Zhang, R; Cao, J; Zhang, YD",,,,"Wu, Tianyi; Tang, Sheng; Zhang, Rui; Cao, Juan; Zhang, Yongdong",,,CGNet: A Light-Weight Context Guided Network for Semantic Segmentation,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Semantics; Image segmentation; Context modeling; Computer architecture; Computational modeling; Mobile handsets; Predictive models; Semantic segmentation; surrounding context; global context; context guided,,"The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters hence unsuitable for mobile devices, while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the joint feature of both local feature and surrounding context effectively and efficiently, and further improves the joint feature with the global context. Based on the CG block, we develop CGNet which captures contextual information in all stages of the network. CGNet is specially tailored to exploit the inherent property of semantic segmentation and increase the segmentation accuracy. Moreover, CGNet is elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing light-weight segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters.","[Wu, Tianyi; Tang, Sheng; Zhang, Rui; Cao, Juan; Zhang, Yongdong] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China; [Wu, Tianyi; Tang, Sheng; Zhang, Rui; Cao, Juan; Zhang, Yongdong] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China",,"Tang, S (通讯作者)，Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.",wutianyi@ict.ac.cn; ts@ict.ac.cn; zhangrui@ict.ac.cn; caojuan@ict.ac.cn; zhyd@ict.ac.cn,"Tang, Sheng/L-5792-2013","Tang, Sheng/0000-0003-3573-2407; Wu, Tianyi/0000-0001-7434-0487","National Key Research and Development Program of China [2017YFC0820605]; National Natural Science Foundation of China [61525206, U1703261, 61871004]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)),"This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFC0820605 and in part by the National Natural Science Foundation of China under Grant 61525206, Grant U1703261, and Grant 61871004.",,59,56,58,37,73,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,1169,1179,,10.1109/TIP.2020.3042065,,,11,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PI1CF,33306466,Green Submitted,Y,N,2022-05-23,WOS:000600835900003
J,"Wu, YH; Gao, SH; Mei, J; Xu, J; Fan, DP; Zhang, RG; Cheng, MM",,,,"Wu, Yu-Huan; Gao, Shang-Hua; Mei, Jie; Xu, Jun; Fan, Deng-Ping; Zhang, Rong-Guo; Cheng, Ming-Ming",,,JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,COVID-19; Computed tomography; Image segmentation; Sensitivity; X-rays; Pandemics; Lung; COVID-19; joint diagnosis; CT classification; CT segmentation; COVID-19 dataset,CT,"Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.5% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.","[Wu, Yu-Huan; Gao, Shang-Hua; Mei, Jie; Fan, Deng-Ping; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300071, Peoples R China; [Wu, Yu-Huan; Zhang, Rong-Guo] InferVision, Beijing 100089, Peoples R China; [Xu, Jun] Nankai Univ, Sch Stat & Data Sci, Tianjin 300071, Peoples R China",,"Cheng, MM (通讯作者)，Nankai Univ, Coll Comp Sci, TKLNDST, Tianjin 300071, Peoples R China.",wuyuhuan@mail.nankai.edu.cn; shgao@mail.nankai.edu.cn; meijie0507@gmail.com; nankaimathxujun@gmail.com; dengpfan@gmail.com; zrongguo@infervision.com; cmm@nankai.edu.cn,"Wu, Yu-Huan/ABG-4219-2020; Fan, DengPing/AAT-6679-2020; Cheng, Ming-Ming/A-2527-2009","Wu, Yu-Huan/0000-0001-8666-3435; Fan, DengPing/0000-0002-5245-7518; Cheng, Ming-Ming/0000-0001-5550-8758; Mei, Jie/0000-0002-9789-4025","Major Project for New Generation of AI [2018AAA0100400]; NSFC [61922046, 62002176]; Tianjin Natural Science Foundation [17JCJQJC43700, 18ZXZNGX00110]",Major Project for New Generation of AI; NSFC(National Natural Science Foundation of China (NSFC)); Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin),"This work was supported in part by the Major Project for New Generation of AI under Grant 2018AAA0100400, in part by the NSFC under Grant 61922046 and Grant 62002176, and in part by the Tianjin Natural Science Foundation under Grant 17JCJQJC43700 and Grant 18ZXZNGX00110. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Christophoros Nikou.",,76,69,70,29,40,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,3113,3126,,10.1109/TIP.2021.3058783,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QM9MC,33600316,"Bronze, Green Submitted",Y,Y,2022-05-23,WOS:000622096400006
J,"Xue, JZ; Zhao, YQ; Bu, YY; Liao, WZ; Chan, JCW; Philips, W",,,,"Xue, Jize; Zhao, Yong-Qiang; Bu, Yuanyang; Liao, Wenzhi; Chan, Jonathan Cheung-Wai; Philips, Wilfried",,,Spatial-Spectral Structured Sparse Low-Rank Representation for Hyperspectral Image Super-Resolution,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Superresolution; Sparse matrices; Spatial resolution; Dictionaries; Correlation; Tensors; Task analysis; Hyperspectral and multispectral images fusion; low-rank representation; structured sparse; subspace low-rank recovery; affinity matrix,TENSOR FACTORIZATION; OBJECT DETECTION; FUSION; REGULARIZATION; DECOMPOSITION; ALGORITHM; PIXEL,"Hyperspectral image super-resolution by fusing high-resolution multispectral image (HR-MSI) and low-resolution hyperspectral image (LR-HSI) aims at reconstructing high resolution spatial-spectral information of the scene. Existing methods mostly based on spectral unmixing and sparse representation are often developed from a low-level vision task perspective, they cannot sufficiently make use of the spatial and spectral priors available from higher-level analysis. To this issue, this paper proposes a novel HSI super-resolution method that fully considers the spatial/spectral subspace low-rank relationships between available HR-MSI/LR-HSI and latent HSI. Specifically, it relies on a new subspace clustering method named structured sparse low-rank representation (SSLRR), to represent the data samples as linear combinations of the bases in a given dictionary, where the sparse structure is induced by low-rank factorization for the affinity matrix. Then we exploit the proposed SSLRR model to learn the SSLRR along spatial/spectral domain from the MSI/HSI inputs. By using the learned spatial and spectral low-rank structures, we formulate the proposed HSI super-resolution model as a variational optimization problem, which can be readily solved by the ADMM algorithm. Compared with state-of-the-art hyperspectral super-resolution methods, the proposed method shows better performance on three benchmark datasets in terms of both visual and quantitative evaluation.","[Xue, Jize; Zhao, Yong-Qiang; Bu, Yuanyang] Northwestern Polytech Univ Shenzhen, Res & Dev Inst, Shenzhen 518057, Peoples R China; [Xue, Jize; Liao, Wenzhi; Philips, Wilfried] Univ Ghent, IMEC Res Grp, Image Proc & Interpretat, B-9000 Ghent, Belgium; [Liao, Wenzhi] Flemish Inst Technol Res VITO, Sustainable Mat Management, B-2400 Mol, Belgium; [Chan, Jonathan Cheung-Wai] Vrije Univ Brussel, Dept Elect & Informat, B-1050 Brussels, Belgium",,"Zhao, YQ (通讯作者)，Northwestern Polytech Univ Shenzhen, Res & Dev Inst, Shenzhen 518057, Peoples R China.",xuejize900507@mail.nwpu.edu.cn; zhaoyq@nwpu.edu.cn; wenzhi.liao@vito.be; jcheungw@etrovub.be; wilfried.philips@ugent.be,,"Liao, Wenzhi/0000-0002-2183-0324; Bu, Yuanyang/0000-0001-6558-8499; Xue, Jize/0000-0003-2451-7445","National Natural Science Foundation of China (NSFC) [61771391]; Science, Technology, and Innovation Commission of Shenzhen Municipality [JCYJ20170815162956949, JCYJ20180306171146740]; Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University [CX201917]","National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Science, Technology, and Innovation Commission of Shenzhen Municipality; Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University","This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61771391, in part by the Science, Technology, and Innovation Commission of Shenzhen Municipality under Grant JCYJ20170815162956949 and Grant JCYJ20180306171146740, and in part by the Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University under Grant CX201917. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jocelyn Chanussot.",,65,23,23,13,30,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,3084,3097,,10.1109/TIP.2021.3058590,,,14,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,QM9MC,33596175,,Y,N,2022-05-23,WOS:000622096400004
J,"Yuan, D; Chang, XJ; Huang, PY; Liu, Q; He, ZY",,,,"Yuan, Di; Chang, Xiaojun; Huang, Po-Yao; Liu, Qiao; He, Zhenyu",,,Self-Supervised Deep Correlation Tracking,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Target tracking; Feature extraction; Training; Correlation; Trajectory; Task analysis; Time series analysis; Visual tracking; self-supervised learning; multi-cycle consistency loss,OBJECT TRACKING,"The training of a feature extraction network typically requires abundant manually annotated training samples, making this a time-consuming and costly process. Accordingly, we propose an effective self-supervised learning-based tracker in a deep correlation framework (named: self-SDCT). Motivated by the forward-backward tracking consistency of a robust tracker, we propose a multi-cycle consistency loss as self-supervised information for learning feature extraction network from adjacent video frames. At the training stage, we generate pseudo-labels of consecutive video frames by forward-backward prediction under a Siamese correlation tracking framework and utilize the proposed multi-cycle consistency loss to learn a feature extraction network. Furthermore, we propose a similarity dropout strategy to enable some low-quality training sample pairs to be dropped and also adopt a cycle trajectory consistency loss in each sample pair to improve the training loss function. At the tracking stage, we employ the pre-trained feature extraction network to extract features and utilize a Siamese correlation tracking framework to locate the target using forward tracking alone. Extensive experimental results indicate that the proposed self-supervised deep correlation tracker (self-SDCT) achieves competitive tracking performance contrasted to state-of-the-art supervised and unsupervised tracking methods on standard evaluation benchmarks.","[Yuan, Di; Liu, Qiao; He, Zhenyu] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China; [Chang, Xiaojun] Monash Univ, Fac Informat Technol, Melbourne, Vic 3800, Australia; [Chang, Xiaojun] King Abdulaziz Univ, Fac Comp & Informat Technol, Jeddah 21589, Saudi Arabia; [Huang, Po-Yao] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [He, Zhenyu] Peng Cheng Lab, Shenzhen 518055, Peoples R China",,"He, ZY (通讯作者)，Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.",dyuanhit@gmail.com; cxj273@gmail.com; poyaoh@cs.cmu.edu; liuqiao@stu.hit.edu.cn; zhenyuhe@hit.edu.cn,"Liu, Qiao/AAL-5654-2021; Yuan, Di/Q-6521-2019; Chang, Xiaojun/A-2055-2015","Liu, Qiao/0000-0003-0885-7976; Yuan, Di/0000-0001-9403-1112; Chang, Xiaojun/0000-0002-7778-8807","National Natural Science Foundation of China [61672183]; Shenzhen Research Council [JCYJ2017041310455226946, JCYJ20170815113552036]; PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications [PCL2018KP001]; Verficiation Platform of Multi-tier Coverage Communication Network for Oceans [PCL2018KP002]; China Scholarship Council; Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA) [DE190100626]",National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Research Council; PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications; Verficiation Platform of Multi-tier Coverage Communication Network for Oceans; China Scholarship Council(China Scholarship Council); Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA)(Australian Research Council),"This work was supported in part by the National Natural Science Foundation of China under Grant 61672183, in part by the Shenzhen Research Council under Grant JCYJ2017041310455226946 and Grant JCYJ20170815113552036, in part by the PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications under Project PCL2018KP001, and in part by The Verficiation Platform of Multi-tier Coverage Communication Network for Oceans under Project PCL2018KP002. The work of Di Yuan was supported by a scholarship from the China Scholarship Council. The work of Xiaojun Chang was supported in part by the Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA) under Grant DE190100626.",,76,63,63,33,63,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,976,985,,10.1109/TIP.2020.3037518,,,10,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC6ZL,33259298,,Y,Y,2022-05-23,WOS:000597146900007
J,"Zhang, QJ; Cong, RM; Li, CY; Cheng, MM; Fang, YM; Cao, XC; Zhao, Y; Kwong, S",,,,"Zhang, Qijian; Cong, Runmin; Li, Chongyi; Cheng, Ming-Ming; Fang, Yuming; Cao, Xiaochun; Zhao, Yao; Kwong, Sam",,,Dense Attention Fluid Network for Salient Object Detection in Optical Remote Sensing Images,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,English,Article,,,,,,Optical imaging; Optical sensors; Feature extraction; Visualization; Remote sensing; Task analysis; Object detection; Salient object detection; dense attention fluid; global context-aware attention; optical remote sensing images,,"Despite the remarkable advances in visual saliency analysis for natural scene images (NSIs), salient object detection (SOD) for optical remote sensing images (RSIs) still remains an open and challenging problem. In this paper, we propose an end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A Global Context-aware Attention (GCA) module is proposed to adaptively capture long-range semantic context relationships, and is further embedded in a Dense Attention Fluid (DAF) structure that enables shallow attention cues flow into deep layers to guide the generation of high-level feature attention maps. Specifically, the GCA module is composed of two key components, where the global feature aggregation module achieves mutual reinforcement of salient feature embeddings from any two spatial locations, and the cascaded pyramid attention module tackles the scale variation issue by building up a cascaded pyramid framework to progressively refine the attention map in a coarse-to-fine manner. In addition, we construct a new and challenging optical RSI dataset for SOD that contains 2,000 images with pixel-wise saliency annotations, which is currently the largest publicly available benchmark. Extensive experiments demonstrate that our proposed DAFNet significantly outperforms the existing state-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20","[Zhang, Qijian; Cong, Runmin; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China; [Cong, Runmin; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China; [Cong, Runmin; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China; [Li, Chongyi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Tianjin 300071, Peoples R China; [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Jiangxi, Peoples R China; [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China; [Cao, Xiaochun] Peng Cheng Lab, Cyberspace Secur Res Ctr, Shenzhen 518055, Peoples R China; [Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China; [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 51800, Peoples R China",,"Cong, RM (通讯作者)，City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.",qijizhang3-c@my.cityu.edu.hk; rmcong@bjtu.edu.cn; ichongyi25@gmail.com; cmm@nankai.edu.cn; leo.fangyuming@foxmail.com; caoxiaochun@iie.ac.cn; yzhao@bjtu.edu.cn; cssamk@cityu.edu.hk,"Cheng, Ming-Ming/A-2527-2009; , sam/C-9319-2012","Cheng, Ming-Ming/0000-0001-5550-8758; , sam/0000-0001-7484-7261; ZHANG, Qijian/0000-0003-4723-6136; Zhao, Yao/0000-0002-8581-9554","Beijing Nova Program [Z201100006820016]; National Key Research and Development of China [2018AAA0102100]; National Natural Science Foundation of China [62002014, 61532005, U1936212, 61971016, U1803264, 61922046, 61772344, 61672443]; Hong Kong Research Grants Council (RGC) [9042816, CityU 11209819]; Fundamental Research Funds for the Central Universities [2019RC039, 2018JBZ001]; Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology; Hong Kong Scholars Program, Zhejiang Laboratory [2019NB0AB01]; Tianjin Natural Science Foundation [18ZXZNGX00110]; China Postdoctoral Science Foundation [2020T130050, 2019M660438]","Beijing Nova Program(Beijing Municipal Science & Technology Commission); National Key Research and Development of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Hong Kong Research Grants Council (RGC)(Hong Kong Research Grants Council); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology; Hong Kong Scholars Program, Zhejiang Laboratory; Tianjin Natural Science Foundation(Natural Science Foundation of Tianjin); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)","This work was supported in part by the Beijing Nova Program under Grant Z201100006820016; in part by the National Key Research and Development of China under Grant 2018AAA0102100; in part by the National Natural Science Foundation of China under Grant 62002014, Grant 61532005, Grant U1936212, Grant 61971016, Grant U1803264, Grant 61922046, Grant 61772344, and Grant 61672443; in part by the Hong Kong Research Grants Council (RGC) General Research Funds under Grant 9042816 and Grant CityU 11209819; in part by the Fundamental Research Funds for the Central Universities under Grant 2019RC039 and Grant 2018JBZ001; in part by the Elite Scientist Sponsorship Program by the Beijing Association for Science and Technology; in part by the Hong Kong Scholars Program, Zhejiang Laboratory under Grant 2019NB0AB01; in part by the Tianjin Natural Science Foundation under Grant 18ZXZNGX00110; and in part by the China Postdoctoral Science Foundation under Grant 2020T130050 and Grant 2019M660438.",,63,35,36,20,40,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,PISCATAWAY,"445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA",1057-7149,1941-0042,,IEEE T IMAGE PROCESS,IEEE Trans. Image Process.,,2021,30,,,,,,1305,1317,,10.1109/TIP.2020.3042084,,,13,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PL3KY,33306467,Green Submitted,Y,Y,2022-05-23,WOS:000603026100008
J,"Zhang, XQ; Wang, D; Zhou, ZY; Ma, Y",,,,"Zhang, Xiaoqin; Wang, Di; Zhou, Zhengyuan; Ma, Yi",,,Robust Low-Rank Tensor Recovery with Rectification and Alignment,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,English,Article,,,,,,Low-rank tensor recovery; rectification; alignment; ADMM; proximal gradient,MATRIX COMPLETION; MODELS; FACTORIZATION; ALGORITHM,"Low-rank tensor recovery in the presence of sparse but arbitrary errors is an important problem with many practical applications. In this work, we propose a general framework that recovers low-rank tensors, in which the data can be deformed by some unknown transformations and corrupted by arbitrary sparse errors. We give a unified presentation of the surrogate-based formulations that incorporate the features of rectification and alignment simultaneously, and establish worst-case error bounds of the recovered tensor. In this context, the state-of-the-art methods 'RASL' and 'TILT' can be viewed as two special cases of our work, and yet each only performs part of the function of our method. Subsequently, we study the optimization aspects of the problem in detail by deriving two algorithms, one based on the alternating direction method of multipliers (ADMM) and the other based on proximal gradient. We provide convergence guarantees for the latter algorithm, and demonstrate the performance of the former through in-depth simulations. Finally, we present extensive experimental results on public datasets to demonstrate the effectiveness and efficiency of the proposed framework and algorithms.","[Zhang, Xiaoqin; Wang, Di] Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Wenzhou 325035, Zhejiang, Peoples R China; [Zhou, Zhengyuan] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Ma, Yi] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA",,"Wang, D (通讯作者)，Wenzhou Univ, Coll Comp Sci & Artificial Intelligence, Wenzhou 325035, Zhejiang, Peoples R China.",zhangxiaoqinnan@gmail.com; wangdi@wzu.edu.cn; zyzhou@stanford.edu; yima@eecs.berkeley.edu,"Wang, Di/AAB-1437-2019","Wang, Di/0000-0003-0435-0609; Ma, Yi/0000-0001-5485-419X","National Key Research and Development Program of China [2018YFB1004904]; National Natural Science Foundation of China [61772374]; Natural Science Foundation of Zhejiang Province [LY17F030004, LR17F030001]; Project of Science and Technology Plans of Wenzhou City [C20170008, G20160002, ZG2017016]",National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Zhejiang Province(Natural Science Foundation of Zhejiang Province); Project of Science and Technology Plans of Wenzhou City,"This work was supported in part by the National Key Research and Development Program of China [grant no. 2018YFB1004904], in part by the National Natural Science Foundation of China [grant no. 61772374], in part by the Natural Science Foundation of Zhejiang Province [grant nos. LY17F030004, LR17F030001], in part by the Project of Science and Technology Plans of Wenzhou City [grant nos. C20170008, G20160002, ZG2017016]. A preliminary version of this paper was accepted by Advances in Neural Information Processing Systems 2013 [47].",,47,96,96,22,47,IEEE COMPUTER SOC,LOS ALAMITOS,"10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA",0162-8828,1939-3539,,IEEE T PATTERN ANAL,IEEE Trans. Pattern Anal. Mach. Intell.,JAN 1,2021,43,1,,,,,238,255,,10.1109/TPAMI.2019.2929043,,,18,"Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic",Science Citation Index Expanded (SCI-EXPANDED),Computer Science; Engineering,PC7WN,31329109,,Y,N,2022-05-23,WOS:000597206900016
