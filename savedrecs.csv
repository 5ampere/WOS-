Publication Type,Authors,Book Authors,Group Authors,Book Group Authors,Researcher Ids,ORCIDs,Book Editors,Author - Arabic,Article Title,Article Title - SciELO,Article Title - SciELO,Article Title - Chinese,Article Title - Russian,Patent Number,Patent Assignee,Source Title - Arabic,Source Title,Source Title - Korean,Book Series Title,Book Series Subtitle,Volume,Issue,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,Book DOI,Early Access Date,Supplement,Publication Date,Publication Year,Abstract,Abstract - Foreign,Abstract - English Transliteration ,Abstract - Foreign,Abstract - Korean,Conference Title,Conference Date,Conference Sponsor,Conference Location,"Times Cited, WoS Core","Times Cited, CSCD ","Times Cited, RSCI","Times Cited, ARCI","Times Cited, BCI","Times Cited, SCIELO","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,ISSN,eISSN,ISBN,UT (Unique ID),Pubmed Id,
J,"Ibtehaz, Nabil; Rahman, M. Sohel",,,,"Ibtehaz, Nabil/AAX-6846-2021; Rahman, Mohammad Sohelur/AAJ-7376-2020","Ibtehaz, Nabil/0000-0003-3625-5972; Rahman, Mohammad Sohelur/0000-0002-4489-2202",,,MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation,,,,,,,,NEURAL NETWORKS,,,,121,,,,74,87,,10.1016/j.neunet.2019.08.025,,,,JAN 2020,2020,"In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%, 1.41%, and 0.62% respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures. (C) 2019 Elsevier Ltd. All rights reserved.",,,,,,,,,345,27,0,0,57,0,369,,,0893-6080,1879-2782,,WOS:000500922700008,31536901,
J,"Isensee, Fabian; Jaeger, Paul F.; Kohl, Simon A. A.; Petersen, Jens; Maier-Hein, Klaus H.",,,,"Maier-Hein, Klaus/M-6250-2016","Maier-Hein, Klaus/0000-0002-6626-2463",,,nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,,,,,,,,NATURE METHODS,,,,18,2,,,203,+,,10.1038/s41592-020-01008-z,,DEC 2020,,FEB 2021,2021,"nnU-Net is a deep learning-based image segmentation method that automatically configures itself for diverse biological and medical image segmentation tasks. nnU-Net offers state-of-the-art performance as an out-of-the-box tool.Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",,,,,,,,,305,3,0,0,80,0,309,,,1548-7091,1548-7105,,WOS:000599000100001,33288961,
J,"Minaei, Shahin; Jourkesh, Morteza; Kreider, Richard B.; Forbes, Scott C.; Souza-Junior, Tacito P.; McAnulty, Steven R.; Kalman, Douglas",,,,,"Kalman, Douglas/0000-0003-0882-9748",,,CYP1A2 Genotype Polymorphism Influences the Effect of Caffeine on Anaerobic Performance in Trained Males,,,,,,,,INTERNATIONAL JOURNAL OF SPORT NUTRITION AND EXERCISE METABOLISM,,,,32,1,,,16,21,,10.1123/ijsnem.2021-0090,,,,JAN 2022,2022,"The purpose was to investigate the effects of CYP1A2 -163C > A polymorphism on the effects of acute caffeine (CAF) supplementation on anaerobic power in trained males. Sixteen trained males (age: 21.6 +/- 7.1 years; height: 179.7 +/- 5.6 cm; body mass: 72.15 +/- 6.8 kg) participated in a randomized, double-blind, placebo (PLA) controlled crossover design. Participants supplemented with CAF (6 mg/kg of body mass) and an isovolumetric PLA (maltodextrin) in random order and separated by 7 days, before an all-out 30-s anaerobic cycling test to determine peak, average, and minimum power output, and fatigue index. Genomic deoxyribonucleic acid was extracted to identify each participants CYP1A2 genotype. Six participants expressed AA homozygote and 10 expressed C alleles. There was a treatment by genotype interaction for peak power output (p = .041, eta 2 = .265, observed power = 0.552) with only those expressing AA genotype showing improvement following CAF supplementation compared with PLA (CAF: 693 +/- 108 watts vs. PLA: 655 +/- 97 watts; p = .039), while no difference between treatments was noted in those expressing C alleles (CAF: 614 +/- 92 watts vs. PLA: 659 +/- 144 watts; p = .135). There were no other interaction or main effects for average or minimum power output, or fatigue index (p > .05). In conclusion, the ingestion of 6 mg/kg of CAF improved peak power output only in participants with the AA genotype compared with PLA; however, expression of the CYP1A2 did not influence average or minimum power output or fatigue index.",,,,,,,,,288,3,1,0,23,0,292,,,1526-484X,1543-2742,,WOS:000732797600004,33596172,
J,"Wang, Jingdong; Sun, Ke; Cheng, Tianheng; Jiang, Borui; Deng, Chaorui; Zhao, Yang; Liu, Dong; Mu, Yadong; Tan, Mingkui; Wang, Xinggang; Liu, Wenyu; Xiao, Bin",,,,"Xiao, Bin/AAL-3936-2020; Liu, Wenyu/AAG-1426-2019","Xiao, Bin/0000-0001-6477-5911; Liu, Wenyu/0000-0002-4582-7488",,,Deep High-Resolution Representation Learning for Visual Recognition,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,43,10,,,3349,3364,,10.1109/TPAMI.2020.2983686,,,,OCT 1 2021,2021,"High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.",,,,,,,,,266,1,1,0,15,0,267,,,0162-8828,1939-3539,,WOS:000692232400010,32248092,
J,"Guo, Yulan; Wang, Hanyun; Hu, Qingyong; Liu, Hao; Liu, Li; Bennamoun, Mohammed",,,,"; Bennamoun, Mohammed/C-2789-2013; /E-7102-2014","Hanyun, Wang/0000-0002-8320-4230; Bennamoun, Mohammed/0000-0002-6603-3257; Liu, li/0000-0002-2011-2873; hu, qing yong/0000-0003-0337-9207; /0000-0001-7051-841X",,,Deep Learning for 3D Point Clouds: A Survey,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,43,12,,,4338,4364,,10.1109/TPAMI.2020.3005434,,,,DEC 1 2021,2021,"Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.",,,,,,,,,261,3,0,0,6,0,261,,,0162-8828,1939-3539,,WOS:000714203900015,32750799,
J,"Zhou, Zongwei; Siddiquee, Md Mahfuzur Rahman; Tajbakhsh, Nima; Liang, Jianming",,,,"Siddiquee, Mahfuzur Rahman/Q-2418-2019","Liang, Jianming/0000-0001-5486-1613",,,UNet plus plus : Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation,,,,,,,,IEEE TRANSACTIONS ON MEDICAL IMAGING,,,,39,6,,,1856,1867,,10.1109/TMI.2019.2959609,,,,JUN 2020,2020,"The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects-an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.",,,,,,,,,219,12,0,0,42,0,227,,,0278-0062,1558-254X,,WOS:000544923000007,31841402,
J,"Huang, Lianghua; Zhao, Xin; Huang, Kaiqi",,,,,"Huang, Lianghua/0000-0002-9686-9354; Zhao, Xin/0000-0002-7660-9897",,,GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,43,5,,,1562,1577,,10.1109/TPAMI.2019.2957464,,,,MAY 1 2021,2021,"We introduce here a large tracking database that offers an unprecedentedly wide coverage of common moving objects in the wild, called GOT-10k. Specifically, GOT-10k is built upon the backbone of WordNet structure [1] and it populates the majority of over 560 classes of moving objects and 87 motion patterns, magnitudes wider than the most recent similar-scale counterparts [19], [20], [23], [26]. By releasing the large high-diversity database, we aim to provide a unified training and evaluation platform for the development of class-agnostic, generic purposed short-term trackers. The features of GOT-10k and the contributions of this article are summarized in the following. (1) GOT-10k offers over 10,000 video segments with more than 1.5 million manually labeled bounding boxes, enabling unified training and stable evaluation of deep trackers. (2) GOT-10k is by far the first video trajectory dataset that uses the semantic hierarchy of WordNet to guide class population, which ensures a comprehensive and relatively unbiased coverage of diverse moving objects. (3) For the first time, GOT-10k introduces the one-shot protocol for tracker evaluation, where the training and test classes are zero-overlapped. The protocol avoids biased evaluation results towards familiar objects and it promotes generalization in tracker development. (4) GOT-10k offers additional labels such as motion classes and object visible ratios, facilitating the development of motion-aware and occlusion-aware trackers. (5) We conduct extensive tracking experiments with 39 typical tracking algorithms and their variants on GOT-10k and analyze their results in this paper. (6) Finally, we develop a comprehensive platform for the tracking community that offers full-featured evaluation toolkits, an online evaluation server, and a responsive leaderboard. The annotations of GOT-10k's test data are kept private to avoid tuning parameters on it.",,,,,,,,,189,15,0,0,4,0,202,,,0162-8828,1939-3539,,WOS:000637533800007,31804928,
J,"Hollon, Todd C.; Pandian, Balaji; Adapa, Arjun R.; Urias, Esteban; Save, Akshay V.; Khalsa, Siri Sahib S.; Eichberg, Daniel G.; D'Amico, Randy S.; Farooq, Zia U.; Lewis, Spencer; Petridis, Petros D.; Marie, Tamara; Shah, Ashish H.; Garton, Hugh J. L.; Maher, Cormac O.; Heth, Jason A.; McKean, Erin L.; Sullivan, Stephen E.; Hervey-Jumper, Shawn L.; Patil, Parag G.; Thompson, B. Gregory; Sagher, Oren; McKhann, Guy M.; Komotar, Ricardo J.; Ivan, Michael E.; Snuderl, Matija; Otten, Marc L.; Johnson, Timothy D.; Sisti, Michael B.; Bruce, Jeffrey N.; Muraszko, Karin M.; Trautman, Jay; Freudiger, Christian W.; Canoll, Peter; Lee, Honglak; Camelo-Piragua, Sandra; Orringer, Daniel A.",,,,"Bruce, Jeffrey N/AAC-5307-2022; Petridis, Petros/Q-5686-2018","Petridis, Petros/0000-0001-7608-5723; Khalsa, Siri Sahib/0000-0003-0800-3866; Hollon, Todd/0000-0001-5987-6531; Orringer, Daniel/0000-0001-6427-8376; Snuderl, Matija/0000-0003-0752-0917",,,Near real-time intraoperative brain tumor diagnosis using stimulated Raman histology and deep neural networks,,,,,,,,NATURE MEDICINE,,,,26,1,,,52,+,,10.1038/s41591-019-0715-9,,,,JAN 2020,2020,"Intraoperative diagnosis is essential for providing safe and effective care during cancer surgery(1). The existing workflow for intraoperative diagnosis based on hematoxylin and eosin staining of processed tissue is time, resource and labor intensive(2,3). Moreover, interpretation of intraoperative histologic images is dependent on a contracting, unevenly distributed, pathology workforce(4). In the present study, we report a parallel workflow that combines stimulated Raman histology (SRH)(5-7), a label-free optical imaging method and deep convolutional neural networks (CNNs) to predict diagnosis at the bedside in near real-time in an automated fashion. Specifically, our CNNs, trained on over 2.5 million SRH images, predict brain tumor diagnosis in the operating room in under 150 s, an order of magnitude faster than conventional techniques (for example, 20-30 min)(2). In a multicenter, prospective clinical trial (n = 278), we demonstrated that CNN-based diagnosis of SRH images was noninferior to pathologist-based interpretation of conventional histologic images (overall accuracy, 94.6% versus 93.9%). Our CNNs learned a hierarchy of recognizable histologic feature representations to classify the major histopathologic classes of brain tumors. In addition, we implemented a semantic segmentation method to identify tumor-infiltrated diagnostic regions within SRH images. These results demonstrate how intraoperative cancer diagnosis can be streamlined, creating a complementary pathway for tissue diagnosis that is independent of a traditional pathology laboratory.A prospective, multicenter, case-control clinical trial evaluates the potential of artificial intelligence for providing accurate bedside diagnosis of patients with brain tumors.",,,,,,,,,164,7,0,0,55,0,168,,,1078-8956,1546-170X,,WOS:000509831900026,31907460,
J,"Diakogiannis, Foivos, I; Waldner, Francois; Caccetta, Peter; Wu, Chen",,,,"Waldner, Francois/AAW-1934-2020; Diakogiannis, Foivos/L-4957-2019","Waldner, Francois/0000-0002-5599-7456; Diakogiannis, Foivos/0000-0002-8788-8174",,,ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,162,,,,94,114,,10.1016/j.isprsjprs.2020.01.013,,,,APR 2020,2020,"Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9% over all classes for our best model.",,,,,,,,,160,2,0,0,12,0,162,,,0924-2716,1872-8235,,WOS:000527709200008,,
J,"Tamiminia, Haifa; Salehi, Bahram; Mahdianpari, Masoud; Quackenbush, Lindi; Adeli, Sarina; Brisco, Brian",,,,"Tamiminia, Haifa/ABI-3508-2020; Quackenbush, Lindi J./AAZ-4209-2021","Mahdianpari, Masoud/0000-0002-7234-959X; Salehi, Bahram/0000-0002-7742-5475",,,Google Earth Engine for geo-big data applications: A meta-analysis and systematic review,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,164,,,,152,170,,10.1016/j.isprsjprs.2020.04.001,,,,JUN 2020,2020,"Google Earth Engine (GEE) is a cloud-based geospatial processing platform for large-scale environmental monitoring and analysis. The free-to-use GEE platform provides access to (1) petabytes of publicly available remote sensing imagery and other ready-to-use products with an explorer web app; (2) high-speed parallel processing and machine learning algorithms using Google's computational infrastructure; and (3) a library of Application Programming Interfaces (APIs) with development environments that support popular coding languages, such as JavaScript and Python. Together these core features enable users to discover, analyze and visualize geospatial big data in powerful ways without needing access to supercomputers or specialized coding expertise. The development of GEE has created much enthusiasm and engagement in the remote sensing and geospatial data science fields. Yet after a decade since GEE was launched, its impact on remote sensing and geospatial science has not been carefully explored. Thus, a systematic review of GEE that can provide readers with the big picture of the current status and general trends in GEE is needed. To this end, the decision was taken to perform a meta-analysis investigation of recent peer-reviewed GEE articles focusing on several features, including data, sensor type, study area, spatial resolution, application, strategy, and analytical methods. A total of 349 peer-reviewed articles published in 146 different journals between 2010 and October 2019 were reviewed. Publications and geographical distribution trends showed a broad spectrum of applications in environmental analyses at both regional and global scales. Remote sensing datasets were used in 90% of studies while 10% of the articles utilized ready-to-use products for analyses. Optical satellite imagery with medium spatial resolution, particularly Landsat data with an archive exceeding 40 years, has been used extensively. Linear regression and random forest were the most frequently used algorithms for satellite imagery processing. Among ready-to-use products, the normalized difference vegetation index (NDVI) was used in 27% of studies for vegetation, crop, land cover mapping and drought monitoring. The results of this study confirm that GEE has and continues to make substantive progress on global challenges involving process of geo-big data.",,,,,,,,,143,9,0,0,24,0,151,,,0924-2716,1872-8235,,WOS:000535696600012,,
C,"Huang, Huimin; Lin, Lanfen; Tong, Ruofeng; Hu, Hongjie; Zhang, Qiaowei; Iwamoto, Yutaro; Han, Xianhua; Chen, Yen-Wei; Wu, Jian",,,IEEE,,,,,UNET 3+: A FULL-SCALE CONNECTED UNET FOR MEDICAL IMAGE SEGMENTATION,,,,,,,,"2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING",,International Conference on Acoustics Speech and Signal Processing ICASSP,,,,,,1055,1059,,,,,,2020,2020,"Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets.",,,,,"IEEE International Conference on Acoustics, Speech, and Signal ProcessingIEEE International Conference on Acoustics, Speech, and Signal Processing","MAY 04-08, 2020MAY 04-08, 2020","Inst Elect & Elect Engineers; Inst Elect & Elect Engineers, Signal Proc SocInst Elect & Elect Engineers; Inst Elect & Elect Engineers, Signal Proc Soc","Barcelona, SPAINBarcelona, SPAIN",135,11,1,0,18,0,146,,,1520-6149,,978-1-5090-6631-5,WOS:000615970401059,,
J,"Feng, Di; Haase-Schutz, Christian; Rosenbaum, Lars; Hertlein, Heinz; Glaser, Claudius; Timm, Fabian; Wiesbeck, Werner; Dietmayer, Klaus",,,,"Haase-Schütz, Christian/ABB-5307-2020; Feng, Di/AAA-1708-2021","Haase-Schütz, Christian/0000-0003-4171-9305; ",,,"Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges",,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,22,3,,,1341,1360,,10.1109/TITS.2020.2972974,,,,MAR 2021,2021,"Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of what to fuse, when to fuse, and how to fuse remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.",,,,,,,,,133,5,0,0,2,0,138,,,1524-9050,1558-0016,,WOS:000626338600001,,
J,"Yang, Fan; Zhang, Lei; Yu, Sijia; Prokhorov, Danil; Mei, Xue; Ling, Haibin",,,,,"Zhang, Lei/0000-0002-4424-4942; yang, fan/0000-0003-1535-447X; Ling, Haibin/0000-0003-4094-8413",,,Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,21,4,,,1525,1535,,10.1109/TITS.2019.2910595,,,,APR 2020,2020,"Pavement crack detection is a critical task for insuring road safety. Manual crack detection is extremely time-consuming. Therefore, an automatic road crack detection method is required to boost this progress. However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavements and possible shadows with a similar intensity. Inspired by recent advances of deep learning in computer vision, we propose a novel network architecture, named feature pyramid and hierarchical boosting network (FPHBN), for pavement crack detection. The proposed network integrates context information to low-level features for crack detection in a feature pyramid way, and it balances the contributions of both easy and hard samples to loss by nested sample reweighting in a hierarchical way during training. In addition, we propose a novel measurement for crack detection named average intersection over union (AIU). To demonstrate the superiority and generalizability of the proposed method, we evaluate it on five crack datasets and compare it with the state-of-the-art crack detection, edge detection, and semantic segmentation methods. The extensive experiments show that the proposed method outperforms these methods in terms of accuracy and generalizability. Code and data can be found in https://github.com/fyangneil/pavement-crack-detection.",,,,,,,,,127,8,1,0,2,0,134,,,1524-9050,1558-0016,,WOS:000523478400016,,
J,"Hong, Danfeng; Wu, Xin; Ghamisi, Pedram; Chanussot, Jocelyn; Yokoya, Naoto; Zhu, Xiao Xiang",,,,"Yokoya, Naoto/AAC-1530-2022; Ghamisi, Pedram/ABD-5419-2021; Wu, Xin/AAT-4490-2021; Zhu, Xiao Xiang/ABE-7138-2020; Hong, Danfeng/U-6082-2019","Zhu, Xiao Xiang/0000-0001-5530-3613; Hong, Danfeng/0000-0002-3212-9584; Chanussot, Jocelyn/0000-0003-4817-2875",,,Invariant Attribute Profiles: A Spatial-Frequency Joint Feature Extractor for Hyperspectral Image Classification,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,6,,,3791,3808,,10.1109/TGRS.2019.2957251,,,,JUN 2020,2020,"So far, a large number of advanced techniques have been developed to enhance and extract the spatially semantic information in hyperspectral image processing and analysis. However, locally semantic change, such as scene composition, relative position between objects, spectral variability caused by illumination, atmospheric effects, and material mixture, has been less frequently investigated in modeling spatial information. Consequently, identifying the same materials from spatially different scenes or positions can be difficult. In this article, we propose a solution to address this issue by locally extracting invariant features from hyperspectral imagery (HSI) in both spatial and frequency domains, using a method called invariant attribute profiles (IAPs). IAPs extract the spatial invariant features by exploiting isotropic filter banks or convolutional kernels on HSI and spatial aggregation techniques (e.g., superpixel segmentation) in the Cartesian coordinate system. Furthermore, they model invariant behaviors (e.g., shift, rotation) by the means of a continuous histogram of oriented gradients constructed in a Fourier polar coordinate. This yields a combinatorial representation of spatial-frequency invariant features with application to HSI classification. Extensive experiments conducted on three promising hyperspectral data sets (Houston2013 and Houston2018) to demonstrate the superiority and effectiveness of the proposed IAP method in comparison with several state-of-the-art profile-related techniques. The codes will be available from the website: <uri>https://sites.google.com/view/danfeng-hong/data-code</uri>.",,,,,,,,,124,2,1,0,2,0,126,,,0196-2892,1558-0644,,WOS:000538748700004,,
B,Yuhui Yuan; Xilin Chen; Jingdong Wang,,,,"Wang, Jingdong/E-9920-2017","Wang, Jingdong/0000-0002-4888-4445","Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,Object-Contextual Representations for Semantic Segmentation,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351),,,,,,,,173,90,,10.1007/978-3-030-58539-6_11,,,,2020,2020,"In this paper, we study the context aggregation problem in semantic segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. We empirically demonstrate our method achieves competitive performance on various benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission ldquoHRNet + OCR + SegFixrdquo achieves the 1st place on the Cityscapes leaderboard by the ECCV 2020 submission deadline. Code is available at: https://git.io/ openseg and https://git.io/HRNet.OCR.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",117,4,0,0,4,0,121,,,,,978-3-030-58538-9,INSPEC:20088203,,
J,"Asgari Taghanaki, Saeid; Abhishek, Kumar; Cohen, Joseph Paul; Cohen-Adad, Julien; Hamarneh, Ghassan",,,,"Hamarneh, Ghassan/AAE-6673-2021",,,,Deep semantic segmentation of natural and medical images: a review,,,,,,,,ARTIFICIAL INTELLIGENCE REVIEW,,,,54,1,,,137,178,,10.1007/s10462-020-09854-1,,JUN 2020,,JAN 2021,2021,"The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.",,,,,,,,,99,3,0,0,11,0,101,,,0269-2821,1573-7462,,WOS:000539923400001,,
B,Xinlong Wang; Tao Kong; Chunhua Shen; Yuning Jiang; Lei Li,,,,,"Li, Lei/0000-0003-3095-9776; Wang, Xinlong/0000-0002-6974-7976","Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,SOLO: Segmenting Objects by Locations,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363),,,,,,,,649,65,,10.1007/978-3-030-58523-5_38,,,,2020,2020,"We present a new, embarrassingly simple approach to instance segmentation. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the ldquodetect-then-segmentrdquo strategy (e.g., Mask R-CNN), or predict embedding vectors first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of ldquoinstance categoriesrdquo, which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance segmentation into a single-shot classification-solvable problem. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving onpar accuracy with Mask R-CNN and outperforming recent single-shot instance segmenters in accuracy. We hope that this simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation. Code is available at https://git.io/AdelaiDet.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",93,5,0,0,5,0,98,,,,,978-3-030-58522-8,INSPEC:20174035,,
J,"Kuutti, Sampo; Bowden, Richard; Jin, Yaochu; Barber, Phil; Fallah, Saber",,,,"Bowden, Richard/AAF-8283-2019; Jin, Yaochu/B-3776-2012","Bowden, Richard/0000-0003-3285-8020; Kuutti, Sampo/0000-0002-7020-4370; Fallah, Saber/0000-0002-1298-1040; Jin, Yaochu/0000-0003-1100-0631",,,A Survey of Deep Learning Applications to Autonomous Vehicle Control,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,22,2,,,712,733,,10.1109/TITS.2019.2962338,,,,FEB 2021,2021,"Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.",,,,,,,,,88,0,0,0,0,0,88,,,1524-9050,1558-0016,,WOS:000615045000003,,
J,"Zhang, Chenxiao; Yue, Peng; Tapete, Deodato; Jiang, Liangcun; Shangguan, Boyi; Huang, Li; Liu, Guangchao",,,,"Tapete, Deodato/AAB-7528-2021","Tapete, Deodato/0000-0002-7242-4473; Jiang, Liangcun/0000-0003-4994-7644; Yue, Peng/0000-0003-3006-4542",,,A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,166,,,,183,200,,10.1016/j.isprsjprs.2020.06.003,,,,AUG 2020,2020,"Change detection in high resolution remote sensing images is crucial to the understanding of land surface changes. As traditional change detection methods are not suitable for the task considering the challenges brought by the fine image details and complex texture features conveyed in high resolution images, a number of deep learning-based change detection methods have been proposed to improve the change detection performance. Although the state-of-the-art deep feature based methods outperform all the other deep learning-based change detection methods, networks in the existing deep feature based methods are mostly modified from architectures that are originally proposed for single-image semantic segmentation. Transferring these networks for change detection task still poses some key issues. In this paper, we propose a deeply supervised image fusion network (IFN) for change detection in high resolution bi-temporal remote sensing images. Specifically, highly representative deep features of bi-temporal images are firstly extracted through a fully convolutional two-stream architecture. Then, the extracted deep features are fed into a deeply supervised difference discrimination network (DDN) for change detection. To improve boundary completeness and internal compactness of objects in the output change maps, multi-level deep features of raw images are fused with image difference features by means of attention modules for change map reconstruction. DDN is further enhanced by directly introducing change map losses to intermediate layers in the network, and the whole network is trained in an end-to-end manner. IFN is applied to a publicly available dataset, as well as a challenging dataset consisting of multi-source bi-temporal images from Google Earth covering different cities in China. Both visual interpretation and quantitative assessment confirm that IFN outperforms four benchmark methods derived from the literature, by returning changed areas with complete boundaries and high internal compactness compared to the state-of-the-art methods.",,,,,,,,,85,3,0,0,3,0,88,,,0924-2716,1872-8235,,WOS:000551268300015,,
J,"Luo, Yuanjing; Qin, Jiaohua; Xiang, Xuyu; Tan, Yun; Liu, Qiang; Xiang, Lingyun",,,,,"xiang, xuyu/0000-0002-2778-7531",,,Coverless real-time image information hiding based on image block matching and dense convolutional network,,,,,,,,JOURNAL OF REAL-TIME IMAGE PROCESSING,,,,17,1,SI,,125,135,,10.1007/s11554-019-00917-3,,,,FEB 2020,2020,"Information security has become a key issue of public concern recently. In order to radically resist the decryption and analysis in the field of image information hiding and significantly improve the security of the secret information, a novel coverless information hiding approach based on deep learning is proposed in this paper. Deep learning can select the appropriate carrier according to requirements to achieve real-time image data hiding and the high-level semantic features extracted by CNN are more accurate than the low-level features. This method does not need to employ the designated image for embedding the secret data but transfer a set of real-time stego-images which share one or several visually similar blocks with the given secret image. In this approach, a group of real-time images searched online are segmented according to specific requirements. Then, the DenseNet is used to extract the high-level semantic features of each similar block. At the same time, a robust hash sequence with feature sequence, DC and location is generated by DCT. The inverted index structure based on the hash sequence is constructed to attain real-time image matching efficiently. At the sending end, the stego-images are matched and sent through feature retrieval. At the receiving end, the secret image can be recovered by extracting similar blocks through the received stego-images and stitching the image blocks according to the location information. Experimental results demonstrate that the proposed method without any modification traces provides better robustness and has higher retrieval accuracy and capacity when compared with some existing coverless image information hiding.",,,,,,,,,85,1,0,0,2,0,86,,,1861-8200,1861-8219,,WOS:000513247800012,,
J,"Zou, Qin; Jiang, Hanwen; Dai, Qiyu; Yue, Yuanhao; Chen, Long; Wang, Qian",,,,"Zou, Qin/AFM-0040-2022",,,,Robust Lane Detection From Continuous Driving Scenes Using Deep Neural Networks,,,,,,,,IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY,,,,69,1,,,41,54,,10.1109/TVT.2019.2949603,,,,JAN 2020,2020,"Lane detection in driving scenes is an important module for autonomous vehicles and advanced driver assistance systems. In recent years, many sophisticated lane detection methods have been proposed. However, most methods focus on detecting the lane from one single image, and often lead to unsatisfactory performance in handling some extremely-bad situations such as heavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In fact, lanes are continuous line structures on the road. Consequently, the lane that cannot be accurately detected in one current frame may potentially be inferred out by incorporating information of previous frames. To this end, we investigate lane detection by using multiple frames of a continuous driving scene, and propose a hybrid deep architecture by combining the convolutional neural network (CNN) and the recurrent neural network (RNN). Specifically, information of each frame is abstracted by a CNN block, and the CNN features of multiple continuous frames, holding the property of time-series, are then fed into the RNN block for feature learning and lane prediction. Extensive experiments on two large-scale datasets demonstrate that, the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations.",,,,,,,,,77,4,0,0,2,0,80,,,0018-9545,1939-9359,,WOS:000512550600004,,
J,"Qin, Haotong; Gong, Ruihao; Liu, Xianglong; Bai, Xiao; Song, Jingkuan; Sebe, Nicu",,,,,"Liu, Xianglong/0000-0002-7618-3275; Sebe, Niculae/0000-0002-6597-7248; song, jingkuan/0000-0002-2549-8322",,,Binary neural networks: A survey,,,,,,,,PATTERN RECOGNITION,,,,105,,,,,,107281,10.1016/j.patcog.2020.107281,,,,SEP 2020,2020,"The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected. (C) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,77,2,0,0,7,0,79,,,0031-3203,1873-5142,,WOS:000539457100002,,
J,"Tian, Zhihong; Gao, Xiangsong; Su, Shen; Qiu, Jing",,,,,"tian, zhihong/0000-0002-9409-5359",,,Vcash: A Novel Reputation Framework for Identifying Denial of Traffic Service in Internet of Connected Vehicles,,,,,,,,IEEE INTERNET OF THINGS JOURNAL,,,,7,5,,,3901,3909,,10.1109/JIOT.2019.2951620,,,,MAY 2020,2020,"Trust management of the Internet of Connected Vehicles has been a hot topic in recent years with the rapid development of UGV technologies. However, existing resolutions based on trustworthiness verification among vehicles make the traffic event transmission quite inefficient. In this article, we assume that the deployed roadside units (RSUs) can provide efficient communication between any pair of RSU and vehicle and propose vehicle cash (Vcash), a reputation framework for identifying denial of traffic service, to resolve the trustworthiness problem in the application level of the Internet of Connected Vehicles. In our reputation framework, every vehicle communicates with the RSU directly for traffic event verification, and spreads verified traffic event notification. We borrow the idea of market trading, and set up trading rules to restrict the malicious vehicle's spread of false message, and to encourage vehicles to contribute to the traffic event monitoring and verification. To evaluate the effectiveness of our reputation framework, we conduct a simulation experimental. Our experiment results indicate that our proposal manages to avoid bogus event spread, and a vehicle in our framework has to contribute to the traffic event detection to normally employ the traffic service.",,,,,,,,,75,6,0,0,1,0,77,,,2327-4662,,,WOS:000536066300020,,
J,"Rahimian, Farzad Pour; Seyedzadeh, Saleh; Oliver, Stephen; Rodriguez, Sergio; Dawood, Nashwan",,,,"; Rahimian, Farzad/Q-6332-2016","Rodriguez Trejo, Sergio/0000-0002-4994-0816; Rahimian, Farzad/0000-0001-7443-4723; Seyedzadeh, Saleh/0000-0001-6017-289X",,,On-demand monitoring of construction projects through a game-like hybrid application of BIM and machine learning,,,,,,,,AUTOMATION IN CONSTRUCTION,,,,110,,,,,,103012,10.1016/j.autcon.2019.103012,,,,FEB 2020,2020,"While unavoidable, inspections, progress monitoring, and comparing as-planned with as-built conditions in construction projects do not readily add tangible intrinsic value to the end-users. In large-scale construction projects, the process of monitoring the implementation of every single part of buildings and reflecting them on the BIM models can become highly labour intensive and error-prone, due to the vast amount of data produced in the form of schedules, reports and photo logs. In order to address the mentioned methodological and technical gap, this paper presents a framework and a proof of concept prototype for on-demand automated simulation of construction projects, integrating some cutting edge IT solutions, namely image processing, machine learning, BIM and Virtual Reality. This study utilised the Unity game engine to integrate data from the original BIM models and the as-built images, which were processed via various computer vision techniques. These methods include object recognition and semantic segmentation for identifying different structural elements through supervised training in order to superimpose the real world images on the as-planned model. The proposed framework leads to an automated update of the 3D virtual environment with states of the construction site. This framework empowers project managers and stockholders with an advanced decision-making tool, highlighting the inconsistencies in an effective manner. This paper contributes to body knowledge by providing a technical exemplar for the integration of ML and image processing approaches with immersive and interactive BIM interfaces, the algorithms and program codes of which can help replicability of these approaches by other scholars.",,,,,,,,,75,1,0,0,0,0,76,,,0926-5805,1872-7891,,WOS:000517664900010,,
J,"Fu, Kun; Chang, Zhonghan; Zhang, Yue; Xu, Guangluan; Zhang, Keshu; Sun, Xian",,,,"张, 跃/AAT-7479-2020",,,,Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,161,,,,294,308,,10.1016/j.isprsjprs.2020.01.025,,,,MAR 2020,2020,"Object detection plays an important role in the field of remote sensing imagery analysis. The most challenging issues in advancing this task are the large variation in object scales and the arbitrary orientation of objects. In this paper, we build a unified framework upon the region-based convolutional neural network for arbitrary-oriented and multi-scale object detection in remote sensing images. To handle the problem of multi-scale object detection, a feature-fusion architecture is proposed to generate a multi-scale feature hierarchy, which augments the features of shallow layers with semantic representations via a top-down pathway and combines the feature maps of top layers with low-level information by a bottom-up pathway. By combining features of different levels, we can form a powerful feature representation for multi-scale objects. Most previous methods locate objects with arbitrary orientations and dense spatial distributions via axis-aligned boxes, which may cover adjacent instances and background areas. We build a rotation-aware object detector that uses oriented boxes to localize objects in remote sensing images. The region proposal network augments the anchors with multiple default angles to cover oriented objects. It utilizes oriented proposal boxes to enclose objects rather than horizontal proposals that coarsely locate oriented objects. The orientation RoI pooling operation is introduced to extract the feature maps of oriented proposals for the following R-CNN subnetwork. We conduct comprehensive experiments on a public dataset for oriented object detection in remote sensing images. Our method achieves state-of-the-art performance, which demonstrates the effectiveness of the proposed methods.",,,,,,,,,72,3,0,0,1,0,75,,,0924-2716,1872-8235,,WOS:000517849600023,,
J,"Huang, Xinyu; Wang, Peng; Cheng, Xinjing; Zhou, Dingfu; Geng, Qichuan; Yang, Ruigang",,,,"Zhou, Dingfu/AAM-9192-2021","Huang, Xinyu/0000-0002-5786-3101; Yuille, Alan L./0000-0001-5207-9249; Yang, Ruigang/0000-0001-5296-6307",,,The ApolloScape Open Dataset for Autonomous Driving and Its Application,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,42,10,,,2702,2719,,10.1109/TPAMI.2019.2926463,,,,OCT 1 2020,2020,"Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving. Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3] , ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as joint 3D-2D segment labeling, active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system. We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.",,,,,,,,,66,6,0,0,0,0,71,,,0162-8828,1939-3539,,WOS:000567471300027,31283496,
J,"Choi, Wooram; Cha, Young-Jin",,,,"Cha, Young-Jin/ABH-3680-2020; Cha, Young-Jin/AFU-5444-2022","Cha, Young-Jin/0000-0002-0738-5615",,,SDDNet: Real-Time Crack Segmentation,,,,,,,,IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS,,,,67,9,,,8016,8025,,10.1109/TIE.2019.2945265,,,,SEP 2020,2020,"This article reports the development of a pure deep learning method for segmenting concrete cracks in images. The objectives are to achieve the real-time performance while effectively negating a wide range of various complex backgrounds and crack-like features. To achieve the goals, an original convolutional neural network is proposed. The model consists of standard convolutions, densely connected separable convolution modules, a modified atrous spatial pyramid pooling module, and a decoder module. The semantic damage detection network (SDDNet) is trained on a manually created crack dataset, and the trained network records the mean intersection-over-union of 0.846 on the test set. Each test image is analyzed, and the representative segmentation results are presented. The results show that the SDDNet segments cracks effectively unless the features are too faint. The proposed model is also compared with the most recent models, which show that it returns better evaluation metrics even though its number of parameters is 88 times less than in the compared models. In addition, the model processes in real-time (36 FPS) images at 1025 x 512 pixels, which is 46 times faster than in a recent work.",,,,,,,,,70,1,0,0,0,0,71,,,0278-0046,1557-9948,,WOS:000536291000084,,
C,"Jadon, Shruti",,,IEEE,,,,,A survey of loss functions for semantic segmentation,,,,,,,,2020 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB),,,,,,,,115,121,,,,,,2020,2020,"Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.",,,,,IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),"OCT 27-29, 2020OCT 27-29, 2020",IEEEIEEE,ELECTR NETWORKELECTR NETWORK,70,1,0,0,6,0,71,,,,,978-1-7281-9468-4,WOS:000662133900015,,
J,"Sinha, Ashish; Dolz, Jose",,,,,"Dolz, Jose/0000-0002-2436-7750; Sinha, Ashish/0000-0003-1395-6629",,,Multi-Scale Self-Guided Attention for Medical Image Segmentation,,,,,,,,IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS,,,,25,1,,,121,130,,10.1109/JBHI.2020.2986926,,,,JAN 2021,2021,"Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation, standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder architectures, leads to a redundant use of information, where similar low-level features are extracted multiple times at multiple scales. Second, long-range feature dependencies are not efficiently modeled, resulting in non-optimal discriminative feature representations associated with each semantic class. In this paper we attempt to overcome these limitations with the proposed architecture, by capturing richer contextual dependencies based on the use of guided self-attention mechanisms. This approach is able to integrate local features with their corresponding global dependencies, as well as highlight interdependent channel maps in an adaptive manner. Further, the additional loss between different modules guides the attention mechanisms to neglect irrelevant information and focus on more discriminant regions of the image by emphasizing relevant feature associations. We evaluate the proposed model in the context of semantic segmentation on three different datasets: abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments support the importance of these attention modules in the proposed architecture. In addition, compared to other state-of-the-art segmentation networks our model yields better segmentation performance, increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates the efficiency of our approach to generate precise and reliable automatic segmentations of medical images. Our code is made publicly available at: <uri>https://github.com/sinAshish/Multi-Scale-Attention</uri>.",,,,,,,,,65,3,1,0,12,0,68,,,2168-2194,2168-2208,,WOS:000641705100012,32305947,
J,"Keuper, Margret; Tang, Siyu; Andres, Bjoern; Brox, Thomas; Schiele, Bernt",,,,,,,,Motion Segmentation & Multiple Object Tracking by Correlation Co-Clustering,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,42,1,,,140,153,,10.1109/TPAMI.2018.2876253,,,,JAN 1 2020,2020,"Models for computer vision are commonly defined either w.r.t. low-level concepts such as pixels that are to be grouped, or w.r.t. high-level concepts such as semantic objects that are to be detected and tracked. Combining bottom-up grouping with top-down detection and tracking, although highly desirable, is a challenging problem. We state this joint problem as a co-clustering problem that is principled and tractable by existing algorithms. We demonstrate the effectiveness of this approach by combining bottom-up motion segmentation by grouping of point trajectories with high-level multiple object tracking by clustering of bounding boxes. We show that solving the joint problem is beneficial at the low-level, in terms of the FBMS59 motion segmentation benchmark, and at the high-level, in terms of the Multiple Object Tracking benchmarks MOT15, MOT16, and the MOT17 challenge, and is state-of-the-art in some metrics.",,,,,,,,,66,2,0,0,1,0,68,,,0162-8828,1939-3539,,WOS:000502294300011,30334779,
J,"Zhang, Xiangrong; Ma, Wenkang; Li, Chen; Wu, Jie; Tang, Xu; Jiao, Licheng",,,,,"Jiao, Licheng/0000-0003-3354-9617",,,Fully Convolutional Network-Based Ensemble Method for Road Extraction From Aerial Images,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,17,10,,,1777,1781,,10.1109/LGRS.2019.2953523,,,,OCT 2020,2020,"This letter proposed a road extraction method based on fully convolutional networks (FCNs) with an ensemble strategy in order to solve the imbalance of road and background areas in aerial images. By utilizing the FCN, we consider road extraction as a semantic segmentation problem. In the network, the weight of the loss function is modified because of the imbalance between the roads and backgrounds, and there will be a larger punishment if roads are wrongly classified as background. Since it is difficult to determine an appropriate weight of the loss function for a given image, an ensemble method based on spatial consistency (SC) is proposed. The result maps that are obtained from the FCNs with different loss functions are fused in our proposed ensemble strategy, which also avoids the determination of weights. Our method is tested using the Massachusetts road data set, and it was proven to be effective compared with the base fully convolutional model according to our experimental result.",,,,,,,,,66,2,0,0,5,0,67,,,1545-598X,1558-0571,,WOS:000573808500024,,
J,"Xie, Yuxing; Tian, Jiaojiao; Zhu, Xiao Xiang",,,,"Zhu, Xiao Xiang/ABE-7138-2020","Zhu, Xiao Xiang/0000-0001-5530-3613; Tian, Jiaojiao/0000-0002-8407-5098; Xie, Yuxing/0000-0002-6408-5109",,,Linking Points With Labels in 3D: A Review of Point Cloud Semantic Segmentation,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE,,,,8,4,,,38,59,,10.1109/MGRS.2019.2937630,,,,DEC 2020,2020,"Ripe with possibilities offered by deep-learning techniques and useful in applications related to remote sensing, computer vision, and robotics, 3D point cloud semantic segmentation (PCSS) and point cloud segmentation (PCS) are attracting increasing interest. This article summarizes available data sets and relevant studies on recent developments in PCSS and PCS.",,,,,,,,,64,3,0,0,1,0,66,,,2473-2397,2168-6831,,WOS:000600285900005,,
J,"Hao, Shijie; Zhou, Yuan; Guo, Yanrong",,,,,,,,A Brief Survey on Semantic Segmentation with Deep Learning,,,,,,,,NEUROCOMPUTING,,,,406,,,,302,321,,10.1016/j.neucom.2019.11.118,,,,SEP 17 2020,2020,,,,,,,,,,65,1,0,0,4,0,66,,,0925-2312,1872-8286,,WOS:000540922000001,,
J,"Liao, Minghui; Lyu, Pengyuan; He, Minghang; Yao, Cong; Wu, Wenhao; Bai, Xiang",,,,,"Liao, Minghui/0000-0002-2583-4314; Bai, Xiang/0000-0002-3449-5940",,,Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,43,2,,,532,548,,10.1109/TPAMI.2019.2937086,,,,FEB 2021,2021,"Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.",,,,,,,,,61,4,0,0,0,0,65,,,0162-8828,1939-3539,,WOS:000607795000002,31449005,
C,"Pan, Fei; Shin, Inkyu; Rameau, Francois; Lee, Seokju; Kweon, In So",,,IEEE,"Lee, Seokju/ABD-4821-2021","Lee, Seokju/0000-0002-8692-7653",,,Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,3763,3772,,10.1109/CVPR42600.2020.00382,,,,2020,2020,"Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model, from this adaptation, we separate target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard subdomain. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,65,0,0,0,3,0,65,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504004,,
C,"Rosinol, Antoni; Abate, Marcus; Chang, Yun; Carlone, Luca",,,IEEE,,"Chang, Yun/0000-0002-2829-5256",,,Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping,,,,,,,,2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA),,IEEE International Conference on Robotics and Automation ICRA,,,,,,1689,1696,,,,,,2020,2020,"We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.",,,,,IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA),"MAY 31-JUN 15, 2020MAY 31-JUN 15, 2020",IEEEIEEE,ELECTR NETWORKELECTR NETWORK,63,2,0,0,0,0,65,,,1050-4729,2577-087X,978-1-7281-7395-5,WOS:000712319501044,,
J,"Boulch, Alexandre",,,,,,,,ConvPoint: Continuous convolutions for point cloud processing,,,,,,,,COMPUTERS & GRAPHICS-UK,,,,88,,,,24,34,,10.1016/j.cag.2020.02.005,,,,MAY 2020,2020,"Point clouds are unstructured and unordered data, as opposed to images. Thus, most machine learning approach developed for image cannot be directly transferred to point clouds. In this paper, we propose a generalization of discrete convolutional neural networks (CNNs) in order to deal with point clouds by replacing discrete kernels by continuous ones. This formulation is simple, allows arbitrary point cloud sizes and can easily be used for designing neural networks similarly to 2D CNNs. We present experimental results with various architectures, highlighting the flexibility of the proposed approach. We obtain competitive results compared to the state-of-the-art on shape classification, part segmentation and semantic segmentation for large-scale point clouds. (C) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,59,7,0,0,0,0,64,,,0097-8493,1873-7684,,WOS:000534375700006,,
J,"Zhang, Ziang; Wu, Chengdong; Coleman, Sonya; Kerr, Dermot",,,,,"Coleman, Sonya/0000-0002-4676-7640",,,DENSE-INception U -net for medical image segmentation,,,,,,,,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,,,,192,,,,,,105395,10.1016/j.cmpb.2020.105395,,,,AUG 2020,2020,,,,,,,,,,54,7,0,0,14,0,61,,,0169-2607,1872-7565,,WOS:000540460400012,32163817,
J,"Wu, Tianyi; Tang, Sheng; Zhang, Rui; Cao, Juan; Zhang, Yongdong",,,,"Tang, Sheng/L-5792-2013","Tang, Sheng/0000-0003-3573-2407; Wu, Tianyi/0000-0001-7434-0487",,,CGNet: A Light-Weight Context Guided Network for Semantic Segmentation,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,30,,,,1169,1179,,10.1109/TIP.2020.3042065,,,,2021,2021,"The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters hence unsuitable for mobile devices, while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the joint feature of both local feature and surrounding context effectively and efficiently, and further improves the joint feature with the global context. Based on the CG block, we develop CGNet which captures contextual information in all stages of the network. CGNet is specially tailored to exploit the inherent property of semantic segmentation and increase the segmentation accuracy. Moreover, CGNet is elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing light-weight segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters.",,,,,,,,,58,2,0,0,1,0,60,,,1057-7149,1941-0042,,WOS:000600835900003,33306466,
J,"Huang, Qinghua; Huang, Yonghao; Luo, Yaozhong; Yuan, Feiniu; Li, Xuelong",,,,"Li, Xuelong/ABF-3381-2020","Huang, Qinghua/0000-0003-1080-6940",,,Segmentation of breast ultrasound image with semantic classification of superpixels,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,61,,,,,,101657,10.1016/j.media.2020.101657,,,,APR 2020,2020,"Breast cancer is a great threat to females. Ultrasound imaging has been applied extensively in diagnosis of breast cancer. Due to the poor image quality, segmentation of breast ultrasound (BUS) image remains a very challenging task. Besides, BUS image segmentation is a crucial step for further analysis. In this paper, we proposed a novel method to segment the breast tumor via semantic classification and merging patches. The proposed method firstly selects two diagonal points to crop a region of interest (ROI) on the original image. Then, histogram equalization, bilateral filter and pyramid mean shift filter are adopted to enhance the image. The cropped image is divided into many superpixels using simple linear iterative clustering (SLIC). Furthermore, some features are extracted from the superpixels and a bag-of-words model can be created. The initial classification can be obtained by a back propagation neural network (BPNN). To refine preliminary result, k-nearest neighbor (KNN) is used for reclassification and the final result is achieved. To verify the proposed method, we collected a BUS dataset containing 320 cases. The segmentation results of our method have been compared with the corresponding results obtained by five existing approaches. The experimental results show that our method achieved competitive results compared to conventional methods in terms of TP and FP, and produced good approximations to the hand-labelled tumor contours with comprehensive consideration of all metrics (the F1-score = 89.87% +/- 4.05%, and the average radial error = 9.95% +/- 4.42%). (C) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,58,2,0,0,8,0,60,,,1361-8415,1361-8423,,WOS:000521115200012,32032899,
J,"Ju, Huyan; Li, Wei; Tighe, Susan; Xu, Zhengchao; Zhai, Junzhi",,,,"huyan, ju/ABG-8865-2020","huyan, ju/0000-0002-5292-9896; Li, Wei/0000-0003-4508-3076",,,CrackU-net: A novel deep convolutional neural network for pixelwise pavement crack detection,,,,,,,,STRUCTURAL CONTROL & HEALTH MONITORING,,,,27,8,,,,,e2551,10.1002/stc.2551,,MAR 2020,,AUG 2020,2020,"Periodic road crack monitoring is an essential procedure for effective pavement management. Highly efficient and accurate crack measurements are key research topics in both academia and industry. Automatic methods gradually replaced traditional manual surveys for more reliable evaluation outputs and better efficiency, whereas the devices are not available to all functional classes of pavements and different departments considering the high cost versus the limited budget. Recently, the widespread use of smartphones and digital cameras made it possible to collect pavement surface crack images at an affordable price in easier ways. However, the qualities of these crack images are diversely influenced by the noises from pavement background, roadways, and so forth. Thus, traditional methods usually fail to extract accurate crack information from pavement images. Therefore, this research proposes a state-of-the-art pixelwise crack detection architecture called CrackU-net, which is featured by its utilization of advanced deep convolutional neural network technology. CrackU-net achieved pixelwise crack detection through convolution, pooling, transpose convolution, and concatenation operations, forming the U-shaped model architecture. The model is trained and validated by 3,000 pavement crack images, in which 2,400 for training and 600 for validating, using the Adam algorithm. CrackU-net has the performance of loss = 0.025, accuracy = 0.9901, precision = 0.9856, recall = 0.9798, and F-measure = 0.9842 with learning rate of 10(-2). Meanwhile, the false-positive crack detection problem is avoided in CrackU-net. Therefore, CrackU-net outperforms both traditional approaches and fully convolutional network (FCN) and U-net for pixelwise crack detections.",,,,,,,,,59,1,0,0,0,0,59,,,1545-2255,1545-2263,,WOS:000521535100001,,
C,"Jha, Debesh; Smedsrud, Pia H.; Riegler, Michael A.; Halvorsen, Pal; de Lange, Thomas; Johansen, Dag; Johansen, Havard D.",,,,"Riegler, Michael A/E-5443-2015; de Lange, Thomas/Q-9063-2016","Riegler, Michael A/0000-0002-3153-2064; de Lange, Thomas/0000-0003-3989-7487; Jha, Debesh/0000-0002-8078-6730","Ro, YM; Cheng, WH; Kim, J; Chu, WT; Cui, P; Choi, JW; Hu, MC; DeNeve, W",,Kvasir-SEG: A Segmented Polyp Dataset,,,,,,,,"MULTIMEDIA MODELING (MMM 2020), PT II",,Lecture Notes in Computer Science,,11962,,,,451,462,,10.1007/978-3-030-37734-2_37,,,,2020,2020,"Pixel-wise image segmentation is a highly demanding task in medical-image analysis. In practice, it is difficult to find annotated medical images with corresponding segmentation masks. In this paper, we present Kvasir-SEG: an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. Moreover, we also generated the bounding boxes of the polyp regions with the help of segmentation masks. We demonstrate the use of our dataset with a traditional segmentation approach and a modern deep-learning based Convolutional Neural Network (CNN) approach. The dataset will be of value for researchers to reproduce results and compare methods. By adding segmentation masks to the Kvasir dataset, which only provide frame-wise annotations, we enable multimedia and computer vision researchers to contribute in the field of polyp segmentation and automatic analysis of colonoscopy images.",,,,,26th International Conference on MultiMedia Modeling (MMM)26th International Conference on MultiMedia Modeling (MMM),"JAN 05-08, 2020JAN 05-08, 2020",,"Daejeon, SOUTH KOREADaejeon, SOUTH KOREA",58,1,0,0,9,0,59,,,0302-9743,1611-3349,978-3-030-37734-2; 978-3-030-37733-5,WOS:000611566100037,,
B,Yude Wang; Jie Zhang; Meina Kan; Shiguang Shan; Xilin Chen,,,,,"Shan, Shiguang/0000-0002-8348-392X",,,Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12272,81,,10.1109/CVPR42600.2020.01229,,,,2020,2020,"Image-level weakly supervised semantic segmentation is a challenging problem that has been deeply studied in recent years. Most of advanced solutions exploit class activation map (CAM). However, CAMs can hardly serve as the object mask due to the gap between full and weak supervisions. In this paper, we propose a self-supervised equivariant attention mechanism (SEAM) to discover additional supervision and narrow the gap. Our method is based on the observation that equivariance is an implicit constraint in fully supervised semantic segmentation, whose pixel-level labels take the same spatial transformation as the input images during data augmentation. However, this constraint is lost on the CAMs trained by image-level supervision. Therefore, we propose consistency regularization on predicted CAMs from various transformed images to provide self-supervision for network learning. Moreover, we propose a pixel correlation module (PCM), which exploits context appearance information and refines the prediction of current pixel by its similar neighbors, leading to further improvement on CAMs consistency. Extensive experiments on PASCAL VOC 2012 dataset demonstrate our method outperforms state-of-the-art methods using the same level of supervision. The code is released online.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",57,3,0,0,2,0,59,,,,,978-1-7281-7168-5,INSPEC:19873955,,
J,"Prakash, Nikhil; Manconi, Andrea; Loew, Simon",,,,"; Manconi, Andrea/B-8367-2009","Prakash, Nikhil/0000-0002-5216-5881; Manconi, Andrea/0000-0003-2930-4422; Loew, Simon/0000-0003-4014-1425",,,Mapping Landslides on EO Data: Performance of Deep Learning Models vs. Traditional Machine Learning Models,,,,,,,,REMOTE SENSING,,,,12,3,,,,,346,10.3390/rs12030346,,,,FEB 2020,2020,"Mapping landslides using automated methods is a challenging task, which is still largely done using human efforts. Today, the availability of high-resolution EO data products is increasing exponentially, and one of the targets is to exploit this data source for the rapid generation of landslide inventory. Conventional methods like pixel-based and object-based machine learning strategies have been studied extensively in the last decade. In addition, recent advances in CNN (convolutional neural network), a type of deep-learning method, has been widely successful in extracting information from images and have outperformed other conventional learning methods. In the last few years, there have been only a few attempts to adapt CNN for landslide mapping. In this study, we introduce a modified U-Net model for semantic segmentation of landslides at a regional scale from EO data using ResNet34 blocks for feature extraction. We also compare this with conventional pixel-based and object-based methods. The experiment was done in Douglas County, a study area selected in the south of Portland in Oregon, USA, and landslide inventory extracted from SLIDO (Statewide Landslide Information Database of Oregon) was considered as the ground truth. Landslide mapping is an imbalanced learning problem with very limited availability of training data. Our network was trained on a combination of focal Tversky loss and cross-entropy loss functions using augmented image tiles sampled from a selected training area. The deep-learning method was observed to have a better performance than the conventional methods with an MCC (Matthews correlation coefficient) score of 0.495 and a POD (probability of detection) rate of 0.72.",,,,,,,,,56,3,0,0,5,0,58,,,,2072-4292,,WOS:000515393800005,,
J,"Li, Chongyi; Cong, Runmin; Kwong, Sam; Hou, Junhui; Fu, Huazhu; Zhu, Guopu; Zhang, Dingwen; Huang, Qingming",,,,"Fu, Huazhu/A-1411-2014; Zhang, Dingwen/S-9447-2017; , sam/C-9319-2012","Fu, Huazhu/0000-0002-9702-5524; Hou, Junhui/0000-0003-3431-2021; Zhu, Guopu/0000-0001-7956-5343; Zhang, Dingwen/0000-0001-8369-8886; , sam/0000-0001-7484-7261",,,ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection,,,,,,,,IEEE TRANSACTIONS ON CYBERNETICS,,,,51,1,,,88,100,,10.1109/TCYB.2020.2969255,,,,JAN 2021,2021,"Salient object detection from RGB-D images is an important yet challenging vision task, which aims at detecting the most distinctive objects in a scene by combining color information and depth constraints. Unlike prior fusion manners, we propose an attention steered interweave fusion network (ASIF-Net) to detect salient objects, which progressively integrates cross-modal and cross-level complementarity from the RGB image and corresponding depth map via steering of an attention mechanism. Specifically, the complementary features from RGB-D images are jointly extracted and hierarchically fused in a dense and interweaved manner. Such a manner breaks down the barriers of inconsistency existing in the cross-modal data and also sufficiently captures the complementarity. Meanwhile, an attention mechanism is introduced to locate the potential salient regions in an attention-weighted fashion, which advances in highlighting the salient objects and suppressing the cluttered background regions. Instead of focusing only on pixelwise saliency, we also ensure that the detected salient objects have the objectness characteristics (e.g., complete structure and sharp boundary) by incorporating the adversarial learning that provides a global semantic constraint for RGB-D salient object detection. Quantitative and qualitative experiments demonstrate that the proposed method performs favorably against 17 state-of-the-art saliency detectors on four publicly available RGB-D salient object detection datasets. The code and results of our method are available at https://github.com/Li-Chongyi/ASIF-Net.",,,,,,,,,57,2,0,0,1,0,57,,,2168-2267,2168-2275,,WOS:000602709000008,32078571,
C,"Yang, Yanchao; Soatto, Stefano",,,IEEE,,,,,FDA: Fourier Domain Adaptation for Semantic Segmentation,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4084,4094,,10.1109/CVPR42600.2020.00414,,,,2020,2020,"We describe a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. We illustrate the method in semantic segmentation, where densely annotated images are aplenty in one domain (e.g., synthetic data), but difficult to obtain in another (e.g., real images). Current state-of-the-art methods are complex, some requiring adversarial optimization to render the backbone of a neural network invariant to the discrete domain selection variable. Our method does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Despite its simplicity, it achieves state-of-the-art performance in the current benchmarks, when integrated into a relatively standard semantic segmentation model. Our results indicate that even simple procedures can discount nuisance variability in the data that more sophisticated methods struggle to learn away.(1)",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,57,0,0,0,4,0,57,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504036,,
J,"Wang, Zhao; Liu, Quande; Dou, Qi",,,,"Wang, Zhao/AAG-2483-2021; Wang, Zhao/ABE-7750-2021; Wang, Zhao/AAG-2474-2021","Wang, Zhao/0000-0003-1860-8391; Liu, Quande/0000-0002-3921-5960",,,Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification,,,,,,,,IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS,,,,24,10,,,2806,2813,,10.1109/JBHI.2020.3023246,,,,OCT 2020,2020,"The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performanceson both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.",,,,,,,,,55,1,0,0,13,0,56,,,2168-2194,2168-2208,,WOS:000576429900009,32915751,
J,"Yo, Hongfeng; Tian, Shengwei; Yu, Long; Lv, Yalong",,,,,"You, Hongfeng/0000-0001-9202-7611",,,Pixel-Level Remote Sensing Image Recognition Based on Bidirectional Word Vectors,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,2,,,1281,1293,,10.1109/TGRS.2019.2945591,,,,FEB 2020,2020,"In the traditional remote sensing image recognition, the traditional features (e.g., color features and texture features) cannot fully describe complex images, and the relationships between image pixels cannot be captured well. Using a single model or a traditional sequential joint model, it is easy to lose deep features during feature mining. This article proposes a new feature extraction method that uses the word embedding method from natural language processing to generate bidirectional real dense vectors to reflect the contextual relationships between the pixels. A bidirectional independent recurrent neural network (BiIndRNN) is combined with a convolutional neural network (CNN) to improve the sliced recurrent neural network (SRNN) algorithm model, which is then constructed in parallel with graph convolutional networks (GCNs) under an attention mechanism to fully exploit the deep features of images and to capture the semantic information of the context. This model is collectively named an improved SRNN and attention-treated GCN-based parallel (SAGP) model. Experiments conducted on Populus euphratica forests demonstrate that the proposed method outperforms traditional methods in terms of recognition accuracy. The validation done on public data set also proved it.",,,,,,,,,55,2,0,0,1,0,56,,,0196-2892,1558-0644,,WOS:000510710600042,,
J,"Chen, Mingyou; Tang, Yunchao; Zou, Xiangjun; Huang, Kuangyu; Huang, Zhaofeng; Zhou, Hao; Wang, Chenglin; Lian, Guoping",,,,"zou, xiangjun/ABG-6926-2021; Tang, Yunchao/T-6632-2019","zou, xiangjun/0000-0001-5146-599X; Tang, Yunchao/0000-0002-6178-4457",,,Three-dimensional perception of orchard banana central stock enhanced by adaptive multi-vision technology,,,,,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,174,,,,,,105508,10.1016/j.compag.2020.105508,,,,JUL 2020,2020,"Automatic vision-based picking in orchards and fields is a highly challenging task. The orchard banana central stock, which is large in size, low in color contrast, and falls within a complex background, was taken as the subject in this research. A measurement framework based on multi-vision technology was established, and a set of general methods were utilized to improve the comprehensive performance of multi-view-geometry-based vision modules in orchard picking tasks. Multiple cameras at different angles were deployed to maximize the perception range. The global geometric parameters of the cameras were calibrated and a robust semantic segmentation network was trained to achieve effective image pre-processing. A novel adaptive stereo matching strategy was designed to ensure that the robot reliably completes 3D triangulation at various depths as it moves across the target area. Global calibration errors were corrected via a high-accuracy point cloud stitching algorithm. Experimental results indicated that the proposed adaptive stereo matching strategy was accurate to different sampling depths and showed stable performance, and the proposed point cloud stitching algorithm accurately stitched multi-view point clouds. This work provides theoretical and practical references for the 3D sensing of banana central stocks in complex environments. The proposed technique was designed for adaptability of the multi-vision system for field perception, so it can be easily transferred to similar applications such as the 3D reconstruction of agricultural targets, 3D positioning of fruit clusters, and 3D robotic arm obstacle avoidance.",,,,,,,,,51,4,0,0,8,0,55,,,0168-1699,1872-7107,,WOS:000540218000043,,
J,"Shen, Jianbing; Tang, Xin; Dong, Xingping; Shao, Ling",,,,"Dong, Xingping/AAA-8107-2019; Shen, Jianbing/U-8796-2019","Dong, Xingping/0000-0003-1613-9288; ",,,Visual Object Tracking by Hierarchical Attention Siamese Network,,,,,,,,IEEE TRANSACTIONS ON CYBERNETICS,,,,50,7,,,3068,3080,,10.1109/TCYB.2019.2936503,,,,JUL 2020,2020,"Visual tracking addresses the problem of localizing an arbitrary target in video according to the annotated bounding box. In this article, we present a novel tracking method by introducing the attention mechanism into the Siamese network to increase its matching discrimination. We propose a new way to compute attention weights to improve matching performance by a sub-Siamese network [Attention Net (A-Net)], which locates attentive parts for solving the searching problem. In addition, features in higher layers can preserve more semantic information while features in lower layers preserve more location information. Thus, in order to solve the tracking failure cases by the higher layer features, we fully utilize location and semantic information by multilevel features and propose a new way to fuse multiscale response maps from each layer to obtain a more accurate position estimation of the object. We further propose a hierarchical attention Siamese network by combining the attention weights and multilayer integration for tracking. Our method is implemented with a pretrained network which can outperform most well-trained Siamese trackers even without any fine-tuning and online updating. The comparison results with the state-of-the-art methods on popular tracking benchmarks show that our method achieves better performance. Our source code and results will be available at https://github.com/shenjianbing/HASN.",,,,,,,,,55,1,0,0,1,0,55,,,2168-2267,2168-2275,,WOS:000544035300016,31536029,
J,"Zhang, Dingwen; Han, Junwei; Zhang, Yu; Xu, Dong",,,,"Xu, Dong/A-3694-2011","Xu, Dong/0000-0003-2775-9730",,,Synthesizing Supervision for Learning Deep Saliency Network without Human Annotation,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,42,7,,,1755,1769,,10.1109/TPAMI.2019.2900649,,,,JUL 1 2020,2020,"Recently, the research field of salient object detection is undergoing a rapid and remarkable development along with the wide usage of deep neural networks. Being trained with a large number of images annotated with strong pixel-level ground-truth masks, the deep salient object detectors have achieved the state-of-the-art performance. However, it is expensive and time-consuming to provide the pixel-level ground-truth masks for each training image. To address this problem, this paper proposes one of the earliest frameworks to learn deep salient object detectors without requiring any human annotation. The supervisory signals used in our learning framework are generated through a novel supervision synthesis scheme, in which the key insights are knowledge source transition and supervision by fusion. Specifically, in the proposed learning framework, both the external knowledge source and the internal knowledge source are explored dynamically to provide informative cues for synthesizing supervision required in our approach, while a two-stream fusion mechanism is also established to implement the supervision synthesis process. Comprehensive experiments on four benchmark datasets demonstrate that the deep salient object detector trained by our newly proposed learning framework often works well without requiring any human annotated masks, which even approaches to its upper-bound obtained under the fully supervised learning fashion (within only 3 percent performance gap). Besides, we also apply the salient object detector learnt with our annotation-free learning framework to assist the weakly supervised semantic segmentation task, which demonstrates that our approach can also alleviate the heavy supplementary supervision required in the existing weakly supervised semantic segmentation framework.",,,,,,,,,55,0,0,0,3,0,55,,,0162-8828,1939-3539,,WOS:000542967200018,30794509,
J,"Mi, Li; Chen, Zhenzhong",,,,,,,,Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,159,,,,140,152,,10.1016/j.isprsjprs.2019.11.006,,,,JAN 2020,2020,"Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects' spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF.",,,,,,,,,52,0,0,0,1,0,52,,,0924-2716,1872-8235,,WOS:000508739800012,,
C,"Zhu, Peihao; Abdal, Rameen; Qin, Yipeng; Wonka, Peter",,,IEEE,,"Qin, Yipeng/0000-0002-1551-9126",,,SEAN: Image Synthesis with Semantic Region-Adaptive Normalization,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,5103,5112,,10.1109/CVPR42600.2020.00515,,,,2020,2020,"We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region. Code: https: //github. com/ZPdesu/SEAN.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,51,1,0,0,0,0,52,,,1063-6919,,978-1-7281-7168-5,WOS:000620679505038,,
J,"Kumar, Neeraj; Verma, Ruchika; Anand, Deepak; Zhou, Yanning; Onder, Omer Fahri; Tsougenis, Efstratios; Chen, Hao; Heng, Pheng-Ann; Li, Jiahui; Hu, Zhiqiang; Wang, Yunzhi; Koohbanani, Navid Alemi; Jahanifar, Mostafa; Tajeddin, Neda Zamani; Gooya, Ali; Rajpoot, Nasir; Ren, Xuhua; Zhou, Sihang; Wang, Qian; Shen, Dinggang; Yang, Cheng-Kun; Weng, Chi-Hung; Yu, Wei-Hsiang; Yeh, Chao-Yuan; Yang, Shuang; Xu, Shuoyu; Yeung, Pak Hei; Sun, Peng; Mahbod, Amirreza; Schaefer, Gerald; Ellinger, Isabella; Ecker, Rupert; Smedby, Orjan; Wang, Chunliang; Chidester, Benjamin; That-Vinh Ton; Minh-Triet Tran; Ma, Jian; Minh N Do; Graham, Simon; Quoc Dang Vu; Kwak, Jin Tae; Gunda, Akshaykumar; Chunduri, Raviteja; Hu, Corey; Zhou, Xiaoyang; Lotfi, Dariush; Safdari, Reza; Kascenas, Antanas; O'Neil, Alison; Eschweiler, Dennis; Stegmaier, Johannes; Cui, Yanping; Yin, Baocai; Chen, Kailin; Tian, Xinmei; Gruening, Philipp; Barth, Erhardt; Arbel, Elad; Remer, Itay; Ben-Dor, Amir; Sirazitdinova, Ekaterina; Kohl, Matthias; Braunewell, Stefan; Li, Yuexiang; Xie, Xinpeng; Shen, Linlin; Ma, Jun; Das Baksi, Krishanu; Khan, Mohammad Azam; Choo, Jaegul; Colomer, Adrian; Naranjo, Valery; Pei, Linmin; Lftekharuddin, Khan M.; Roy, Kaushiki; Bhattacharjee, Debotosh; Pedraza, Anibal; Bueno, Maria Gloria; Devanathan, Sabarinathan; Radhakrishnan, Saravanan; Koduganty, Praveen; Wu, Zihan; Cai, Guanyu; Liu, Xiaojie; Wang, Yuqin; Sethi, Amit",,,,"Li, Yuexiang/AAZ-2676-2020; Stegmaier, Johannes/ABA-4230-2021; Mahbod, Amirreza/D-5175-2018; Verma, Ruchika/AAD-9345-2019; N., Minh/AAX-8498-2020; nathan, sabari/AAV-9169-2020; Kumar, Neeraj/L-3500-2016; Safdari, Reza/AAK-7482-2021; Choo, Jaegul/ABF-8315-2020; Shen, Dinggang/ABF-6812-2020; Ellinger, Isabella/AAV-9170-2020; Ma, Jian/A-9838-2008; Pedraza, Anibal/AAI-5341-2020; Pei, Linmin/AHA-2433-2022; Bueno, Gloria/K-6286-2014","Li, Yuexiang/0000-0001-8076-2619; Stegmaier, Johannes/0000-0003-4072-3759; Mahbod, Amirreza/0000-0001-5042-1442; Verma, Ruchika/0000-0003-4870-128X; N., Minh/0000-0001-5132-4986; Kumar, Neeraj/0000-0002-3020-3947; Choo, Jaegul/0000-0003-1071-4835; Shen, Dinggang/0000-0002-7934-5698; Ma, Jian/0000-0002-4202-5834; Pedraza, Anibal/0000-0001-7748-6756; Bueno, Gloria/0000-0002-7345-4869; Rajpoot, Nasir/0000-0002-4706-1308; Heng, Pheng Ann/0000-0003-3055-5034; Remer, Itay/0000-0003-4669-782X; Kwak, Jin Tae/0000-0003-0287-4097; Ellinger, Isabella/0000-0001-6335-076X; Ecker, Rupert/0000-0002-1095-8592; nathan, sabari/0000-0003-2621-0690; Colomer, Adrian/0000-0002-7616-6029; Pei, Linmin/0000-0001-6135-9429; Yeh, Chao-Yuan/0000-0002-1947-1596; Zhou, Sihang/0000-0003-1491-4594",,,A Multi-Organ Nucleus Segmentation Challenge,,,,,,,,IEEE TRANSACTIONS ON MEDICAL IMAGING,,,,39,5,,,1380,1391,,10.1109/TMI.2019.2947628,,,,MAY 2020,2020,"Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.",,,,,,,,,50,2,0,0,13,0,51,,,0278-0062,1558-254X,,WOS:000532214700009,31647422,
J,"Su, Zhidong; Li, Yang; Yang, Guanci",,,,,"Yang, Guanci/0000-0001-8761-5195; Su, Zhidong/0000-0001-6592-0666; Li, Yang/0000-0002-3006-7420",,,Dietary Composition Perception Algorithm Using Social Robot Audition for Mandarin Chinese,,,,,,,,IEEE ACCESS,,,,8,,,,8768,8782,,10.1109/ACCESS.2019.2963560,,,,2020,2020,"As the problem of an aging population becomes more and more serious, social robots have an increasingly significant influence on human life. By employing regular question-and-answer conversations or wearable devices, some social robotics products can establish personal health archives. But those robots are unable to collect diet information automatically through robot vision or audition. A healthy diet can reduce a person;s risk of developing cancer, diabetes, heart disease, and other age-related diseases. In order to automatically perceive the dietary composition of the elderly by listening to people;s chatting, this paper proposed a chat-based automatic dietary composition perception algorithm (DCPA). DCPA uses social robot audition to understand the semantic information and percept dietary composition for Mandarin Chinese. Firstly, based on the Mel-frequency cepstrum coefficient and convolutional neural network, a speaker recognition method is designed to identify speech data. Based on speech segmentation and speaker recognition algorithm, an audio segment classification method is proposed to distinguish different speakers, store their identity information and the sequence of expression in a speech conversation. Secondly, a dietetic lexicon is established, and two kinds of dietary composition semantic understanding algorithms are proposed to understand the eating semantics and sensor dietary composition information. To evaluate the performance of the proposed DCPA algorithm, we implemented the proposed DCPA in our social robot platform. Then we established two categories of test datasets relating to a one-person and a multi-person chat. The test results show that DCPA is capable of understanding users; dietary compositions, with an F1 score of 0.9505, 0.8940 and 0.8768 for one-person talking, a two-person chat and a three-person chat, respectively. DCPA has good robustness for obtaining dietary information.",,,,,,,,,50,1,0,0,3,1,51,,,2169-3536,,,WOS:000525422900009,,
J,"Heller, Nicholas; Isensee, Fabian; Maier-Hein, Klaus H.; Hou, Xiaoshuai; Xie, Chunmei; Li, Fengyi; Nan, Yang; Mu, Guangrui; Lin, Zhiyong; Han, Miofei; Yao, Guang; Gao, Yaozong; Zhang, Yao; Wang, Yixin; Hou, Feng; Yang, Jiawei; Xiong, Guangwei; Tian, Jiang; Zhong, Cheng; Ma, Jun; Rickman, Jack; Dean, Joshua; Stai, Bethany; Tejpaul, Resha; Oestreich, Makinna; Blake, Paul; Kaluzniak, Heather; Raza, Shaneabbas; Rosenberg, Joel; Moore, Keenan; Walczak, Edward; Rengel, Zachary; Edgerton, Zach; Vasdev, Ranveer; Peterson, Matthew; McSweeney, Sean; Peterson, Sarah; Kalapara, Arveen; Sathianathen, Niranjan; Papanikolopoulos, Nikolaos; Weight, Christopher",,,,"Sathianathen, Niranjan/H-6701-2019; Heller, Nicholas/AAI-2524-2019","Sathianathen, Niranjan/0000-0002-3710-014X; Heller, Nicholas/0000-0001-8516-8707; McSweeney, Sean/0000-0002-7682-2073; Weight, Christopher/0000-0001-9381-959X",,,The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,67,,,,,,101821,10.1016/j.media.2020.101821,,,,JAN 2021,2021,"There is a large body of literature linking anatomic and geometric characteristics of kidney tumors to perioperative and oncologic outcomes. Semantic segmentation of these tumors and their host kidneys is a promising tool for quantitatively characterizing these lesions, but its adoption is limited due to the manual effort required to produce high-quality 3D segmentations of these structures. Recently, methods based on deep learning have shown excellent results in automatic 3D segmentation, but they require large datasets for training, and there remains little consensus on which methods perform best. The 2019 Kidney and Kidney Tumor Segmentation challenge (KiTS19) was a competition held in conjunction with the 2019 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) which sought to address these issues and stimulate progress on this automatic segmentation problem. A training set of 210 cross sectional CT images with kidney tumors was publicly released with corresponding semantic segmentation masks. 106 teams from five continents used this data to develop automated systems to predict the true segmentation masks on a test set of 90 CT images for which the corresponding ground truth segmentations were kept private. These predictions were scored and ranked according to their average Sorensen-Dice coefficient between the kidney and tumor across all 90 cases. The winning team achieved a Dice of 0.974 for kidney and 0.851 for tumor, approaching the inter-annotator performance on kidney (0.983) but falling short on tumor (0.923). This challenge has now entered an open leaderboard phase where it serves as a challenging benchmark in 3D semantic segmentation. (C) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,50,0,0,0,12,0,50,,,1361-8415,1361-8423,,WOS:000598891600009,33049579,
C,"Zheng, Sixiao; Lu, Jiachen; Zhao, Hengshuang; Zhu, Xiatian; Luo, Zekun; Wang, Yabiao; Fu, Yanwei; Feng, Jianfeng; Xiang, Tao; Torr, Philip H. S.; Zhang, Li",,,IEEE COMP SOC,,,,,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,,,,,,,,"2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021",,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,6877,6886,,10.1109/CVPR46437.2021.00681,,,,2021,2021,"Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE2OK test server leaderboard on the day of submission.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 19-25, 2021JUN 19-25, 2021",IEEE; IEEE Comp Soc; CVFIEEE; IEEE Comp Soc; CVF,ELECTR NETWORKELECTR NETWORK,50,0,0,0,0,0,50,,,1063-6919,,978-1-6654-4509-2,WOS:000739917307010,,
J,"Chen, Yuantao; Tao, Jiajun; Liu, Linwu; Xiong, Jie; Xia, Runlong; Xie, Jingbo; Zhang, Qian; Yang, Kai",,,,"Chen, Yuantao/AAC-7165-2019","Chen, Yuantao/0000-0003-2277-1765",,,Research of improving semantic image segmentation based on a feature fusion model,,,,,,,,JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,,,,,,,,,,,10.1007/s12652-020-02066-z,,MAY 2020,,,2020,"The context information of images had been lost due to the low resolution of features, and due to repeated combinations of max-pooling layer and down-sampling layer. When the feature extraction process had been performed using a convolutional network, the result of semantic image segmentation loses sensitivity to the location of the object. The semantic image segmentation based on a feature fusion model with context features layer-by-layer had been proposed. Firstly, the original images had been pre-processed by the Gaussian Kernel Function to generate a series of images with different resolutions to form an image pyramid. Secondly, inputting an image pyramid into the network structure in which the plurality of fully convolutional network was been combined in parallel to obtain a set of initial features with different granularities by expanding receptive fields using Atrous Convolutions, and the initialization of feature fusion with different layer-by-layer granularities in a top-down method. Finally, the score map of feature fusion model had been calculated and sent to the conditional random field, modeling the class correlations between image pixels of the original image by the fully connected conditional random field, and the spatial position information and color vector information of image pixels were jointed to optimize and obtain results. The experiments on the PASCAL VOC 2012 and PASCAL Context datasets had achieved better mean Intersection Over Union than the state-of-the-art works. The proposed method has about 6.3% improved to the conventional methods.",,,,,,,,,50,0,0,0,1,0,50,,,1868-5137,1868-5145,,WOS:000531212900003,,
J,"Zhang, Yiqing; Chu, Jun; Leng, Lu; Miao, Jun",,,,,"Zhang, Yiqing/0000-0002-1997-4871",,,Mask-Refined R-CNN: A Network for Refining Object Details in Instance Segmentation,,,,,,,,SENSORS,,,,20,4,,,,,1010,10.3390/s20041010,,,,FEB 2020,2020,"With the rapid development of flexible vision sensors and visual sensor networks, computer vision tasks, such as object detection and tracking, are entering a new phase. Accordingly, the more challenging comprehensive task, including instance segmentation, can develop rapidly. Most state-of-the-art network frameworks, for instance, segmentation, are based on Mask R-CNN (mask region-convolutional neural network). However, the experimental results confirm that Mask R-CNN does not always successfully predict instance details. The scale-invariant fully convolutional network structure of Mask R-CNN ignores the difference in spatial information between receptive fields of different sizes. A large-scale receptive field focuses more on detailed information, whereas a small-scale receptive field focuses more on semantic information. So the network cannot consider the relationship between the pixels at the object edge, and these pixels will be misclassified. To overcome this problem, Mask-Refined R-CNN (MR R-CNN) is proposed, in which the stride of ROIAlign (region of interest align) is adjusted. In addition, the original fully convolutional layer is replaced with a new semantic segmentation layer that realizes feature fusion by constructing a feature pyramid network and summing the forward and backward transmissions of feature maps of the same resolution. The segmentation accuracy is substantially improved by combining the feature layers that focus on the global and detailed information. The experimental results on the COCO (Common Objects in Context) and Cityscapes datasets demonstrate that the segmentation accuracy of MR R-CNN is about 2% higher than that of Mask R-CNN using the same backbone. The average precision of large instances reaches 56.6%, which is higher than those of all state-of-the-art methods. In addition, the proposed method requires low time cost and is easily implemented. The experiments on the Cityscapes dataset also prove that the proposed method has great generalization ability.",,,,,,,,,50,0,0,0,1,0,50,,,,1424-8220,,WOS:000522448600061,32069927,
C,"Vora, Sourabh; Lang, Alex H.; Helou, Bassam; Beijbom, Oscar",,,IEEE,,,,,PointPainting: Sequential Fusion for 3D Object Detection,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4603,4611,,10.1109/CVPR42600.2020.00466,,,,2020,2020,"Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to fill this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidaronly method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird's-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,47,2,0,0,0,0,49,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504088,,
J,"Goyal, Manu; Oakley, Amanda; Bansal, Priyanka; Dancey, Darren; Yap, Moi Hoon",,,,,"Yap, Moi Hoon/0000-0001-7681-4287; Goyal, Manu/0000-0002-9201-1385",,,Skin Lesion Segmentation in Dermoscopic Images With Ensemble Deep Learning Methods,,,,,,,,IEEE ACCESS,,,,8,,,,4171,4181,,10.1109/ACCESS.2019.2960504,,,,2020,2020,"Early detection of skin cancer, particularly melanoma, is crucial to enable advanced treatment. Due to the rapid growth in the number of skin cancers, there is a growing need of computerised analysis for skin lesions. The state-of-the-art public available datasets for skin lesions are often accompanied with a very limited amount of segmentation ground truth labeling. Also, the available segmentation datasets consist of noisy expert annotations reflecting the fact that precise annotations to represent the boundary of skin lesions are laborious and expensive. The lesion boundary segmentation is vital to locate the lesion accurately in dermoscopic images and lesion diagnosis of different skin lesion types. In this work, we propose the fully automated deep learning ensemble methods to achieve high sensitivity and high specificity in lesion boundary segmentation. We trained the ensemble methods based on Mask R-CNN and DeeplabV3 C methods on ISIC-2017 segmentation training set and evaluate the performance of the ensemble networks on ISIC-2017 testing set and PH2 dataset. Our results showed that the proposed ensemble methods segmented the skin lesions with Sensitivity of 89.93% and Specificity of 97.94% for the ISIC-2017 testing set. The proposed ensemble method Ensemble-A outperformed FrCN, FCNs, U-Net, and SegNet in Sensitivity by 4.4%, 8.8%, 22.7%, and 9.8% respectively. Furthermore, the proposed ensemble method Ensemble-S achieved a specificity score of 97.98% for clinically benign cases, 97.30% for the melanoma cases, and 98.58% for the seborrhoeic keratosis cases on ISIC-2017 testing set, exhibiting better performance than FrCN, FCNs, U-Net, and SegNet.",,,,,,,,,48,1,1,0,0,0,48,,,2169-3536,,,WOS:000549769000002,,
J,"Shao, Zhenfeng; Zhou, Weixun; Deng, Xueqing; Zhang, Maoding; Cheng, Qimin",,,,,,,,Multilabel Remote Sensing Image Retrieval Based on Fully Convolutional Network,,,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,13,,,,318,328,,10.1109/JSTARS.2019.2961634,,,,2020,2020,"Conventional remote sensing image retrieval (RSIR) system usually performs single-label retrieval where each image is annotated by a single label representing the most significant semantic content of the image. In this scenario, however, the scene complexity of remote sensing images is ignored, where an image might have multiple classes (i.e., multiple labels), resulting in poor retrieval performance. We therefore propose a novel multilabel RSIR approach based on fully convolutional network (FCN). Specifically, FCN is first trained to predict segmentation map of each image in the considered image archive. We then obtain multilabel vector and extract region convolutional features of each image based on its segmentation map. The extracted region features are finally used to perform region-based multilabel retrieval. The experimental results show that our approach achieves state-of-the-art performance in contrast to handcrafted and convolutional neural network features.",,,,,,,,,48,0,0,0,4,0,48,,,1939-1404,2151-1535,,WOS:000526639900026,,
J,"Shi, Qian; Liu, Mengxi; Liu, Xiaoping; Liu, Penghua; Zhang, Pengyuan; Yang, Jinxing; Li, Xia",,,,"LI, Xia/A-1235-2013","LI, Xia/0000-0003-3050-8529; Liu, Mengxi/0000-0001-5237-4758; Liu, Penghua/0000-0002-8574-891X",,,Domain Adaption for Fine-Grained Urban Village Extraction From Satellite Images,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,17,8,,,1430,1434,,10.1109/LGRS.2019.2947473,,,,AUG 2020,2020,"Urban villages (UVs) are distinctive products formed in the process of rapid urbanization. The fine-grained mapping of UVs from satellite images has always been a considerable challenge because of the complex urban structures and the insufficiency of labeled samples. In this letter, we propose using the domain adaptation strategy to tackle the domain shift problem by employing adversarial learning to tune the semantic segmentation network so as to adaptively obtain similar outputs for input images from different domains. The proposed method was coupled with several segmentation networks, including U-Net, RefineNet, and DeepLab v3+, and the results show that domain adaptation can significantly improve the pixel-level mapping of UVs.",,,,,,,,,47,0,0,0,1,0,47,,,1545-598X,1558-0571,,WOS:000552271800029,,
J,"Kang, Hanwen; Chen, Chao",,,,,"kang, hanwen/0000-0001-5553-9239",,,"Fruit detection, segmentation and 3D visualisation of environments in apple orchards",,,,,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,171,,,,,,105302,10.1016/j.compag.2020.105302,,,,APR 2020,2020,"Development of an accurate and reliable fruit detection system is a challenging task. There are many complex conditions in orchard environments, such as changing illumination, appearance variation, and occlusion. Robotic vision is required to understand the working environments from the sensory data and guide the robotic arm to detach the fruits. In our previous work, a deep neural network DaSNet-v1 was developed to perform detection and segmentation on fruits and branches in orchard environments. However, semantic segmentation returns the mask for each class instead of each object. Segmentation on each fruit is important as it can provide abundant information of each object, especially for those overlapped fruits. This work presents an improved deep neural network DaSNet-v2, which can perform detection and instance segmentation on fruits, and semantic segmentation on branches. DaSNet-v2 is tested and validated by experimental results obtained from field-testing in an apple orchard. From the experiment results, DaSNet-v2 with resnet-101 achieves 0.868, 0.88 and 0.873 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.794 on the accuracy of branches segmentation, respectively. DaSNet-v2 with light-weight backbone resnet-18 achieves 0.85, 0.87 and 0.866 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.775 on the accuracy of branches segmentation, respectively. The average running time and weight size of light-weight DaSNet-v2 are 55 ms and 8.1 M, respectively. Experimental results show DaSNet-v2 can robustly and efficiently perform the vision sensing for robotic harvesting in apple orchards.",,,,,,,,,42,2,2,0,19,0,46,,,0168-1699,1872-7107,,WOS:000525324500021,,
B,"Bowen Cheng; Collins, M.D.; Yukun Zhu; Ting Liu; Huang, T.S.; Adam, H.; Liang-Chieh Chen",,,,,,,,"Panoptic-DeepLab: a simple, strong, and fast baseline for bottom-up panoptic segmentation",,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12472,82,,10.1109/CVPR42600.2020.01249,,,,2020,2020,"In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast system for panoptic segmentation, aiming to establish a solid baseline for bottom-up methods that can achieve comparable performance of two-stage methods while yielding fast inference speed. In particular, Panoptic-DeepLab adopts the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. As a result, our single Panoptic-DeepLab simultaneously ranks first at all three Cityscapes benchmarks, setting the new state-of-art of 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set. Additionally, equipped with MobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025times2049 image (15.8 frames per second), while achieving a competitive performance on Cityscapes (54.1 PQ% on test set). On Mapillary Vistas test set, our ensemble of six models attains 42.7% PQ, outperforming the challenge winner in 2018 by a healthy margin of 1.5%. Finally, our Panoptic-DeepLab also performs on par with several top-down approaches on the challenging COCO dataset. For the first time, we demonstrate a bottom-up approach could deliver state-of-the-art results on panoptic segmentation.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",45,1,0,0,3,0,46,,,,,978-1-7281-7168-5,INSPEC:19874675,,
B,Changqian Yu; Jingbo Wang; Changxin Gao; Gang Yu; Chunhua Shen; Nong Sang,,,,,"Yu, Changqian/0000-0002-4488-4157",,,Context prior for scene segmentation,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12413,22,,10.1109/CVPR42600.2020.01243,,,,2020,2020,"Recent works have widely explored the contextual dependencies to achieve more accurate segmentation results. However, most approaches rarely distinguish different types of contextual dependencies, which may pollute the scene understanding. In this work, we directly supervise the feature aggregation to distinguish the intra-class and interclass context clearly. Specifically, we develop a Context Prior with the supervision of the Affinity Loss. Given an input image and corresponding ground truth, Affinity Loss constructs an ideal affinity map to supervise the learning of Context Prior. The learned Context Prior extracts the pixels belonging to the same category, while the reversed prior focuses on the pixels of different classes. Embedded into a conventional deep CNN, the proposed Context Prior Layer can selectively capture the intra-class and inter-class contextual dependencies, leading to robust feature representation. To validate the effectiveness, we design an effective Context Prior Network (CPNet). Extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against state-of-the-art semantic segmentation approaches. More specifically, our algorithm achieves 46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on Cityscapes. Code is available at https://git.io/ContextPrior.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",44,2,0,0,2,0,46,,,,,978-1-7281-7168-5,INSPEC:19874305,,
J,"Zhang, Qiang; Huang, Nianchang; Yao, Lin; Zhang, Dingwen; Shan, Caifeng; Han, Jungong",,,,"Shan, Caifeng/W-6178-2019","Shan, Caifeng/0000-0002-2131-1671; Nianchang, Huang/0000-0001-9530-3490",,,RGB-T Salient Object Detection via Fusing Multi-Level CNN Features,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,3321,3335,,10.1109/TIP.2019.2959253,,,,2020,2020,"RGB-induced salient object detection has recently witnessed substantial progress, which is attributed to the superior feature learning capability of deep convolutional neural networks (CNNs). However, such detections suffer from challenging scenarios characterized by cluttered backgrounds, low-light conditions and variations in illumination. Instead of improving RGB based saliency detection, this paper takes advantage of the complementary benefits of RGB and thermal infrared images. Specifically, we propose a novel end-to-end network for multi-modal salient object detection, which turns the challenge of RGB-T saliency detection to a CNN feature fusion problem. To this end, a backbone network (e.g., VGG-16) is first adopted to extract the coarse features from each RGB or thermal infrared image individually, and then several adjacent-depth feature combination (ADFC) modules are designed to extract multi-level refined features for each single-modal input image, considering that features captured at different depths differ in semantic information and visual details. Subsequently, a multi-branch group fusion (MGF) module is employed to capture the cross-modal features by fusing those features from ADFC modules for a RGB-T image pair at each level. Finally, a joint attention guided bi-directional message passing (JABMP) module undertakes the task of saliency prediction via integrating the multi-level fused features from MGF modules. Experimental results on several public RGB-T salient object detection datasets demonstrate the superiorities of our proposed algorithm over the state-of-the-art approaches, especially under challenging conditions, such as poor illumination, complex background and low contrast.",,,,,,,,,44,4,0,0,0,0,46,,,1057-7149,1941-0042,,WOS:000510750900034,31869791,
J,"Fu, Jun; Liu, Jing; Jiang, Jie; Li, Yong; Bao, Yongjun; Lu, Hanqing",,,,,"Jiang, Jie/0000-0002-4173-4911; Fu, Jun/0000-0001-5734-7134",,,Scene Segmentation With Dual Relation-Aware Attention Network,,,,,,,,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,,,32,6,,,2547,2560,,10.1109/TNNLS.2020.3006524,,,,JUN 2021,2021,"In this article, we propose a Dual Relation-aware Attention Network (DRANet) to handle the task of scene segmentation. How to efficiently exploit context is essential for pixel-level recognition. To address the issue, we adaptively capture contextual information based on the relation-aware attention mechanism. Especially, we append two types of attention modules on the top of the dilated fully convolutional network (FCN), which model the contextual dependencies in spatial and channel dimensions, respectively. In the attention modules, we adopt a self-attention mechanism to model semantic associations between any two pixels or channels. Each pixel or channel can adaptively aggregate context from all pixels or channels according to their correlations. To reduce the high cost of computation and memory caused by the abovementioned pairwise association computation, we further design two types of compact attention modules. In the compact attention modules, each pixel or channel is built into association only with a few numbers of gathering centers and obtains corresponding context aggregation over these gathering centers. Meanwhile, we add a cross-level gating decoder to selectively enhance spatial details that boost the performance of the network. We conduct extensive experiments to validate the effectiveness of our network and achieve new state-of-the-art segmentation performance on four challenging scene segmentation data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff data sets. In particular, a Mean IoU score of 82.9% on the Cityscapes test set is achieved without using extra coarse annotated data.",,,,,,,,,43,2,0,0,1,0,45,,,2162-237X,2162-2388,,WOS:000658349600019,32745005,
J,"Oulefki, Adel; Agaian, Sos; Trongtirakul, Thaweesak; Laouar, Azzeddine Kassah",,,,"oulefki, adel/M-5513-2019; Oulefki, Adel/AAE-9017-2022; Trongtirakul, Thaweesak/ABG-5084-2021","oulefki, adel/0000-0003-2930-9215; Trongtirakul, Thaweesak/0000-0002-7982-5202",,,Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images,,,,,,,,PATTERN RECOGNITION,,,,114,,,,,,107747,10.1016/j.patcog.2020.107747,,MAR 2021,,JUN 2021,2021,"History shows that the infectious disease (COVID-19) can stun the world quickly, causing massive losses to health, resulting in a profound impact on the lives of billions of people, from both a safety and an economic perspective, for controlling the COVID-19 pandemic. The best strategy is to provide early intervention to stop the spread of the disease. In general, Computer Tomography (CT) is used to detect tumors in pneumonia, lungs, tuberculosis, emphysema, or other pleura (the membrane covering the lungs) diseases. Disadvantages of CT imaging system are: inferior soft tissue contrast compared to MRI as it is X-ray-based Radiation exposure. Lung CT image segmentation is a necessary initial step for lung image analysis. The main challenges of segmentation algorithms exaggerated due to intensity in-homogeneity, presence of artifacts, and closeness in the gray level of different soft tissue. The goal of this paper is to design and evaluate an automatic tool for automatic COVID-19 Lung Infection segmentation and measurement using chest CT images. The extensive computer simulations show better efficiency and flexibility of this end to-end learning approach on CT image segmentation with image enhancement comparing to the state of the art segmentation approaches, namely GraphCut, Medical Image Segmentation (MIS), and Watershed. Experiments performed on COVID-CT-Dataset containing (275) CT scans that are positive for COVID-19 and new data acquired from the EL-BAYANE center for Radiology and Medical Imaging. The means of statistical measures obtained using the accuracy, sensitivity, F-measure, precision, MCC, Dice, Jacquard, and specificity are 0.98, 0.73, 0.71, 0.73, 0.71, 0.71, 0.57, 0.99 respectively; which is better than methods mentioned above. The achieved results prove that the proposed approach is more robust, accurate, and straightforward.(c) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,43,2,0,0,10,0,45,,,0031-3203,1873-5142,,WOS:000632383600006,33162612,
J,"Schiefer, Felix; Kattenborn, Teja; Frick, Annett; Frey, Julian; Schall, Peter; Koch, Barbara; Schmidtlein, Sebastian",,,,"Kattenborn, Teja/I-7457-2019; Schiefer, Felix/ABD-5796-2020; Frey, Julian/AAP-1360-2020","Kattenborn, Teja/0000-0001-7381-3828; Schiefer, Felix/0000-0002-2258-2246; Frey, Julian/0000-0001-7895-702X; Frick, Annett/0000-0002-0213-8981",,,Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,170,,,,205,215,,10.1016/j.isprsjprs.2020.10.015,,,,DEC 2020,2020,"The use of unmanned aerial vehicles (UAVs) in vegetation remote sensing allows a time-flexible and cost-effective acquisition of very high-resolution imagery. Still, current methods for the mapping of forest tree species do not exploit the respective, rich spatial information. Here, we assessed the potential of convolutional neural networks (CNNs) and very high-resolution RGB imagery from UAVs for the mapping of tree species in temperate forests. We used multicopter UAVs to obtain very high-resolution (<2 cm) RGB imagery over 51 ha of temperate forests in the Southern Black Forest region, and the Hainich National Park in Germany. To fully harness the end-to-end learning capabilities of CNNs, we used a semantic segmentation approach (U-net) that concurrently segments and classifies tree species from imagery. With a diverse dataset in terms of study areas, site conditions, illumination properties, and phenology, we accurately mapped nine tree species, three genus-level classes, deadwood, and forest floor (mean F1-score 0.73). A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy (mean F1-score of 0.26 at 32 cm resolution). Our results highlight the key role that UAVs can play in the mapping of forest tree species, given that air- and spaceborne remote sensing currently does not provide comparable spatial resolutions. The end-to-end learning capability of CNNs makes extensive preprocessing partly obsolete. The use of large and diverse datasets facilitate a high degree of generalization of the CNN, thus fostering transferability. The synergy of high-resolution UAV imagery and CNN provide a fast and flexible yet accurate means of mapping forest tree species.",,,,,,,,,45,0,0,0,6,0,45,,,0924-2716,1872-8235,,WOS:000592242600015,,
J,"Mou, Lichao; Hua, Yuansheng; Zhu, Xiao Xiang",,,,"Zhu, Xiao Xiang/ABE-7138-2020","Zhu, Xiao Xiang/0000-0001-5530-3613; Hua, Yuansheng/0000-0001-9238-2920",,,Relation Matters: Relational Context-Aware Fully Convolutional Network for Semantic Segmentation of High-Resolution Aerial Images,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,11,,,7557,7569,,10.1109/TGRS.2020.2979552,,,,NOV 2020,2020,"Most current semantic segmentation approaches fall back on deep convolutional neural networks (CNNs). However, their use of convolution operations with local receptive fields causes failures in modeling contextual spatial relations. Prior works have sought to address this issue by using graphical models or spatial propagation modules in networks. But such models often fail to capture long-range spatial relationships between entities, which leads to spatially fragmented predictions. Moreover, recent works have demonstrated that channel-wise information also acts a pivotal part in CNNs. In this article, we introduce two simple yet effective network units, the spatial relation module, and the channel relation module to learn and reason about global relationships between any two spatial positions or feature maps, and then produce Relation-Augmented (RA) feature representations. The spatial and channel relation modules are general and extensible, and can be used in a plug-andplay fashion with the existing fully convolutional network (FCN) framework. We evaluate relation module-equipped networks on semantic segmentation tasks using two aerial image data sets, namely International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets, which fundamentally depend on long-range spatial relational reasoning. The networks achieve very competitive results, a mean F1 score of 88.54% on the Vaihingen data set and a mean F1 score of 88.01% on the Potsdam data set, bringing significant improvements over baselines.",,,,,,,,,44,2,0,0,0,0,45,,,0196-2892,1558-0644,,WOS:000583850500004,,
J,"Yang, Ming-Der; Tseng, Hsin-Hung; Hsu, Yu-Chun; Tsai, Hui Ping",,,,"Tsai, HuiPing/AAC-2802-2022","Yang, Ming-Der/0000-0003-2904-5838; TSAI, HUI PING/0000-0002-4915-1075; Tseng, Hsin-Hung/0000-0002-3019-4307; Hsu, Yu-Chun/0000-0002-6616-6906",,,Semantic Segmentation Using Deep Learning with Vegetation Indices for Rice Lodging Identification in Multi-date UAV Visible Images,,,,,,,,REMOTE SENSING,,,,12,4,,,,,633,10.3390/rs12040633,,,,FEB 2020,2020,"A rapid and precise large-scale agricultural disaster survey is a basis for agricultural disaster relief and insurance but is labor-intensive and time-consuming. This study applies Unmanned Aerial Vehicles (UAVs) images through deep-learning image processing to estimate the rice lodging in paddies over a large area. This study establishes an image semantic segmentation model employing two neural network architectures, FCN-AlexNet, and SegNet, whose effects are explored in the interpretation of various object sizes and computation efficiency. Commercial UAVs imaging rice paddies in high-resolution visible images are used to calculate three vegetation indicators to improve the applicability of visible images. The proposed model was trained and tested on a set of UAV images in 2017 and was validated on a set of UAV images in 2019. For the identification of rice lodging on the 2017 UAV images, the F1-score reaches 0.80 and 0.79 for FCN-AlexNet and SegNet, respectively. The F1-score of FCN-AlexNet using RGB + ExGR combination also reaches 0.78 in the 2019 images for validation. The proposed model adopting semantic segmentation networks is proven to have better efficiency, approximately 10 to 15 times faster, and a lower misinterpretation rate than that of the maximum likelihood method.",,,,,,,,,43,2,0,0,4,0,45,,,,2072-4292,,WOS:000519564600044,,
C,"Jha, Debesh; Riegler, Michael A.; Johansen, Dag; Halvorsen, Pal; Johansen, Havard D.",,,,,"Jha, Debesh/0000-0002-8078-6730","DeHerrera, AGS; Gonzalez, AR; Santosh, KC; Temesgen, Z; Kane, B; Soda, P",,DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation,,,,,,,,2020 IEEE 33RD INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS(CBMS 2020),,IEEE International Symposium on Computer-Based Medical Systems,,,,,,558,564,,10.1109/CBMS49503.2020.00111,,,,2020,2020,"Semantic image segmentation is the process of labeling each pixel of an image with its corresponding class. An encoder-decoder based approach, like U-Net and its variants, is a popular strategy for solving medical image segmentation tasks. To improve the performance of U-Net on various segmentation tasks, we propose a novel architecture called DoubleU-Net, which is a combination of two U-Net architectures stacked on top of each other. The first U-Net uses a pre-trained VGG-19 as the encoder, which has already learned features from ImageNet and can be transferred to another task easily. To capture more semantic information efficiently, we added another U-Net at the bottom. We also adopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information within the network. We have evaluated DoubleU-Net using four medical segmentation datasets, covering various imaging modalities such as colonoscopy, dermoscopy, and microscopy. Experiments on the 2015 MICCAI sub-challenge on automatic polyp detection dataset, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the Lesion boundary segmentation datasets demonstrate that the DoubleU-Net outperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more accurate segmentation masks, especially in the case of the CVC-ClinicDB and 2015 MICCAI sub-challenge on automatic polyp detection dataset, which have challenging images such as smaller and fiat polyps. These results show the improvement over the existing U-Net model. The encouraging results, produced on various medical image segmentation datasets, show that DoubleU-Net can be used as a strong baseline for both medical image segmentation and cross-dataset evaluation testing to measure the generalizability of Deep Learning (DL) models.",,,,,33rd IEEE International Symposium on Computer-Based Medical Systems (CBMS)33rd IEEE International Symposium on Computer-Based Medical Systems (CBMS),"JUL 28-30, 2020JUL 28-30, 2020",IEEE; IEEE Comp SocIEEE; IEEE Comp Soc,ELECTR NETWORKELECTR NETWORK,40,5,0,0,4,0,45,,,2372-9198,,978-1-7281-9429-5,WOS:000786468800103,,
J,"Zhou, Sihang; Nie, Dong; Adeli, Ehsan; Yin, Jianping; Lian, Jun; Shen, Dinggang",,,,"Shen, Dinggang/ABF-6812-2020","Shen, Dinggang/0000-0002-7934-5698; Yin, Jianping/0000-0002-5474-4764; Adeli, Ehsan/0000-0002-0579-7763; Zhou, Sihang/0000-0003-1491-4594",,,High-Resolution Encoder-Decoder Networks for Low-Contrast Medical Image Segmentation,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,461,475,,10.1109/TIP.2019.2919937,,,,2020,2020,"Automatic image segmentation is an essential step for many medical image analysis applications, include computer-aided radiation therapy, disease diagnosis, and treatment effect evaluation. One of the major challenges for this task is the blurry nature of medical images (e.g., CT, MR, and microscopic images), which can often result in low-contrast and vanishing boundaries. With the recent advances in convolutional neural networks, vast improvements have been made for image segmentation, mainly based on the skip-connection-linked encoder-decoder deep architectures. However, in many applications (with adjacent targets in blurry images), these models often fail to accurately locate complex boundaries and properly segment tiny isolated parts. In this paper, we aim to provide a method for blurry medical image segmentation and argue that skip connections are not enough to help accurately locate indistinct boundaries. Accordingly, we propose a novel high-resolution multi-scale encoder-decoder network (HMEDN), in which multi-scale dense connections are introduced for the encoder-decoder structure to finely exploit comprehensive semantic information. Besides skip connections, extra deeply supervised high-resolution pathways (comprised of densely connected dilated convolutions) are integrated to collect high-resolution semantic information for accurate boundary localization. These pathways are paired with a difficulty-guided cross-entropy loss function and a contour regression task to enhance the quality of boundary detection. The extensive experiments on a pelvic CT image dataset, a multi-modal brain tumor dataset, and a cell segmentation dataset show the effectiveness of our method for 2D/3D semantic segmentation and 2D instance segmentation, respectively. Our experimental results also show that besides increasing the network complexity, raising the resolution of semantic feature maps can largely affect the overall model performance. For different tasks, finding a balance between these two factors can further improve the performance of the corresponding network.",,,,,,,,,44,2,0,0,6,0,45,,,1057-7149,1941-0042,,WOS:000497434700015,31226074,
J,"Sultana, Farhana; Sufian, Abu; Dutta, Paramartha",,,,"Sufian, Abu/AAB-8032-2022; Dutta, Paramartha/AAD-1635-2021; Dutta, Paramartha/AAP-9966-2020","Sufian, Abu/0000-0003-2035-2938; Dutta, Paramartha/0000-0003-3946-2440; ",,,Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey,,,,,,,,KNOWLEDGE-BASED SYSTEMS,,,,201,,,,,,106062,10.1016/j.knosys.2020.106062,,,,AUG 9 2020,2020,"From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural networks (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. We have also drawn a comparison among the performance of those models on different datasets. Lastly, we have given a glimpse of some state-of-the-art panoptic segmentation models. (C) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,43,1,0,0,4,0,44,,,0950-7051,1872-7409,,WOS:000540693100006,,
J,"Huang, Zilong; Wang, Xinggang; Wei, Yunchao; Huang, Lichao; Shi, Humphrey; Liu, Wenyu; Huang, Thomas S",,,,"Huang, Zilong/AAW-6071-2021","Wang, Xinggang/0000-0001-6732-7823",,,CCNet: Criss-Cross Attention for Semantic Segmentation.,,,,,,,,IEEE transactions on pattern analysis and machine intelligence,,,,PP,,,,,,,10.1109/TPAMI.2020.3007032,,,,2020-Jul-03,2020,"Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85\% of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at https://github.com/speedinghzl/CCNet.",,,,,,,,,44,1,0,0,2,0,44,,,,1939-3539,,MEDLINE:32750802,32750802,
J,"Gao, Kai; Su, Jianpo; Jiang, Zhongbiao; Zeng, Ling-Li; Feng, Zhichao; Shen, Hui; Rong, Pengfei; Xu, Xin; Qin, Jian; Yang, Yuexiang; Wang, Wei; Hu, Dewen",,,,"Hu, Dewen/AAN-8511-2020",,,,Dual-branch combination network (DCN): Towards accurate diagnosis and lesion segmentation of COVID-19 using CT images,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,67,,,,,,101836,10.1016/j.media.2020.101836,,,,JAN 2021,2021,"The recent global outbreak and spread of coronavirus disease (COVID-19) makes it an imperative to develop accurate and efficient diagnostic tools for the disease as medical resources are getting increasingly constrained. Artificial intelligence (AI)-aided tools have exhibited desirable potential; for example, chest computed tomography (CT) has been demonstrated to play a major role in the diagnosis and evaluation of COVID-19. However, developing a CT-based AI diagnostic system for the disease detection has faced considerable challenges, which is mainly due to the lack of adequate manually-delineated samples for training, as well as the requirement of sufficient sensitivity to subtle lesions in the early infection stages. In this study, we developed a dual-branch combination network (DCN) for COVID-19 diagnosis that can simultaneously achieve individual-level classification and lesion segmentation. To focus the classification branch more intensively on the lesion areas, a novel lesion attention module was developed to integrate the intermediate segmentation results. Furthermore, to manage the potential influence of different imaging parameters from individual facilities, a slice probability mapping method was proposed to learn the transformation from slice-level to individual-level classification. We conducted experiments on a large dataset of 1202 subjects from ten institutes in China. The results demonstrated that 1) the proposed DCN attained a classification accuracy of 96.74% on the internal dataset and 92.87% on the external validation dataset, thereby outperforming other models; 2) DCN obtained comparable performance with fewer samples and exhibited higher sensitivity, especially in subtle lesion detection; and 3) DCN provided good interpretability on the loci of infection compared to other deep models due to its classification guided by high-level semantic information. An online CT-based diagnostic platform for COVID-19 derived from our proposed framework is now available. (C) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,42,1,0,0,2,0,43,,,1361-8415,1361-8423,,WOS:000598894800011,33129141,
J,"Waldner, Francois; Diakogiannis, Foivos, I",,,,"Waldner, Francois/AAW-1934-2020; Diakogiannis, Foivos/L-4957-2019","Waldner, Francois/0000-0002-5599-7456; Diakogiannis, Foivos/0000-0002-8788-8174",,,Deep learning on edge: Extracting field boundaries from satellite images with a convolutional neural network,,,,,,,,REMOTE SENSING OF ENVIRONMENT,,,,245,,,,,,111741,10.1016/j.rse.2020.111741,,,,AUG 2020,2020,"Applications of digital agricultural services often require either farmers or their advisers to provide digital records of their field boundaries. Automatic extraction of field boundaries from satellite imagery would reduce the reliance on manual input of these records, which is time consuming, and would underpin the provision of remote products and services. The lack of current field boundary data sets seems to indicate low uptake of existing methods, presumably because of expensive image preprocessing requirements and local, often arbitrary, tuning. In this paper, we propose a data-driven, robust and general method to facilitate field boundary extraction from satellite images. We formulated this task as a multi-task semantic segmentation problem. We used ResUNet-a, a deep convolutional neural network with a fully connected UNet backbone that features dilated convolutions and conditioned inference to identify: 1) the extent of fields; 2) the field boundaries; and 3) the distance to the closest boundary. By asking the algorithm to reconstruct three correlated outputs, the model's performance and its ability to generalise greatly improve. Segmentation of individual fields was then achieved by post-processing the three model outputs, e.g., via thresholding or watershed segmentation. Using a single monthly composite image from Sentinel-2 as input, our model was highly accurate in mapping field extent, field boundaries and, consequently, individual fields. Replacing the monthly composite with a single-date image close to the compositing period marginally decreased accuracy. We then showed in a series of experiments that, without recalibration, the same model generalised well across resolutions (10 m to 30 m), sensors (Sentinel-2 to Landsat-8), space and time. Building consensus by averaging model predictions from at least four images acquired across the season is paramount to reducing the temporal variations of accuracy. Our convolutional neural network is capable of learning complex hierarchical contextual features from the image to accurately detect field boundaries and discard irrelevant boundaries, thereby outperforming conventional edge filters. By minimising over-fitting and image preprocessing requirements, and by replacing local arbitrary decisions by data-driven ones, our approach is expected to facilitate the extraction of individual crop fields at scale.",,,,,,,,,42,1,0,0,4,0,43,,,0034-4257,1879-0704,,WOS:000537687300001,,
J,"Guo, Meng-Hao; Cai, Jun-Xiong; Liu, Zheng-Ning; Mu, Tai-Jiang; Martin, Ralph R.; Hu, Shi-Min",,,,,"Liu, Zhengning/0000-0001-6643-6016",,,PCT: Point cloud transformer,,,,,,,,COMPUTATIONAL VISUAL MEDIA,,,,7,2,,,187,199,,10.1007/s41095-021-0229-5,,,,JUN 2021,2021,"The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer (PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation, semantic segmentation, and normal estimation tasks.",,,,,,,,,42,2,0,0,1,0,42,,,2096-0433,2096-0662,,WOS:000648692900003,,
J,"Zhang, Xiaolin; Wei, Yunchao; Yang, Yi; Huang, Thomas S.",,,,"Yang, Yi/B-9273-2017","Yang, Yi/0000-0002-0512-880X",,,SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation,,,,,,,,IEEE TRANSACTIONS ON CYBERNETICS,,,,50,9,,,3855,3865,,10.1109/TCYB.2020.2992433,,,,SEPT 2020,2020,"One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this article, we propose a simple yet effective similarity guidance network to tackle the one-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we first adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adopted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework that can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SG-One achieves the mIoU score of 46.3%, surpassing the baseline methods.",,,,,,,,,39,4,0,0,2,0,42,,,2168-2267,2168-2275,,WOS:000562306000003,32497014,
J,"Pierdicca, Roberto; Paolanti, Marina; Matrone, Francesca; Martini, Massimo; Morbidoni, Christian; Malinverni, Eva Savina; Frontoni, Emanuele; Lingua, Andrea Maria",,,,"Matrone, Francesca/AAX-2965-2020; Frontoni, Emanuele/D-9838-2013; Paolanti, Marina/AAA-1239-2021; Pierdicca, Roberto/B-3193-2016","Frontoni, Emanuele/0000-0002-8893-9244; Paolanti, Marina/0000-0002-5523-7174; Pierdicca, Roberto/0000-0002-9160-834X; Morbidoni, Christian/0000-0003-0244-9322; MALINVERNI, Eva Savina/0000-0001-6582-2943; MATRONE, FRANCESCA/0000-0002-9160-1674; lingua, andrea/0000-0002-5930-2711; Martini, Massimo/0000-0003-1714-4310",,,Point Cloud Semantic Segmentation Using a Deep Learning Framework for Cultural Heritage,,,,,,,,REMOTE SENSING,,,,12,6,,,,,1005,10.3390/rs12061005,,,,MAR 2020,2020,"In the Digital Cultural Heritage (DCH) domain, the semantic segmentation of 3D Point Clouds with Deep Learning (DL) techniques can help to recognize historical architectural elements, at an adequate level of detail, and thus speed up the process of modeling of historical buildings for developing BIM models from survey data, referred to as HBIM (Historical Building Information Modeling). In this paper, we propose a DL framework for Point Cloud segmentation, which employs an improved DGCNN (Dynamic Graph Convolutional Neural Network) by adding meaningful features such as normal and colour. The approach has been applied to a newly collected DCH Dataset which is publicy available: ArCH (Architectural Cultural Heritage) Dataset. This dataset comprises 11 labeled points clouds, derived from the union of several single scans or from the integration of the latter with photogrammetric surveys. The involved scenes are both indoor and outdoor, with churches, chapels, cloisters, porticoes and loggias covered by a variety of vaults and beared by many different types of columns. They belong to different historical periods and different styles, in order to make the dataset the least possible uniform and homogeneous (in the repetition of the architectural elements) and the results as general as possible. The experiments yield high accuracy, demonstrating the effectiveness and suitability of the proposed approach.",,,,,,,,,42,0,0,0,0,0,42,,,,2072-4292,,WOS:000526820600107,,
J,"Yu, Bo; Chen, Fang; Xu, Chong",,,,"Chen, Fang/AAU-7638-2020",,,,Landslide detection based on contour-based deep learning framework in case of national scale of Nepal in 2015,,,,,,,,COMPUTERS & GEOSCIENCES,,,,135,,,,,,104388,10.1016/j.cageo.2019.104388,,,,FEB 2020,2020,"The deadly threat that landslide has brought about is drawing more and more attention to analyze the mechanisms of landslides and the relationship between landslides and climate change. Due to the limited record of historical landslides in developing countries, relevant research is mostly conducted in developed countries. Owing to the publicly available global long time-series Landsat images, such unbalance can be avoided by proposing a practical landslide detection model, especially in terms of national scale. This paper takes the advantage of google earth engine platform to synthesize the annual Landsat images covering the national scale of Nepal into one image and builds an end-to-end contour-based landslide detection deep learning framework. The framework consists of two parts, one is potential landslide detection using vegetation index and degradation of DEM, the other is exact landslide detection using semantic segmentation deep learning model based on the contour regions extracted from the detected potential landslide. The proposed method is applied to detect landslides of Nepal in the year of 2015 and achieves a satisfactory performance with 65% recall and 55.35% precision. The performance is 44% higher accurate than similarly published works, validating its promising applicability in practical landslide detection for national cases.",,,,,,,,,41,3,0,0,1,0,42,,,0098-3004,1873-7803,,WOS:000514248800009,,
J,"Zhang, Jing; Lin, Shaofu; Ding, Lei; Bruzzone, Lorenzo",,,,"Ding, Lei/AAK-2573-2021; Bruzzone, Lorenzo/A-2076-2012","Ding, Lei/0000-0003-0653-8373; Bruzzone, Lorenzo/0000-0002-6036-459X",,,Multi-Scale Context Aggregation for Semantic Segmentation of Remote Sensing Images,,,,,,,,REMOTE SENSING,,,,12,4,,,,,701,10.3390/rs12040701,,,,FEB 2020,2020,"The semantic segmentation of remote sensing images (RSIs) is important in a variety of applications. Conventional encoder-decoder-based convolutional neural networks (CNNs) use cascade pooling operations to aggregate the semantic information, which results in a loss of localization accuracy and in the preservation of spatial details. To overcome these limitations, we introduce the use of the high-resolution network (HRNet) to produce high-resolution features without the decoding stage. Moreover, we enhance the low-to-high features extracted from different branches separately to strengthen the embedding of scale-related contextual information. The low-resolution features contain more semantic information and have a small spatial size; thus, they are utilized to model the long-term spatial correlations. The high-resolution branches are enhanced by introducing an adaptive spatial pooling (ASP) module to aggregate more local contexts. By combining these context aggregation designs across different levels, the resulting architecture is capable of exploiting spatial context at both global and local levels. The experimental results obtained on two RSI datasets show that our approach significantly improves the accuracy with respect to the commonly used CNNs and achieves state-of-the-art performance.",,,,,,,,,41,1,0,0,3,0,42,,,,2072-4292,,WOS:000519564600112,,
J,"Jiang, Du; Li, Gongfa; Tan, Chong; Huang, Li; Sun, Ying; Kong, Jianyi",,,,,,,,Semantic segmentation for multiscale target based on object recognition using the improved Faster-RCNN model,,,,,,,,FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE,,,,123,,,,94,104,,10.1016/j.future.2021.04.019,,MAY 2021,,OCT 2021,2021,"Image semantic segmentation has received great attention in computer vision, whose aim is to segment different objects and provide them different semantic category labels so that the computer can fully obtain the semantic information of the scene. However, the current research mainly focuses on color image data as training, for outdoor scenes and single task semantic segmentation. This paper carries out multi-task semantic segmentation model in the complex indoor environment on joint target detection using RGB-D image information based on the improved Faster-RCNN algorithm, which can simultaneously realize the indoor scene semantic segmentation, target classification and detection multiple visual tasks. In which, in view of the influence of uneven lighting in the environment, the method of fusion of RGB images and depth images is improved. While enhancing the fusion image feature information, it also improves the efficiency of model training. Simultaneously, in order to meet the needs for operating on multi-scale target objects, the non-maximum value suppression algorithm is improved to improve the model's performance. So as to realize the output of the model's multi-task information, the loss function has also been redesigned and optimized. The indoor scene semantic segmentation model constructed in this paper not only has good performance and high efficiency, but also can segment the contours of different scale objects clearly and adapt to the indoor uneven lighting environment. (C) 2021 Published by Elsevier B.V.",,,,,,,,,41,0,0,0,19,0,41,,,0167-739X,1872-7115,,WOS:000661867900009,,
J,"Ji, Yuzhu; Zhang, Haijun; Zhang, Zhao; Liu, Ming",,,,,,,,CNN-based encoder-decoder networks for salient object detection: A comprehensive review and recent advances,,,,,,,,INFORMATION SCIENCES,,,,546,,,,835,857,,10.1016/j.ins.2020.09.003,,,,FEB 6 2021,2021,"Convolutional neural network (CNN)-based encoder-decoder models have profoundly inspired recent works in the field of salient object detection (SOD). With the rapid development of encoder-decoder models with respect to most pixel-level dense prediction tasks, an empirical study still does not exist that evaluates performance by applying a large body of encoder-decoder models on SOD tasks. In this paper, instead of limiting our survey to SOD methods, a broader view is further presented from the perspective of fundamental architectures of key modules and structures in CNN-based encoder-decoder models for pixel-level dense prediction tasks. Moreover, we focus on performing SOD by leveraging deep encoder-decoder models, and present an extensive empirical study on baseline encoder-decoder models in terms of different encoder backbones, loss functions, training batch sizes, and attention structures. Moreover, state-of-the-art encoder-decoder models adopted from semantic segmentation and deep CNN-based SOD models are also investigated. New baseline models that can outperform state-of-the-art performance were discovered. In addition, these newly discovered baseline models were further evaluated on three video-based SOD benchmark datasets. Experimental results demonstrate the effectiveness of these baseline models on both imageand video-based SOD tasks. This empirical study is concluded by a comprehensive summary which provides suggestions on future perspectives. (c) 2020 Elsevier Inc. All rights reserved.",,,,,,,,,39,2,0,0,1,0,41,,,0020-0255,1872-6291,,WOS:000596062600005,,
J,"Yuan, Xiaohui; Shi, Jianfang; Gu, Lichuan",,,,,"Yuan, Xiaohui/0000-0001-6897-4563",,,A review of deep learning methods for semantic segmentation of remote sensing imagery,,,,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,169,,,,,,114417,10.1016/j.eswa.2020.114417,,,,MAY 1 2021,2021,"Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.",,,,,,,,,39,1,0,0,2,0,40,,,0957-4174,1873-6793,,WOS:000623681800004,,
J,"Li, Yansheng; Shi, Te; Zhang, Yongjun; Chen, Wei; Wang, Zhibin; Li, Hao",,,,"Li, Yansheng/AAU-6392-2021","Li, Yansheng/0000-0001-8203-1246; Shi, Te/0000-0002-0874-0471",,,Learning deep semantic segmentation network under multiple weakly-supervised constraints for cross-domain remote sensing image semantic segmentation,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,175,,,,20,33,,10.1016/j.isprsjprs.2021.02.009,,MAR 2021,,MAY 2021,2021,"Due to its wide applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Benefiting from its hierarchical abstract ability, the deep semantic segmentation network (DSSN) has achieved tremendous success on RS image semantic segmentation and has gradually become the mainstream technology. However, the superior performance of DSSN highly depends on two conditions: (I) massive quantities of labeled training data exist; (II) the testing data seriously resemble the training data. In actual RS applications, it is difficult to fully meet these conditions due to the RS sensor variation and the distinct landscape variation in different geographic locations. To make DSSN fit the actual RS scenario, this paper exploits the cross-domain RS image semantic segmentation task, which means that DSSN is trained on one labeled dataset (i.e., the source domain) but is tested on another varied dataset (i.e., the target domain). In this setting, the performance of DSSN is inevitably very limited due to the data shift between the source and target domains. To reduce the disadvantageous influence of data shift, this paper proposes a novel objective function with multiple weakly-supervised constraints to learn DSSN for cross-domain RS image semantic segmentation. Through carefully examining the characteristics of cross-domain RS image semantic segmentation, multiple weakly-supervised constraints include the weakly-supervised transfer invariant constraint (WTIC), weakly-supervised pseudo-label constraint (WPLC) and weakly-supervised rotation consistency constraint (WRCC). Specifically, DualGAN is recommended to conduct unsupervised style transfer between the source and target domains to carry out WTIC. To make full use of the merits of multiple constraints, this paper presents a dynamic optimization strategy that dynamically adjusts the constraint weights of the objective function during the training process. With full consideration of the characteristics of the cross-domain RS image semantic segmentation task, this paper gives two cross-domain RS image semantic segmentation settings: (I) variation in geographic location and (II) variation in both geographic location and imaging mode. Extensive experiments demonstrate that our proposed method remarkably outperforms the state-of-the-art methods under both of these settings. The collected datasets and evaluation benchmarks have been made publicly available online (htt ps://github.com/te-shi/MUCSS).",,,,,,,,,40,0,0,0,2,0,40,,,0924-2716,1872-8235,,WOS:000644695700002,,
J,"Ding, Lei; Zhang, Jing; Bruzzone, Lorenzo",,,,"Ding, Lei/AAK-2573-2021; Bruzzone, Lorenzo/A-2076-2012","Ding, Lei/0000-0003-0653-8373; Bruzzone, Lorenzo/0000-0002-6036-459X",,,Semantic Segmentation of Large-Size VHR Remote Sensing Images Using a Two-Stage Multiscale Training Architecture,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,8,,,5367,5376,,10.1109/TGRS.2020.2964675,,,,AUG 2020,2020,"Very-high resolution (VHR) remote sensing images (RSIs) have significantly larger spatial size compared to typical natural images used in computer vision applications. Therefore, it is computationally unaffordable to train and test classifiers on these images at a full-size scale. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped image patches. Thus, they have the limitation of failing to incorporate enough context information. In order to better exploit the correlations between ground objects, we propose a deep architecture with a two-stage multiscale training strategy that is tailored to the semantic segmentation of large-size VHR RSIs. In the first stage of the training strategy, a semantic embedding network is designed to learn high-level features from downscaled images covering a large area. In the second training stage, a local feature extraction network is designed to introduce low-level information from cropped image patches. The resulting training strategy is able to fuse complementary information learned from multiple levels to make predictions. Experimental results on two data sets show that it outperforms local-patch-based training models in terms of both accuracy and stability.",,,,,,,,,40,0,0,0,1,0,40,,,0196-2892,1558-0644,,WOS:000552371900010,,
J,"Valada, Abhinav; Mohan, Rohit; Burgard, Wolfram",,,,"Valada, Abhinav/ABE-8411-2021","Valada, Abhinav/0000-0003-4710-3114; Burgard, Wolfram/0000-0002-5680-6500",,,Self-Supervised Model Adaptation for Multimodal Semantic Segmentation,,,,,,,,INTERNATIONAL JOURNAL OF COMPUTER VISION,,,,128,5,,,1239,1285,,10.1007/s11263-019-01188-y,,,,MAY 2020,2020,"Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the real-world. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on the relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modality-specific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a larger effective receptive field with more than 10x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$10\,\times $$\end{document} fewer parameters, complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance while simultaneously being efficient in terms of parameters and inference time as well as demonstrating substantial robustness in adverse perceptual conditions.",,,,,,,,,37,3,0,0,0,0,40,,,0920-5691,1573-1405,,WOS:000531431500011,,
J,"Chen, Steven W.; Nardari, Guilherme, V; Lee, Elijah S.; Qu, Chao; Liu, Xu; Romero, Roseli Ap Francelin; Kumar, Vijay",,,,"Kumar, Vijay/AAM-8545-2020","Nardari, Guilherme/0000-0003-2164-6196; Liu, Xu/0000-0003-3155-0171; Romero, Roseli A F/0000-0001-9366-2780; Kumar, Vijay/0000-0002-3902-9391",,,SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory,,,,,,,,IEEE ROBOTICS AND AUTOMATION LETTERS,,,,5,2,,,612,619,,10.1109/LRA.2019.2963823,,,,APR 2020,2020,"This letter describes an end-to-end pipeline for tree diameter estimation based on semantic segmentation and lidar odometry and mapping. Accurate mapping of this type of environment is challenging since the ground and the trees are surrounded by leaves, thorns and vines, and the sensor typically experiences extreme motion. We propose a semantic feature based pose optimization that simultaneously refines the tree models while estimating the robot pose. The pipeline utilizes a custom virtual reality tool for labeling 3D scans that is used to train a semantic segmentation network. The masked point cloud is used to compute a trellis graph that identifies individual instances and extracts relevant features that are used by the SLAM module. We show that traditional lidar and image based methods fail in the forest environment on both Unmanned Aerial Vehicle (UAV) and hand-carry systems, while our method is more robust, scalable, and automatically generates tree diameter estimations.",,,,,,,,,40,0,0,0,0,0,40,,,2377-3766,,,WOS:000509509300005,,
J,"Ding, Henghui; Jiang, Xudong; Shuai, Bing; Liu, Ai Qun; Wang, Gang",,,,"Liu, Aiqun/A-5066-2011; Ding, Henghui/C-7486-2019; Jiang, Xudong/B-1555-2008","Ding, Henghui/0000-0003-4868-6526; Jiang, Xudong/0000-0002-9104-2315",,,Semantic Segmentation With Context Encoding and Multi-Path Decoding,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,3520,3533,,10.1109/TIP.2019.2962685,,,,2020,2020,"Semantic image segmentation aims to classify every pixel of a scene image to one of many classes. It implicitly involves object recognition, localization, and boundary delineation. In this paper, we propose a segmentation network called CGBNet to enhance the segmentation performance by context encoding and multi-path decoding. We first propose a context encoding module that generates context-contrasted local feature to make use of the informative context and the discriminative local information. This context encoding module greatly improves the segmentation performance, especially for inconspicuous objects. Furthermore, we propose a scale-selection scheme to selectively fuse the segmentation results from different-scales of features at every spatial position. It adaptively selects appropriate score maps from rich scales of features. To improve the segmentation performance results at boundary, we further propose a boundary delineation module that encourages the location-specific very-low-level features near the boundaries to take part in the final prediction and suppresses them far from the boundaries. The proposed segmentation network achieves very competitive performance in terms of all three different evaluation metrics consistently on the six popular scene segmentation datasets, Pascal Context, SUN-RGBD, Sift Flow, COCO Stuff, ADE20K, and Cityscapes.",,,,,,,,,40,0,0,0,2,0,40,,,1057-7149,1941-0042,,WOS:000510750900048,31940532,
B,"Guolei Sun; Wenguan Wang; Jifeng Dai; Van Gool, L.",,,,,,"Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347),,,,,,,,347,65,,10.1007/978-3-030-58536-5_21,,,,2020,2020,"This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",38,2,0,0,1,0,40,,,,,978-3-030-58535-8,INSPEC:20087960,,
J,"Lobo Torres, Daliana; Queiroz Feitosa, Raul; Nigri Happ, Patrick; Elena Cue La Rosa, Laura; Marcato Junior, Jose; Martins, Jose; Ola Bressan, Patrik; Goncalves, Wesley Nunes; Liesenberg, Veraldo",,,,"Liesenberg, Veraldo/Y-2609-2019; Feitosa, Raul Queiroz/D-6570-2017; Marcato Junior, Jose/H-4406-2017","Liesenberg, Veraldo/0000-0003-0564-7818; Feitosa, Raul Queiroz/0000-0001-8344-5096; Correa Martins, Jose Augusto/0000-0003-0668-8224; Lobo Torres, Daliana/0000-0001-7916-9463; Cue La Rosa, Laura Elena/0000-0002-6284-9494; Nigri Happ, Patrick/0000-0003-3280-5471; Marcato Junior, Jose/0000-0002-9096-6866",,,Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery,,,,,,,,SENSORS,,,,20,2,,,,,563,10.3390/s20020563,,,,JAN 2020,2020,"This study proposes and evaluates five deep fully convolutional networks (FCNs) for the semantic segmentation of a single tree species: SegNet, U-Net, FC-DenseNet, and two DeepLabv3+ variants. The performance of the FCN designs is evaluated experimentally in terms of classification accuracy and computational load. We also verify the benefits of fully connected conditional random fields (CRFs) as a post-processing step to improve the segmentation maps. The analysis is conducted on a set of images captured by an RGB camera aboard a UAV flying over an urban area. The dataset also contains a mask that indicates the occurrence of an endangered species called Dipteryx alata Vogel, also known as cumbaru, taken as the species to be identified. The experimental analysis shows the effectiveness of each design and reports average overall accuracy ranging from 88.9% to 96.7%, an F1-score between 87.0% and 96.1%, and IoU from 77.1% to 92.5%. We also realize that CRF consistently improves the performance, but at a high computational cost.",,,,,,,,,39,1,0,0,6,0,40,,,,1424-8220,,WOS:000517790100237,31968589,
J,"Sun, Xian; Shi, Aijun; Huang, Hai; Mayer, Helmut",,,,,"Mayer, Helmut/0000-0002-9439-2695; Sun, Xian/0000-0002-0038-9816",,,BAS(4)Net: Boundary-Aware Semi-Supervised Semantic Segmentation Network for Very High Resolution Remote Sensing Images,,,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,13,,,,5398,5413,,10.1109/JSTARS.2020.3021098,,,,2020,2020,"Semantic segmentation is a fundamental task in remote sensing image understanding. Recently, Deep Convolutional Neural Networks (DCNNs) have considerably improved the performance of the semantic segmentation of natural scenes. However, it is still challenging for very high-resolution remote sensing images. Due to the large and complex scenes as well as the influence of illumination and imaging angle, it is particularly difficult for the existing methods to accurately obtain the category of pixels at object boundaries-the so-called boundary blur. We propose a framework called Boundary-Aware Semi-Supervised Semantic Segmentation Network (BAS(4)Net), which obtains more accurate segmentation results without additional annotation workload, especially at the object boundaries. The Channel-weighted Multi-scale Feature (CMF) module balances semantic and spatial information, and the Boundary Attention Module (BAM) weights the features with rich semantic boundary information to alleviate the boundary blur. Additionally, to decrease the amount of difficult and tedious manual labeling of remote sensing images, a discriminator network infers pseudolabels from unlabeled images to assist semisupervised learning, and further improves the performance of the segmentation network. To validate the effectiveness of the proposed framework, extensive experiments have been performed on both the ISPRS Vaihingen dataset, and the novel remote sensing dataset AIR-SEG with more categories, and complex boundaries. The results demonstrate a significant improvement of accuracy especially on boundaries and for small objects.",,,,,,,,,39,1,0,0,1,0,40,,,1939-1404,2151-1535,,WOS:000573802800002,,
J,"Ouahabi, Abdeldjalil; Taleb-Ahmed, Abdelmalik",,,,,,,,Deep learning for real-time semantic segmentation: Application in ultrasound imaging,,,,,,,,PATTERN RECOGNITION LETTERS,,,,144,,,,27,34,,10.1016/j.patrec.2021.01.010,,JAN 2021,,APR 2021,2021,"A real-time architecture of medical image semantic segmentation called Fully Convolution dense Dilated Network, is proposed to improve the segmentation efficiency while ensuring high accuracy. Considering low resolution and contrast, interferences of shadows, as well as differences in nodules' position and size, accurate ultrasound images' segmentation cannot be obtained easily. Therefore, a novel layer that integrates the advantages of dense connectivity, dilated convolutions and factorized filters, is proposed in an attempt to remain efficient while retaining remarkable accuracy. Dense connectivity combines low-level fine segmentation with high-level coarse segmentation to extract more features from ultrasound images. Dilated convolution can expand the receptive field of the filter, and the problem of differences in nodules' size and position can be solved with different sizes of filters. This study also introduces factorized filters into the network to further optimize the efficiency of the model. In addition, aiming at the class imbalance problem in medical image semantic segmentation, a loss function optimization method is proposed which further improves the accuracy of the network. A thorough set of experiments based on thyroid dataset show that the proposed model achieves state-of-the-art performance in terms of robustness and efficiency. (C) 2021 Elsevier B.V. All rights reserved.",,,,,,,,,39,0,0,0,3,0,39,,,0167-8655,1872-7344,,WOS:000624287500005,,
J,"Guo, Mingqiang; Liu, Heng; Xu, Yongyang; Huang, Ying",,,,,"Guo, Mingqiang/0000-0003-4097-4814; Xu, Yongyang/0000-0001-7421-4915",,,Building Extraction Based on U-Net with an Attention Block and Multiple Losses,,,,,,,,REMOTE SENSING,,,,12,9,,,,,1400,10.3390/rs12091400,,,,MAY 2020,2020,"Semantic segmentation of high-resolution remote sensing images plays an important role in applications for building extraction. However, the current algorithms have some semantic information extraction limitations, and these can lead to poor segmentation results. To extract buildings with high accuracy, we propose a multiloss neural network based on attention. The designed network, based on U-Net, can improve the sensitivity of the model by the attention block and suppress the background influence of irrelevant feature areas. To improve the ability of the model, a multiloss approach is proposed during training the network. The experimental results show that the proposed model offers great improvement over other state-of-the-art methods. For the public Inria Aerial Image Labeling dataset, the F1 score reached 76.96% and showed good performance on the Aerial Imagery for Roof Segmentation dataset.",,,,,,,,,36,3,0,0,0,0,39,,,,2072-4292,,WOS:000543394000047,,
J,"Shi, Yilei; Li, Qingyu; Zhu, Xiao Xiang",,,,"Zhu, Xiao Xiang/ABE-7138-2020","Zhu, Xiao Xiang/0000-0001-5530-3613",,,Building segmentation through a gated graph convolutional neural network with deep structured feature embedding,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,159,,,,184,197,,10.1016/j.isprsjprs.2019.11.004,,,,JAN 2020,2020,"Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks.",,,,,,,,,39,0,0,0,1,0,39,,,0924-2716,1872-8235,,WOS:000508739800015,31929682,
B,"Klingner, M.; Termohlen, J.-A.; Mikolajczyk, J.; Fingscheidt, T.",,,,,,"Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,Self-supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365),,,,,,,,582,600,,10.1007/978-3-030-58565-5_35,,,,2020,2020,"Self-supervised monocular depth estimation presents a powerful method to obtain 3D scene information from single camera images, which is trainable on arbitrary image sequences without requiring depth labels, e.g., from a LiDAR sensor. In this work we present a new self-supervised semantically-guided depth estimation (SGDepth) method to deal with moving dynamic-class (DC) objects, such as moving cars and pedestrians, which violate the static-world assumptions typically made during training of such models. Specifically, we propose (i) mutually beneficial cross-domain training of (supervised) semantic segmentation and self-supervised depth estimation with task-specific network heads, (ii) a semantic masking scheme providing guidance to prevent moving DC objects from contaminating the photometric loss, and (iii) a detection method for frames with non-moving DC objects, from which the depth of DC objects can be learned. We demonstrate the performance of our method on several benchmarks, in particular on the Eigen split, where we exceed all baselines without test-time refinement.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",38,0,0,0,0,0,38,,,,,978-3-030-58564-8,INSPEC:20107993,,
J,"Wan, Shaohua; Xu, Xiaolong; Wang, Tian; Gu, Zonghua",,,,"Xu, Xiaolong/U-2547-2019; Wan, Shaohua/B-9243-2014","Xu, Xiaolong/0000-0003-4879-9803; Wang, Tian/0000-0003-4819-621X; Wan, Shaohua/0000-0001-7013-9081; Gu, Zonghua/0000-0003-4228-2774",,,An Intelligent Video Analysis Method for Abnormal Event Detection in Intelligent Transportation Systems,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,22,7,,,4487,4495,,10.1109/TITS.2020.3017505,,,,JUL 2021,2021,"Intelligent transportation systems pervasively deploy thousands of video cameras. Analyzing live video streams from these cameras is of significant importance to public safety. As streaming video is increasing, it becomes infeasible to have human operators sitting in front of hundreds of screens to catch suspicious activities or detect objects of interests in real-time. Actually, with millions of traffic surveillance cameras installed, video retrieval is more vital than ever. To that end, this article proposes a long video event retrieval algorithm based on superframe segmentation. By detecting the motion amplitude of the long video, a large number of redundant frames can be effectively removed from the long video, thereby reducing the number of frames that need to be calculated subsequently. Then, by using a superframe segmentation algorithm based on feature fusion, the remaining long video is divided into several Segments of Interest (SOIs) which include the video events. Finally, the trained semantic model is used to match the answer generated by the text question, and the result with the highest matching value is considered as the video segment corresponding to the question. Experimental results demonstrate that our proposed long video event retrieval and description method which significantly improves the efficiency and accuracy of semantic description, and significantly reduces the retrieval time.",,,,,,,,,37,0,0,0,0,0,37,,,1524-9050,1558-0016,,WOS:000673518500053,,
J,"Mittal, Sudhanshu; Tatarchenko, Maxim; Brox, Thomas",,,,,"Tatarchenko, Maxim/0000-0003-1988-1488; Mittal, Sudhanshu/0000-0002-7809-8058",,,Semi-Supervised Semantic Segmentation With High- and Low-Level Consistency,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,43,4,,,1369,1379,,10.1109/TPAMI.2019.2960224,,,,APR 1 2021,2021,"The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. The proposed approach relies on adversarial training with a feature matching loss to learn from unlabeled images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks-PASCAL VOC 2012, PASCAL-Context, and Cityscapes-the approach achieves new state-of-the-art in semi-supervised learning.",,,,,,,,,37,0,0,0,2,0,37,,,0162-8828,1939-3539,,WOS:000626525300019,31869780,
J,"Kavur, A. Emre; Gezer, N. Sinem; Baris, Mustafa; Aslan, Sinem; Conze, Pierre-Henri; Groza, Vladimir; Duc Duy Pham; Chatterjee, Soumick; Ernst, Philipp; Ozkan, Savas; Baydar, Bora; Lachinov, Dmitry; Han, Shuo; Pauli, Josef; Isensee, Fabian; Perkonigg, Matthias; Sathish, Rachana; Rajan, Ronnie; Sheet, Debdoot; Dovletov, Gurbandurdy; Speck, Oliver; Nurnberger, Andreas; Maier-Hein, Klaus H.; Akar, Gozde Bozdagi; Unal, Gozde; Dicle, Oguz; Selver, M. Alper",,,,"Selver, Alper/AAN-1987-2021; Aslan, Sinem/C-3381-2008; Kavur, Ali Emre/B-9569-2016; Unal, Gozde/A-2360-2013; Groza, Vladimir/AAQ-7398-2021; Chatterjee, Soumick/ADI-7612-2022; Dicle, oğuz/AFM-0858-2022","Aslan, Sinem/0000-0003-0068-6551; Kavur, Ali Emre/0000-0002-9328-8140; Unal, Gozde/0000-0001-5942-8966; Chatterjee, Soumick/0000-0001-7594-1188; Baris, Mustafa Mahmut/0000-0002-6496-2781; Selver, Alper/0000-0002-8445-0388; Perkonigg, Matthias/0000-0002-9107-4755; Isensee, Fabian/0000-0002-3519-5886; Nurnberger, Andreas/0000-0003-4311-0624",,,CHAOS Challenge- combined (CT-MR) healthy abdominal organ segmentation,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,69,,,,,,101950,10.1016/j.media.2020.101950,,JAN 2021,,APR 2021,2021,"Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative anal-ysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS-Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE Interna-tional Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Pre-vious abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were de-signed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 +/- 0.00 / 0.95 +/- 0.01), but the best MSSD performance remains limited (21.89 +/- 13.94 / 20.85 +/- 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 +/- 0.15 MSSD: 36.33 +/- 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon. (c) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,37,1,0,0,4,0,37,,,1361-8415,1361-8423,,WOS:000639620600001,33421920,
J,"Ding, Lei; Tang, Hao; Bruzzone, Lorenzo",,,,"Ding, Lei/AAK-2573-2021; Bruzzone, Lorenzo/A-2076-2012","Ding, Lei/0000-0003-0653-8373; Bruzzone, Lorenzo/0000-0002-6036-459X; Tang, Hao/0000-0002-2077-1246",,,LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,59,1,,,426,435,,10.1109/TGRS.2020.2994150,,,,JAN 2021,2021,"The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic segmentation of remote sensing images (RSIs). High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information but are isolated and noisy. It is therefore difficult to bridge the gap between high- and low-level features due to their difference in terms of physical information content and spatial distribution. In this article, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module (PAM) is proposed to enhance the embedding of context information based on a patchwise calculation of local attention. On the other hand, an attention embedding module (AEM) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both proposed modules are lightweight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into a baseline fully convolutional network (FCN), the resulting local attention network (LANet) greatly improves the performance over the baseline and outperforms other attention-based methods on two RSI data sets.",,,,,,,,,37,0,0,0,1,0,37,,,0196-2892,1558-0644,,WOS:000603079000033,,
J,"Scime, Luke; Siddel, Derek; Baird, Seth; Paquit, Vincent",,,,"Paquit, Vincent/AAU-7559-2021","Paquit, Vincent/0000-0003-0331-2598",,,Layer-wise anomaly detection and classification for powder bed additive manufacturing processes: A machine-agnostic algorithm for real-time pixel-wise semantic segmentation,,,,,,,,ADDITIVE MANUFACTURING,,,,36,,,,,,101453,10.1016/j.addma.2020.101453,,,,DEC 2020,2020,"Increasing industry acceptance of powder bed metal Additive Manufacturing requires improved real-time detection and classification of anomalies. Many of these anomalies, such as recoater blade impacts, binder deposition issues, spatter generation, and some porosities, are surface-visible at each layer of the building process. In this work, the authors present a novel Convolutional Neural Network architecture for pixel-wise localization (semantic segmentation) of layer-wise powder bed imaging data. Key advantages of the algorithm include its ability to return segmentation results at the native resolution of the imaging sensor, seamlessly transfer learned knowledge between different Additive Manufacturing machines, and provide real-time performance. The algorithm is demonstrated on six different machines spanning three technologies: laser fusion, binder jetting, and electron beam fusion. Finally, the performance of the algorithm is shown to be superior to that of previous algorithms presented by the authors with respect to localization, accuracy, computation time, and generalizability.",,,,,,,,,37,1,0,0,1,0,37,,,2214-8604,2214-7810,,WOS:000600807800051,,
J,"Zeineldin, Ramy A.; Karar, Mohamed E.; Coburger, Jan; Wirtz, Christian R.; Burgert, Oliver",,,,"Coburger, Jan/ABB-1360-2020; Zeineldin, Ramy Ashraf/AAR-2506-2020; Karar, Mohamed Esmail/AAG-7305-2019","Coburger, Jan/0000-0002-3677-8258; Zeineldin, Ramy Ashraf/0000-0002-8630-9046; Karar, Mohamed Esmail/0000-0002-0474-4723; Wirtz, Christian Rainer/0000-0001-8358-1813; Burgert, Oliver/0000-0001-7118-4730",,,DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images,,,,,,,,INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY,,,,15,6,SI,,909,920,,10.1007/s11548-020-02186-z,,MAY 2020,,JUN 2020,2020,"Purpose Gliomas are the most common and aggressive type of brain tumors due to their infiltrative nature and rapid progression. The process of distinguishing tumor boundaries from healthy cells is still a challenging task in the clinical routine. Fluid-attenuated inversion recovery (FLAIR) MRI modality can provide the physician with information about tumor infiltration. Therefore, this paper proposes a new generic deep learning architecture, namely DeepSeg, for fully automated detection and segmentation of the brain lesion using FLAIR MRI data. Methods The developed DeepSeg is a modular decoupling framework. It consists of two connected core parts based on an encoding and decoding relationship. The encoder part is a convolutional neural network (CNN) responsible for spatial information extraction. The resulting semantic map is inserted into the decoder part to get the full-resolution probability map. Based on modified U-Net architecture, different CNN models such as residual neural network (ResNet), dense convolutional network (DenseNet), and NASNet have been utilized in this study. Results The proposed deep learning architectures have been successfully tested and evaluated on-line based on MRI datasets of brain tumor segmentation (BraTS 2019) challenge, including s336 cases as training data and 125 cases for validation data. The dice and Hausdorff distance scores of obtained segmentation results are about 0.81 to 0.84 and 9.8 to 19.7 correspondingly. Conclusion This study showed successful feasibility and comparative performance of applying different deep learning models in a new DeepSeg framework for automated brain tumor segmentation in FLAIR MR images. The proposed DeepSeg is open source and freely available at https://github.com/razeineldin/DeepSeg/. .",,,,,,,,,36,1,0,0,3,0,37,,,1861-6410,1861-6429,,WOS:000530614600001,32372386,
B,"Minghao Yin; Zhuliang Yao; Yue Cao; Xiu Li; Zheng Zhang; Lin, S.; Han Hu",,,,,,"Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,Disentangled Non-local Neural Networks,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360),,,,,,,,191,207,,10.1007/978-3-030-58555-6_12,,,,2020,2020,"The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics. Code is available at https://github.com/yinmh17/DNL-Semantic-Segmentation and https://github.com/Howal/DNL-Object-Detection.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",36,1,0,0,2,0,37,,,,,978-3-030-58554-9,INSPEC:20108102,,
B,"Yang Zhang; Zixiang Zhou; David, P.; Xiangyu Yue; Zerong Xi; Boqing Gong; Foroosh, H.",,,,,,,,PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,9598,607,,10.1109/CVPR42600.2020.00962,,,,2020,2020,"The requirement of fine-grained perception by autonomous driving systems has resulted in recently increased research in the online semantic segmentation of single-scan LiDAR. Emerging datasets and technological advancements have enabled researchers to benchmark this problem and improve the applicable semantic segmentation algorithms. Still, online semantic segmentation of LiDAR scans in autonomous driving applications remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware, (2) points are distributed unevenly across space, and (3) an increasing number of more fine-grained semantic classes. The combination of the aforementioned challenges motivates us to propose a new LiDAR-specific, KNN-free segmentation algorithm - PolarNet. Instead of using common spherical or bird's-eye-view projection, our polar bird's-eye-view representation balances the points per grid and thus indirectly redistributes the network's attention over the long-tailed points distribution over the radial axis in polar coordination. We find that our encoding scheme greatly increases the mIoU in three drastically different real urban LiDAR single-scan segmentation datasets while retaining ultra low latency and near real-time throughput.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",34,4,0,0,0,0,37,,,,,978-1-7281-7168-5,INSPEC:19876551,,
S,"Zacks, Jeffrey M.",,,,,,"Fiske, ST",,Event Perception and Memory,,,,,,,,"ANNUAL REVIEW OF PSYCHOLOGY, VOL 71",,Annual Review of Psychology,,71,,,,165,191,,10.1146/annurev-psych-010419-051101,,,,2020,2020,"Events make up much of our lived experience, and the perceptual mechanisms that represent events in experience have pervasive effects on action control, language use, and remembering. Event representations in both perception and memory have rich internal structure and connections one to another, and both are heavily informed by knowledge accumulated from previous experiences. Event perception and memory have been identified with specific computational and neural mechanisms, which show protracted development in childhood and are affected by language use, expertise, and brain disorders and injuries. Current theoretical approaches focus on the mechanisms by which events are segmented from ongoing experience, and emphasize the common coding of events for perception, action, and memory. Abetted by developments in eye-tracking, neuroimaging, and computer science, research on event perception and memory is moving from small-scale laboratory analogs to the complexity of events in the wild.",,,,,,,,,37,0,0,0,10,0,37,,,0066-4308,1545-2085,978-0-8243-0271-9,WOS:000507468100007,31905113,
J,"Liu, Quande; Yu, Lequan; Luo, Luyang; Dou, Qi; Heng, Pheng Ann",,,,,,,,Semi-Supervised Medical Image Classification With Relation-Driven Self-Ensembling Model,,,,,,,,IEEE TRANSACTIONS ON MEDICAL IMAGING,,,,39,11,,,3429,3440,,10.1109/TMI.2020.2995518,,,,NOV 2020,2020,"Training deep neural networks usually requires a large amount of labeled data to obtain good performance. However, in medical image analysis, obtaining high-quality labels for the data is laborious and expensive, as accurately annotating medical images demands expertise knowledge of the clinicians. In this paper, we present a novel relation-driven semi-supervised framework for medical image classification. It is a consistency-based method which exploits the unlabeled data by encouraging the prediction consistency of given input under perturbations, and leverages a self-ensembling model to produce high-quality consistency targets for the unlabeled data. Considering that human diagnosis often refers to previous analogous cases to make reliable decisions, we introduce a novel sample relation consistency (SRC) paradigm to effectively exploit unlabeled data by modeling the relationship information among different samples. Superior to existing consistency-based methods which simply enforce consistency of individual predictions, our framework explicitly enforces the consistency of semantic relation among different samples under perturbations, encouraging the model to explore extra semantic information from unlabeled data. We have conducted extensive experiments to evaluate our method on two public benchmark medical image classification datasets, i.e., skin lesion diagnosis with ISIC 2018 challenge and thorax disease classification with ChestX-ray14. Our method outperforms many state-of-the-art semi-supervised learning methods on both single-label and multi-label image classification scenarios.",,,,,,,,,35,1,0,0,5,0,36,,,0278-0062,1558-254X,,WOS:000586352000016,32746096,
J,"Lyu, Ye; Vosselman, George; Xia, Gui-Song; Yilmaz, Alper; Yang, Michael Ying",,,,"Vosselman, George/D-3985-2009; Yang, Michael/AAC-6698-2019","Vosselman, George/0000-0001-8813-8028; Yang, Michael/0000-0002-0649-9987; Lyu, Ye/0000-0002-6665-7748; Yilmaz, Alper/0000-0003-0755-2628; Xia, Gui-Song/0000-0001-7660-6090",,,UAVid: A semantic segmentation dataset for UAV imagery,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,165,,,,108,119,,10.1016/j.isprsjprs.2020.05.009,,,,JUL 2020,2020,"Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/).",,,,,,,,,35,1,0,0,1,0,36,,,0924-2716,1872-8235,,WOS:000540373500009,,
J,"Zhang, Chenxiao; Yue, Peng; Tapete, Deodato; Shangguan, Boyi; Wang, Mi; Wu, Zhaoyan",,,,"Tapete, Deodato/AAB-7528-2021","Tapete, Deodato/0000-0002-7242-4473",,,A multi-level context-guided classification method with object-based convolutional neural network for land cover classification using very high resolution remote sensing images,,,,,,,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,88,,,,,,102086,10.1016/j.jag.2020.102086,,,,JUN 2020,2020,"Classification of very high resolution imagery (VHRI) is challenging due to the difficulty in mining complex spatial and spectral patterns from rich image details. Various object-based Convolutional Neural Networks (OCNN) for VHRI classification have been proposed to overcome the drawbacks of the redundant pixel-wise CNNs, owing to their low computational cost and fine contour-preserving. However, classification performance of OCNN is still limited by geometric distortions, insufficient feature representation, and lack of contextual guidance. In this paper, an innovative multi-level context-guided classification method with the OCNN (MLCG-OCNN) is proposed. A feature-fusing OCNN, including the object contour-preserving mask strategy with the supplement of object deformation coefficient, is developed for accurate object discrimination by learning simultaneously high-level features from independent spectral patterns, geometric characteristics, and object-level contextual information. Then pixel-level contextual guidance is used to further improve the per-object classification results. The MLCG-OCNN method is intentionally tested on two validated small image datasets with limited training samples, to assess the performance in applications of land cover classification where a trade-off between time-consumption of sample training and overall accuracy needs to be found, as it is very common in the practice. Compared with traditional benchmark methods including the patch-based per-pixel CNN (PBPP), the patch-based per-object CNN (PBPO), the pixel-wise CNN with object segmentation refinement (PO), semantic segmentation U-Net (U-NET), and DeepLabV3+(DLV3+), MLCG-OCNN method achieves remarkable classification performance (> 80 %). Compared with the state-of-the-art architecture DeepLabV3+, the MLCG-OCNN method demonstrates high computational efficiency for VHRI classification (4-5 times faster).",,,,,,,,,34,2,1,0,4,0,36,,,0303-2434,,,WOS:000527390800018,,
J,"Moshkov, Nikita; Mathe, Botond; Kertesz-Farkas, Attila; Hollandi, Reka; Horvath, Peter",,,,"Moshkov, Nikita/AAZ-3568-2020; Moshkov, Nikita/V-7518-2017","Moshkov, Nikita/0000-0002-5823-4884; Moshkov, Nikita/0000-0002-5823-4884",,,Test-time augmentation for deep learning-based cell segmentation on microscopy images,,,,,,,,SCIENTIFIC REPORTS,,,,10,1,,,,,5068,10.1038/s41598-020-61808-3,,,,MAR 19 2020,2020,"Recent advancements in deep learning have revolutionized the way microscopy images of cells are processed. Deep learning network architectures have a large number of parameters, thus, in order to reach high accuracy, they require a massive amount of annotated data. A common way of improving accuracy builds on the artificial increase of the training set by using different augmentation techniques. A less common way relies on test-time augmentation (TTA) which yields transformed versions of the image for prediction and the results are merged. In this paper we describe how we have incorporated the test-time argumentation prediction method into two major segmentation approaches utilized in the single-cell analysis of microscopy images. These approaches are semantic segmentation based on the U-Net, and instance segmentation based on the Mask R-CNN models. Our findings show that even if only simple test-time augmentations (such as rotation or flipping and proper merging methods) are applied, TTA can significantly improve prediction accuracy. We have utilized images of tissue and cell cultures from the Data Science Bowl (DSB) 2018 nuclei segmentation competition and other sources. Additionally, boosting the highest-scoring method of the DSB with TTA, we could further improve prediction accuracy, and our method has reached an ever-best score at the DSB.",,,,,,,,,34,2,0,0,11,0,36,,,2045-2322,,,WOS:000563443900007,32193485,
J,"Spoorthi, G. E.; Gorthi, Rama Krishna Sai Subrahmanyam; Gorthi, Subrahmanyam",,,,"GE, Spoorthi/ABB-5823-2020","G. E., Spoorthi/0000-0002-8906-5112; Gorthi, Subrahmanyam/0000-0003-1957-6985",,,PhaseNet 2.0: Phase Unwrapping of Noisy Data Based on Deep Learning Approach,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,4862,4872,,10.1109/TIP.2020.2977213,,,,2020,2020,"Phase unwrapping is an ill-posed classical problem in many practical applications of significance such as 3D profiling through fringe projection, synthetic aperture radar and magnetic resonance imaging. Conventional phase unwrapping techniques estimate the phase either by integrating through the confined path (referred to as path-following methods) or by minimizing the energy function between the wrapped phase and the approximated true phase (referred to as minimum-norm approaches). However, these conventional methods have some critical challenges like error accumulation and high computational time and often fail under low SNR conditions. To address these problems, this paper proposes a novel deep learning framework for unwrapping the phase and is referred to as PhaseNet 2.0. The phase unwrapping problem is formulated as a dense classification problem and a fully convolutional DenseNet based neural network is trained to predict the wrap-count at each pixel from the wrapped phase maps. To train this network, we simulate arbitrary shapes and propose new loss function that integrates the residues by minimizing the difference of gradients and also uses loss to overcome class imbalance problem. The proposed method, unlike our previous approach PhaseNet, does not require post-processing, highly robust to noise, accurately unwraps the phase even at the severe noise level of -5 dB, and can unwrap the phase maps even at relatively high dynamic ranges. Simulation results from the proposed framework are compared with different classes of existing phase unwrapping methods for varying SNR values and discontinuity, and these evaluations demonstrate the advantages of the proposed framework. We also demonstrate the generality of the proposed method on 3D reconstruction of synthetic CAD models that have diverse structures and finer geometric variations. Finally, the proposed method is applied to real-data for 3D profiling of objects using fringe projection technique and digital holographic interferometry. The proposed framework achieves significant improvements over existing methods while being highly efficient with interactive frame-rates on modern GPUs.",,,,,,,,,36,1,0,0,1,0,36,,,1057-7149,1941-0042,,WOS:000526524400003,,
J,"Zhang, Xixin; Yang, Yuhang; Li, Zhiyong; Ning, Xin; Qin, Yilang; Cai, Weiwei",,,,"Ning, Xin/M-9479-2018; Cai, Weiwei/AAH-5456-2020","Ning, Xin/0000-0001-7897-1673; Cai, Weiwei/0000-0001-6795-6152; li, zhiyong/0000-0003-2402-7657",,,An Improved Encoder-Decoder Network Based on Strip Pool Method Applied to Segmentation of Farmland Vacancy Field,,,,,,,,ENTROPY,,,,23,4,,,,,435,10.3390/e23040435,,,,APR 2021,2021,"In the research of green vegetation coverage in the field of remote sensing image segmentation, crop planting area is often obtained by semantic segmentation of images taken from high altitude. This method can be used to obtain the rate of cultivated land in a region (such as a country), but it does not reflect the real situation of a particular farmland. Therefore, this paper takes low-altitude images of farmland to build a dataset. After comparing several mainstream semantic segmentation algorithms, a new method that is more suitable for farmland vacancy segmentation is proposed. Additionally, the Strip Pooling module (SPM) and the Mixed Pooling module (MPM), with strip pooling as their core, are designed and fused into the semantic segmentation network structure to better extract the vacancy features. Considering the high cost of manual data annotation, this paper uses an improved ResNet network as the backbone of signal transmission, and meanwhile uses data augmentation to improve the performance and robustness of the model. As a result, the accuracy of the proposed method in the test set is 95.6%, mIoU is 77.6%, and the error rate is 7%. Compared to the existing model, the mIoU value is improved by nearly 4%, reaching the level of practical application.",,,,,,,,,35,0,0,0,0,0,35,,,,1099-4300,,WOS:000642950300001,33917753,
J,"Kervadec, Hoel; Bouchtiba, Jihene; Desrosiers, Christian; Granger, Eric; Dolz, Jose; Ben Ayed, Ismail",,,,,,,,Boundary loss for highly unbalanced segmentation,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,67,,,,,,101851,10.1016/j.media.2020.101851,,,,JAN 2021,2021,"Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy, are based on integrals over the segmentation regions. Unfortunately, for highly unbalanced segmentations, such regional summations have values that differ by several orders of magnitude across classes, which affects training performance and stability. We propose a boundary loss, which takes the form of a distance metric on the space of contours, not regions. This can mitigate the difficulties of highly unbalanced problems because it uses integrals over the interface between regions instead of unbalanced integrals over the regions. Furthermore, a boundary loss complements regional information. Inspired by graph-based optimization techniques for computing active-contour flows, we express a non-symmetric L-2 distance on the space of contours as a regional integral, which avoids completely local differential computations involving contour points. This yields a boundary loss expressed with the regional softmax probability outputs of the network, which can be easily combined with standard regional losses and implemented with any existing deep network architecture for N-D segmentation. We report comprehensive evaluations and comparisons on different unbalanced problems, showing that our boundary loss can yield significant increases in performances while improving training stability. Our code is publicly available(1). (C) 2020 Elsevier B.V. All rights reserved.",,,,,,,,,35,0,0,0,7,0,35,,,1361-8415,1361-8423,,WOS:000598892100002,33080507,
J,"Liu, Qinghui; Kampffmeyer, Michael; Jenssen, Robert; Salberg, Arnt-Borre",,,,,"Jenssen, Robert/0000-0002-7496-8474; Kampffmeyer, Michael/0000-0002-7699-0405; Liu, Qinghui/0000-0002-3334-8631",,,Dense Dilated Convolutions Merging Network for Land Cover Classification,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,9,,,6309,6320,,10.1109/TGRS.2020.2976658,,,,SEPT 2020,2020,"Land cover classification of remote sensing images is a challenging task due to limited amounts of annotated data, highly imbalanced classes, frequent incorrect pixel-level annotations, and an inherent complexity in the semantic segmentation task. In this article, we propose a novel architecture called the dense dilated convolutions merging network (DDCM-Net) to address this task. The proposed DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates. This effectively utilizes rich combinations of dilated convolutions that enlarge the networks receptive fields with fewer parameters and features compared with the state-of-the-art approaches in the remote sensing domain. Importantly, DDCM-Net obtains fused local- and global-context information, in effect incorporating surrounding discriminative capability for multiscale and complex-shaped objects with similar color and textures in very high-resolution aerial imagery. We demonstrate the effectiveness, robustness, and flexibility of the proposed DDCM-Net on the publicly available ISPRS Potsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set. Our single model, trained on three-band Potsdam and Vaihingen data sets, achieves better accuracy in terms of both mean intersection over union (mIoU) and F1-score compared with other published models trained with more than three-band data. We further validate our model on the DeepGlobe data set, achieving state-of-the-art result 56.2% mIoU with much fewer parameters and at a lower computational cost compared with related recent work.",,,,,,,,,34,1,0,0,0,0,35,,,0196-2892,1558-0644,,WOS:000564455700023,,
J,"Won, Jong; Czerniawski, Thomas; Leite, Fernanda",,,,"Leite, Fernanda/AAT-2099-2021","Leite, Fernanda/0000-0002-7789-4474; Czerniawski, Thomas/0000-0002-7310-6522",,,Semantic segmentation of point clouds of building interiors with deep learning: Augmenting training datasets with synthetic BIM-based point clouds,,,,,,,,AUTOMATION IN CONSTRUCTION,,,,113,,,,,,103144,10.1016/j.autcon.2020.103144,,,,MAY 2020,2020,"This paper investigates the viability of using synthetic point clouds generated from building information models (BIMs) to train deep neural networks to perform semantic segmentation of point clouds of building interiors. In order to achieve these goals, this paper first presents a procedure for converting digital 3D BIMs into synthetic point clouds using three commercially available software systems. Then the generated synthetic point clouds are used to train a deep neural network. Semantic segmentation performance is compared for several models trained on: real point clouds, synthetic point clouds, and combinations of real and synthetic point clouds. A key finding is the 7.1% IOU boost in performance achieved when a small real point cloud dataset is augmented by synthetic point clouds for training, as compared to training the classifier on the real data alone. The experimental results confirmed the viability of using synthetic point clouds generated from building information models in combination with small datasets of real point clouds. This opens up the possibility of developing a segmentation model for building interiors that can be applied to as-built modeling of buildings that contain unseen indoor structures.",,,,,,,,,34,1,0,0,0,0,35,,,0926-5805,1872-7891,,WOS:000526785700032,,
J,"Feng, Xue; Tustison, Nicholas J.; Patel, Sohil H.; Meyer, Craig H.",,,,,,,,Brain Tumor Segmentation Using an Ensemble of 3D U-Nets and Overall Survival Prediction Using Radiomic Features,,,,,,,,FRONTIERS IN COMPUTATIONAL NEUROSCIENCE,,,,14,,,,,,25,10.3389/fncom.2020.00025,,,,APR 8 2020,2020,"Accurate segmentation of different sub-regions of gliomas such as peritumoral edema, necrotic core, enhancing, and non-enhancing tumor core from multimodal MRI scans has important clinical relevance in diagnosis, prognosis and treatment of brain tumors. However, due to the highly heterogeneous appearance and shape of these tumors, segmentation of the sub-regions is challenging. Recent developments using deep learning models has proved its effectiveness in various semantic and medical image segmentation tasks, many of which are based on the U-Net network structure with symmetric encoding and decoding paths for end-to-end segmentation due to its high efficiency and good performance. In brain tumor segmentation, the 3D nature of multimodal MRI poses challenges such as memory and computation limitations and class imbalance when directly adopting the U-Net structure. In this study we aim to develop a deep learning model using a 3D U-Net with adaptations in the training and testing strategies, network structures, and model parameters for brain tumor segmentation. Furthermore, instead of picking one best model, an ensemble of multiple models trained with different hyper-parameters are used to reduce random errors from each model and yield improved performance. Preliminary results demonstrate the effectiveness of this method and achieved the 9th place in the very competitive 2018 Multimodal Brain Tumor Segmentation (BraTS) challenge. In addition, to emphasize the clinical value of the developed segmentation method, a linear model based on the radiomics features extracted from segmentation and other clinical features are developed to predict patient overall survival. Evaluation of these innovations shows high prediction accuracy in both low-grade glioma and glioblastoma patients, which achieved the 1st place in the 2018 BraTS challenge.",,,,,,,,,34,1,0,0,5,0,35,,,,1662-5188,,WOS:000529224500001,32322196,
J,"Bulens, Philippe; Couwenberg, Alice; Intven, Martijn; Debucquoy, Annelies; Vandecaveye, Vincent; Van Cutsem, Eric; D'Hoore, Andre; Wolthuis, Albert; Mukherjee, Pritam; Gevaert, Olivier; Haustermans, Karin",,,,"Van+Cutsem, Eric/ABE-1762-2021; Gevaert, Olivier/H-3413-2019; Haustermans, Karin/AAB-5912-2021; Mukherjee, Pritam/AAL-6003-2021; Intven, Martijn/Z-1746-2018","Gevaert, Olivier/0000-0002-9965-5466; Haustermans, Karin/0000-0003-0364-682X; Bulens, Philippe/0000-0001-7450-4162; Intven, Martijn/0000-0002-5068-5517",,,Predicting the tumor response to chemoradiotherapy for rectal cancer: Model development and external validation using MRI radiomics,,,,,,,,RADIOTHERAPY AND ONCOLOGY,,,,142,,,,246,252,,10.1016/j.radonc.2019.07.033,,,,JAN 2020,2020,"Background: In well-responding patients to chemoradiotherapy for locally advanced rectal cancer (LARC), a watch-and-wait strategy can be considered. To implement organ-sparing strategies, accurate patient selection is needed. We investigate the use of MRI-based radiomics models to predict tumor response to improve patient selection.Materials and methods: Models were developed in a cohort of 70 patients and validated in an external cohort of 55 patients. Patients received chemoradiation followed by surgery and underwent T2-weighted and diffusion-weighted MRI (DW-MRI) before and after chemoradiation. The outcome measure was (near-)complete pathological tumor response (ypT0-1N0). Tumor segmentation was done on T2-images and transferred to b800-images and ADC maps, after which quantitative and four semantic features were extracted. We combined features using principal component analysis and built models using LASSO regression analysis. The best models based on precision and performance were selected for validation.Results: 21/70 patients (30%) achieved ypT0-1N0 in the development cohort versus 13/55 patients (24%) in the validation cohort. Three models (t2_dwi_pre_post, semantic_dwi_adc_pre, semantic_dwi_post) were identified with an area-under-the-curve (AUC) of 0.83 (95% CI 0.70-0.95), 0.86 (95% CI 0.75-0.98) and 0.84 (95% CI 0.75-0.94) respectively. Two models (t2_dwi_pre_post, semantic_dwi_post) validated well in the external cohort with AUCs of 0.83 (95% CI 0.70-0.95) and 0.86 (95% CI 0.76-0.97). These models however did not outperform a previously established four-feature semantic model.Conclusion: Prediction models based on MRI radiomics non-invasively predict tumor response after chemoradiation for rectal cancer and can be used as an additional tool to identify patients eligible for an organ-preserving treatment. (C) 2019 Elsevier B.V. All rights reserved.",,,,,,,,,30,5,0,0,5,0,35,,,0167-8140,1879-0887,,WOS:000510015800033,31431368,
J,"Zhu, Qing; Liao, Cheng; Hu, Han; Mei, Xiaoming; Li, Haifeng",,,,"Hu, Han/V-5068-2018","Hu, Han/0000-0003-1137-2208; Li, Haifeng/0000-0003-1173-6593",,,MAP-Net: Multiple Attending Path Neural Network for Building Footprint Extraction From Remote Sensed Imagery,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,59,7,,,6169,6181,,10.1109/TGRS.2020.3026051,,,,JUL 2021,2021,"Building footprint extraction is a basic task in the fields of mapping, image understanding, computer vision, and so on. Accurately and efficiently extracting building footprints from a wide range of remote sensed imagery remains a challenge due to the complex structures, variety of scales, and diverse appearances of buildings. Existing convolutional neural network (CNN)-based building extraction methods are criticized for their inability to detect tiny buildings because the spatial information of CNN feature maps is lost during repeated pooling operations of the CNN. In addition, large buildings still have inaccurate segmentation edges. Moreover, features extracted by a CNN are always partially restricted by the size of the receptive field, and large-scale buildings with low texture are always discontinuous and holey when extracted. To alleviate these problems, multiscale strategies are introduced in the latest research works to extract buildings with different scales. The features with higher resolution generally extracted from shallow layers, which extracted insufficient semantic information for tiny buildings. This article proposes a novel multiple attending path neural network (MAP-Net) for accurately extracting multiscale building footprints and precise boundaries. Unlike existing multiscale feature extraction strategies, MAP-Net learns spatial localization-preserved multiscale features through a multiparallel path in which each stage is gradually generated to extract high-level semantic features with fixed resolution. Then, an attention module adaptively squeezes the channel-wise features extracted from each path for optimized multiscale fusion, and a pyramid spatial pooling module captures global dependence for refining discontinuous building footprints. Experimental results show that our method achieved 0.88%, 0.93%, and 0.45% F1-score and 1.53%, 1.50%, and 0.82% intersection over union (IoU) score improvements without increasing computational complexity compared with the latest HRNetv2 on the Urban 3-D, Deep Globe, and WHU data sets, respectively. Specifically, MAP-Net outperforms multiscale aggregation fully convolutional network (MA-FCN), which is the state-of-the-art (SOTA) algorithms with postprocessing and model voting strategies, on the WHU data set without pretraining and postprocessing. The TensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.",,,,,,,,,33,1,0,0,0,0,34,,,0196-2892,1558-0644,,WOS:000665167500054,,
J,"Majeed, Yaqoob; Zhang, Jing; Zhang, Xin; Fu, Longsheng; Karkee, Manoj; Zhang, Qin; Whiting, Matthew D.",,,,"Fu, Longsheng/AAB-5720-2022; Majeed, Yaqoob/AAM-2712-2020; Zhang, Qin/AAM-2009-2020","Fu, Longsheng/0000-0003-3253-2637; Majeed, Yaqoob/0000-0003-1782-3436; Zhang, Qin/0000-0002-6121-6513",,,Deep learning based segmentation for automated training of apple trees on trellis wires,,,,,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,170,,,,,,105277,10.1016/j.compag.2020.105277,,,,MAR 2020,2020,"Trellised fruiting-wall training systems are becoming standard for modern apple orchards due to their high fruit yield and quality, and their suitability to robotic operations in pruning and harvesting. In a common practice of training young apple trees to a trellis-trained canopy system in PNW region of US, trees branches are manually selected and then tied to horizontal trellis wires in 6 or 7 tiers. As manual training of apple trees to these modern orchard architectures is becoming challenging due to less availability of skilled human labor with quickly increased labor cost, automated training using sensing and robotic techniques could be an alternative solution. Segmenting trunks, branches, and trellis wires is a critical step in automating a tree training operation. In this study, a deep learning-based semantic segmentation method was developed for automatically performing this segmentation task. A Kinect V2 sensor was used to obtain the RGB and point cloud data of target trees. Then both Simple- and Foreground-RGB images were used for training a convolutional neural network (CNN)-based segmentation network (SegNet) to segment the trunk, branch, and trellis wire. Trunks and branches, which share some common features, were segmented from each other with accuracies of 0.82 and 0.89 for Simple-RGB images and 0.91 and 0.92 for Foreground-RGB images, respectively. Similarly, trellis wires, which have distinct features from both the trunk and branches, were segmented with accuracies of 0.92 and 0.97 for the Simple- and Foreground-RGB images, respectively. Obtained results showed that the performance of the developed semantic segmentation technique was better with Foreground-RGB images compared to the same with Simple-RGB images. Accuracy in identifying the segmented region boundaries in Foreground-RGB images, represented by the Boundary-F1 score, was 0.93, 0.89, and 0.91 for the trunk, branch, and trellis wire, respectively. These results showed a promising potential for adopting deep learning-based semantic segmentation for automating apple tree training in orchard environment.",,,,,,,,,33,1,0,0,22,0,34,,,0168-1699,1872-7107,,WOS:000519652000018,,
J,"Roy, Abhijit Guha; Siddiqui, Shayan; Poelsterl, Sebastian; Navab, Nassir; Wachinger, Christian",,,,,"Guha Roy, Abhijit/0000-0001-7221-468X; Polsterl, Sebastian/0000-0002-1607-7550",,,'Squeeze & excite' guided few-shot segmentation of volumetric images,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,59,,,,,,101587,10.1016/j.media.2019.101587,,,,JAN 2020,2020,"Deep neural networks enable highly accurate image segmentation, but require large amounts of manually annotated data for supervised training. Few-shot learning aims to address this shortcoming by learning a new class from a few annotated support examples. We introduce, a novel few-shot framework, for the segmentation of volumetric medical images with only a few annotated slices. Compared to other related works in computer vision, the major challenges are the absence of pre-trained networks and the volumetric nature of medical scans. We address these challenges by proposing a new architecture for few-shot segmentation that incorporates 'squeeze & excite' blocks. Our two-armed architecture consists of a conditioner arm, which processes the annotated support input and generates a task-specific representation. This representation is passed on to the segmenter arm that uses this information to segment the new query image. To facilitate efficient interaction between the conditioner and the segmenter arm, we propose to use 'channel squeeze & spatial excitation' blocks - a light-weight computational module - that enables heavy interaction between both the arms with negligible increase in model complexity. This contribution allows us to perform image segmentation without relying on a pre-trained model, which generally is unavailable for medical scans. Furthermore, we propose an efficient strategy for volumetric segmentation by optimally pairing a few slices of the support volume to all the slices of the query volume. We perform experiments for organ segmentation on whole-body contrast-enhanced CT scans from the Visceral Dataset. Our proposed model outperforms multiple baselines and existing approaches with respect to the segmentation accuracy by a significant margin. The source code is available at https://github.comiabhi4ssj/few-shot-segmentation. (C) 2019 Elsevier B.V. All rights reserved.",,,,,,,,,34,0,0,0,6,0,34,,,1361-8415,1361-8423,,WOS:000501405800005,31630012,
C,"Tezcan, M. Ozan; Ishwar, Prakash; Konrad, Janusz",,,IEEE Comp Soc,,,,,BSUV-Net: A Fully-Convolutional Neural Network for Background Subtraction of Unseen Videos,,,,,,,,2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV),,IEEE Winter Conference on Applications of Computer Vision,,,,,,2763,2772,,,,,,2020,2020,"Background subtraction is a basic task in computer vision and video processing often applied as a pre-processing step for object tracking, people recognition, etc. Recently, a number of successful background-subtraction algorithms have been proposed, however nearly all of the top-performing ones are supervised. Crucially, their success relies upon the availability of some annotated frames of the test video during training. Consequently, their performance on completely unseen videos is undocumented in the literature. In this work, we propose a new, supervised, background-subtraction algorithm for unseen videos (BSUV-Net) based on a fully-convolutional neural network. The input to our network consists of the current frame and two background frames captured at different time scales along with their semantic segmentation maps. In order to reduce the chance of overfitting, we also introduce a new data-augmentation technique which mitigates the impact of illumination difference between the background frames and the current frame. On the CDNet-2014 dataset, BSUV-Net outperforms state-of-the-art algorithms evaluated on unseen videos in terms of several metrics including F-measure, recall and precision.",,,,,IEEE Winter Conference on Applications of Computer Vision (WACV)IEEE Winter Conference on Applications of Computer Vision (WACV),"MAR 01-05, 2020MAR 01-05, 2020",IEEEIEEE,"Snowmass, COSnowmass, CO",33,1,0,0,0,0,34,,,2472-6737,,978-1-7281-6553-0,WOS:000578444802087,,
J,"Ferreira, Matheus Pinheiro; Alves de Almeida, Danilo Roberti; Papa, Daniel de Almeida; Silva Minervino, Juliano Baldez; Pessoa Veras, Hudson Franklin; Formighieri, Arthur; Nascimento Santos, Caio Alexandre; Dantas Ferreira, Marcio Aurelio; Figueiredo, Evandro Orfano; Linhares Ferreira, Evandro Jose",,,,"Ferreira, Matheus/J-2389-2018; de Almeida, Danilo Roberti Alves/L-3044-2019","Ferreira, Matheus/0000-0003-0687-4422; de Almeida, Danilo Roberti Alves/0000-0002-8747-0085; Linhares Ferreira, Evandro Jose/0000-0001-9591-9615",,,Individual tree detection and species classification of Amazonian palms using UAV images and deep learning,,,,,,,,FOREST ECOLOGY AND MANAGEMENT,,,,475,,,,,,118397,10.1016/j.foreco.2020.118397,,,,NOV 1 2020,2020,"Information regarding the spatial distribution of palm trees in tropical forests is crucial for commercial exploitation and management. However, spatially continuous knowledge of palms occurrence is scarce and difficult to obtain with conventional approaches such as field inventories. Here, we developed a new method to map Amazonian palm species at the individual tree crown (ITC) level using RGB images acquired by a low-cost unmanned aerial vehicle (UAV). Our approach is based on morphological operations performed in the score maps of palm species derived from a fully convolutional neural network model. We first constructed a labeled dataset by dividing the study area (135 ha within an old-growth Amazon forest) into 28 plots of 250 m x 150 m. Then, we manually outlined all palm trees seen in RGB images with 4 cm pixels. We identified three palm species: Attalea butyracea, Euterpe precatoria and Iriartea deltoidea. We randomly selected 22 plots (80%) for training and six plots (20%) for testing. We changed the plots for training and testing to evaluate the variability in the classification accuracy and assess model generalization. Our method outperformed the average producer's accuracy of conventional patch-wise semantic segmentation (CSS) in 4.7%. Moreover, our method correctly identified, on average, 34.7 percentage points more ITCs than CSS, which tended to merge trees that are close to each other. The producer's accuracy of A. butyracea, E. precatoria and I. deltoidea was 78.6 +/- 5.5%, 98.6 +/- 1.4% and 96.6 +/- 3.4%, respectively. Fortunately, one of the most exploited and commercialized palm species in the Amazon (E. precatoria, a.k.a, Acai) was mapped with the highest classification accuracy. Maps of E. precatoria derived from low-cost UAV systems can support management projects and community-based forest monitoring programs in the Amazon.",,,,,,,,,33,0,0,0,6,0,33,,,0378-1127,1872-7042,,WOS:000581970500011,,
J,"Peng, Chengli; Ma, Jiayi",,,,"Ma, Jiayi/Y-2470-2019","Ma, Jiayi/0000-0003-3264-3265; Peng, Chengli/0000-0001-6042-3871",,,Semantic segmentation using stride spatial pyramid pooling and dual attention decoder,,,,,,,,PATTERN RECOGNITION,,,,107,,,,,,107498,10.1016/j.patcog.2020.107498,,,,NOV 2020,2020,"Semantic segmentation is an end-to-end task that requires both semantic and spatial accuracy. It is important for deep learning-based segmentation methods to effectively utilize the high-level feature map whose semantic information is abundant and the low-level feature map whose spatial information is accurate. However, existing segmentation networks typically cannot take full advantage of these two kinds of feature maps, leading to inferior performance. This paper attempts to overcome this challenge by introducing two novel structures. On the one hand, we propose a structure called stride spatial pyramid pooling (SSPP) to capture multiscale semantic information from the high-level feature map. Compared with existing pyramid pooling methods based on the atrous convolution, the SSPP structure is able to gather more information from the high-level feature map with faster inference speed, which improves the utilization efficiency of the high-level feature map significantly. On the other hand, we propose a dual attention decoder consisting of a channel attention branch and a spatial attention branch to make full use of the high- and low-level feature maps simultaneously. The dual attention decoder can result in a more semantic low-level feature map and a high-level feature map with more accurate spatial information, which bridges the gap between these two kinds of feature maps and benefits their fusion. We evaluate the proposed model on several publicly available semantic image segmentation benchmarks including PASCAL VOC 2012, Cityscapes and COCO-Stuff. The qualitative and quantitative results demonstrate that our method can achieve the state-of-the-art performance. (C) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,30,3,0,0,4,0,33,,,0031-3203,1873-5142,,WOS:000552866000039,,
J,"Peng, Jizong; Estrada, Guillermo; Pedersoli, Marco; Desrosiers, Christian",,,,,,,,Deep co-training for semi-supervised image segmentation,,,,,,,,PATTERN RECOGNITION,,,,107,,,,,,107269,10.1016/j.patcog.2020.107269,,,,NOV 2020,2020,"In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation. (C) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,30,3,0,0,7,0,33,,,0031-3203,1873-5142,,WOS:000552866000001,,
J,"Yekeen, Shamsudeen Temitope; Balogun, Abdul-Lateef; Yusof, Khamaruzaman B. Wan",,,,,,,,A novel deep learning instance segmentation model for automated marine oil spill detection,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,167,,,,190,200,,10.1016/j.isprsjprs.2020.07.011,,,,SEP 2020,2020,"The visual similarity of oil slick and other elements, known as look-alike, affects the reliability of synthetic aperture radar (SAR) images for marine oil spill detection. So far, detection and discrimination of oil spill and look-alike are still limited to the use of traditional machine learning algorithms and semantic segmentation deep learning models with limited accuracy. Thus, this study developed a novel deep learning oil spill detection model using computer vision instance segmentation Mask-Region-based Convolutional Neural Network (Mask R-CNN) model. The model training was conducted using transfer learning on the ResNet 101 on COCO as backbone in combination with Feature Pyramid Network (FPN) architecture for feature extraction at 30 epochs with 0.001 learning rate. Testing of the model was conducted using the least training and validation loss value on the withheld testing images. The model's performance was evaluated using precision, recall, specificity, IoU, F1-measure and overall accuracy values. Ship detection and segmentation had the highest performance with overall accuracy of 98.3%. The model equally showed a higher accuracy for oil spill and look-alike detection and segmentation although oil spill detection outperformed look-alike with overall accuracy values of 96.6% and 91.0% respectively. The study concluded that the deep learning instance segmentation model performs better than conventional machine learning models and deep learning semantic segmentation models in detection and segmentation.",,,,,,,,,33,1,0,0,3,0,33,,,0924-2716,1872-8235,,WOS:000561346200014,,
J,"Hafiz, Abdul Mueed; Bhat, Ghulam Mohiuddin",,,,"Hafiz, Abdul Mueed/AAF-3806-2020","Hafiz, Abdul Mueed/0000-0002-2266-3708",,,A survey on instance segmentation: state of the art,,,,,,,,INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL,,,,9,3,,,171,189,,10.1007/s13735-020-00195-x,,JUL 2020,,SEP 2020,2020,"Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.",,,,,,,,,32,0,1,0,3,0,33,,,2192-6611,2192-662X,,WOS:000545199700001,,
J,"Du, Getao; Cao, Xu; Liang, Jimin; Chen, Xueli; Zhan, Yonghua",,,,,"Liang, Jimin/0000-0003-1428-5804",,,Medical Image Segmentation based on U-Net: A Review,,,,,,,,JOURNAL OF IMAGING SCIENCE AND TECHNOLOGY,,,,64,2,,,,,20508,10.2352/J.ImagingSci.Technol.2020.64.2.020508,,,,MAR 2020,2020,"Medical image analysis is performed by analyzing images obtained by medical imaging systems to solve clinical problems. The purpose is to extract effective information and improve the level of clinical diagnosis. In recent years, automatic segmentation based on deep learning (DL) methods has been widely used, where a neural network can automatically learn image features, which is in sharp contrast with the traditional manual learning method. U-net is one of the most important semantic segmentation frameworks for a convolutional neural network (CNN). It is widely used in the medical image analysis domain for lesion segmentation, anatomical segmentation, and classification. The advantage of this network framework is that it can not only accurately segment the desired feature target and effectively process and objectively evaluate medical images but also help to improve accuracy in the diagnosis by medical images. Therefore, this article presents a literature review of medical image segmentation based on U-net, focusing on the successful segmentation experience of U-net for different lesion regions in six medical imaging systems. Along with the latest advances in DL, this article introduces the method of combining the original U-net architecture with deep learning and a method for improving the U-net network. (C) 2020 Society for Imaging Science and Technology.",,,,,,,,,32,1,0,0,5,0,33,,,1062-3701,1943-3522,,WOS:000534263600008,,
J,"Bueno, Gloria; Milagro Fernandez-Carrobles, M.; Gonzalez-Lopez, Lucia; Deniz, Oscar",,,,"Bueno, Gloria/K-6286-2014","Bueno, Gloria/0000-0002-7345-4869; Deniz, Oscar/0000-0002-0841-4131",,,Glomerulosclerosis identification in whole slide images using semantic segmentation,,,,,,,,COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,,,,184,,,,,,105273,10.1016/j.cmpb.2019.105273,,,,FEB 2020,2020,"Background and Objective: Glomeruli identification, i.e., detection and characterization, is a key procedure in many nephropathology studies. In this paper, semantic segmentation based on convolutional neural networks (CNN) is proposed to detect glomeruli using Whole Slide Imaging (WSI) follows by a classification CNN to divide the glomeruli into normal and sclerosed.Methods: Comparison between U-Net and SegNet CNNs is performed for pixel-level segmentation considering both a two and three class problem, that is, a) non-glomerular and glomerular structures and b) non-glomerular normal glomerular and sclerotic structures. The two class semantic segmentation result is then used for a CNN classification where glomerular regions are divided into normal and global sclerosed glomeruli.Results: These methods were tested on a dataset composed of 47 WSIs belonging to human kidney sections stained with Periodic Acid Schiff (PAS). The best approach was the SegNet for two class segmentation follows by a fine-tuned AlexNet network to characterize the glomeruli. 98.16% of accuracy was obtained with this process of consecutive CNNs (SegNet-AlexNet) for segmentation and classification.Conclusion: The results obtained demonstrate that the sequential CNN segmentation-classification strategy achieves higher accuracy reducing misclassified cases and therefore being the methodology proposed for glomerulosclerosis detection. (C) 2019 Elsevier B.V. All rights reserved.",,,,,,,,,33,0,0,0,12,0,33,,,0169-2607,1872-7565,,WOS:000513169400024,31891905,
J,"Ahmed, Luqman; Iqbal, Muhammad Munwar; Aldabbas, Hamza; Khalid, Shehzad; Saleem, Yasir; Saeed, Saqib",,,,"Saeed, Saqib/E-4381-2014; Aldabbas, Hamza/AAD-3989-2021","Saeed, Saqib/0000-0001-7136-3480; ",,,Images data practices for Semantic Segmentation of Breast Cancer using Deep Neural Network,,,,,,,,JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,,,,,,,,,,,10.1007/s12652-020-01680-1,,JAN 2020,,,2020,"Image data in healthcare is playing a vital role. Medical data records are increasing rapidly, which is beneficial and detrimental at the same time. Large Image dataset are difficult to handle, extracting information, and machine learning. The mammograms data used in this research are low range x-ray images of the breast region, which contains abnormalities. Breast cancer is the most frequently diagnosed cancer and ranked 9th worldwide in breast cancer-related deaths. In Pakistan 1 in 9 women expected to have breast cancer at some stage in life. Screening mammography is the most effective means for its early detection. This high rate of oversampling is responsible for billions in excess health care cost and unnecessary patient anxiety. This research mainly focuses on the development of deep learning based computer-aided system to detect, classify and segment the cancerous region in mammograms. Moreover, the preprocessing mechanism is proposed that remove noise, artifacts and muscle region that can cause a high false positive rate. In order to increase the efficiency of the system and counter the large resource requirement, the pre-processed image is converted to 512 x 512 patches. The two publicly available breast cancer dataset are employed i.e. Mammographic Image Analysis Society (MIAS) digital mammogram dataset and Curated Breast Imaging Subset of (Digital Database for Screening Mammography) (CBIS-DDSM). The two states of art deep learning-based instance segmentation frameworks are used, i.e. DeepLab and Mask RCNN. The pre-processing algorithm helps to increase the area under the receiver operating curve for each transfer learning method. The fine tuning is performed for better performance, the area under the curve was equal to 0.98 and 0.95 for mask RCNN and deep lab respectively on a test set of 150 cases. However, mean average precision for the segmentation task is 0.80 and 0.75. The radiologists accuracy ranged from 0.80 to 0.88. The proposed research has the potential to help radiologists with breast mass classification as well as segmentation of the cancerous region.",,,,,,,,,33,0,0,0,4,0,33,,,1868-5137,1868-5145,,WOS:000515624200001,,
J,"Liu, Juhua; Wang, Chaoyue; Su, Hai; Du, Bo; Tao, Dacheng",,,,,"Wang, Chaoyue/0000-0002-9002-1029; Liu, Juhua/0000-0002-3907-8820",,,Multistage GAN for Fabric Defect Detection,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,3388,3400,,10.1109/TIP.2019.2959741,,,,2020,2020,"Fabric defect detection is an intriguing but challenging topic. Many methods have been proposed for fabric defect detection, but these methods are still suboptimal due to the complex diversity of both fabric textures and defects. In this paper, we propose a generative adversarial network (GAN)-based framework for fabric defect detection. Considering existing challenges in real-world applications, the proposed fabric defect detection system is capable of learning existing fabric defect samples and automatically adapting to different fabric textures during different application periods. Specifically, we customize a deep semantic segmentation network for fabric defect detection that can detect different defect types. Furthermore, we attempted to train a multistage GAN to synthesize reasonable defects in new defect-free samples. First, a texture-conditioned GAN is trained to explore the conditional distribution of defects given different texture backgrounds. Given a novel fabric, we aim to generate reasonable defective patches. Then, a GAN-based fusion network fuses the generated defects to specific locations. Finally, the well-trained multistage GAN continuously updates the existing fabric defect datasets and contributes to the fine-tuning of the semantic segmentation network to better detect defects under different conditions. Comprehensive experiments on various representative fabric samples are conducted to verify the detection performance of our proposed method.",,,,,,,,,33,0,0,0,0,0,33,,,1057-7149,1941-0042,,WOS:000510750900039,31870985,
B,"Qingyong Hu; Bo Yang; Linhai Xie; Rosa, S.; Yulan Guo; Zhihua Wang; Trigoni, N.; Markham, A.",,,,,,,,RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,11105,14,,10.1109/CVPR42600.2020.01112,,,,2020,2020,"We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200x faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",31,2,0,0,0,0,33,,,,,978-1-7281-7168-5,INSPEC:19874276,,
B,Rui Fan; Hengli Wang; Peide Cai; Ming Liu,,,,,,"Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection,,,,,,,,Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375),,,,,,,,340,56,,10.1007/978-3-030-58577-8_21,,,,2020,2020,"Freespace detection is an essential component of visual perception for self-driving cars. The recent efforts made in data-fusion convolutional neural networks (CNNs) have significantly improved semantic driving scene segmentation. Freespace can be hypothesized as a ground plane, on which the points have similar surface normals. Hence, in this paper, we first introduce a novel module, named surface normal estimator (SNE), which can infer surface normal information from dense depth/disparity images with high accuracy and efficiency. Furthermore, we propose a data-fusion CNN architecture, referred to as RoadSeg, which can extract and fuse features from both RGB images and the inferred surface normal information for accurate freespace detection. For research purposes, we publish a large-scale synthetic freespace detection dataset, named Ready-to-Drive (R2D) road dataset, collected under different illumination and weather conditions. The experimental results demonstrate that our proposed SNE module can benefit all the state-of-the-art CNNs for freespace detection, and our SNE-RoadSeg achieves the best overall performance among different datasets.",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",33,0,0,0,0,0,33,,,,,978-3-030-58576-1,INSPEC:20044313,,
C,"Wang, Li; Li, Dong; Zhu, Yousong; Tian, Lu; Shan, Yi",,,IEEE,,,,,Dual Super-Resolution Learning for Semantic Segmentation,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,3773,3782,,10.1109/CVPR42600.2020.00383,,,,2020,2020,"Current state-of-the-art semantic segmentation methods often apply high-resolution input to attain high performance, which brings large computation budgets and limits their applications on resource-constrained devices. In this paper, we propose a simple and flexible two-stream framework named Dual Super-Resolution Learning (DSRL) to effectively improve the segmentation accuracy without introducing extra computation costs. Specifically, the proposed method consists of three parts: Semantic Segmentation Super-Resolution (SSSR), Single Image Super-Resolution (SISR) and Feature Affinity (FA) module, which can keep high-resolution representations with low-resolution input while simultaneously reducing the model computation complexity. Moreover; it can be easily generalized to other tasks, e.g., human pose estimation. This simple yet effective method leads to strong representations and is evidenced by promising performance on both semantic segmentation and human pose estimation. Specifically, for semantic segmentation on CityScapes, we can achieve >= 2% higher mIoU with similar FLOPs, and keep the performance with 70% FLOPs. For human pose estimation, we can gain >= 2% mAP with the same FLOPs and maintain mAP with 30% fewer FLOPs. Code and models are available at https://github.com/wanglixilinx/DSRL.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,33,1,0,0,0,0,33,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504005,,
B,Zhi Qiao; Yu Zhou; Dongbao Yang; Yucan Zhou; Weiping Wang,,,,,"Qiao, Zhi/0000-0002-8942-5907",,,SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,13525,34,,10.1109/CVPR42600.2020.01354,,,,2020,2020,"Scene text recognition is a hot research topic in computer vision. Recently, many recognition methods based on the encoder-decoder framework have been proposed, and they can handle scene texts of perspective distortion and curve shape. Nevertheless, they still face lots of challenges like image blur, uneven illumination, and incomplete characters. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information. In this work, we propose a semantics enhanced encoder-decoder framework to robustly recognize low-quality scene texts. The semantic information is used both in the encoder module for supervision and in the decoder module for initializing. In particular, the state-of-the-art ASTER method is integrated into the proposed framework as an exemplar. Extensive experiments demonstrate that the proposed framework is more robust for low-quality text images, and achieves state-of-the-art results on several benchmark datasets. The source code will be available.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",33,0,0,0,0,0,33,,,,,978-1-7281-7168-5,INSPEC:19876416,,
J,"Song, Guorong; Song, Kechen; Yan, Yunhui",,,,"Song, Kechen/T-1896-2019","Song, Kechen/0000-0002-7636-3460",,,EDRNet: Encoder-Decoder Residual Network for Salient Object Detection of Strip Steel Surface Defects,,,,,,,,IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT,,,,69,12,,,9709,9719,,10.1109/TIM.2020.3002277,,,,DEC 2020,2020,"It is still a challenging task to detect the surface defects of strip steel due to its complex variations, including variable defect types, cluttered background, low contrast, and noise interference. The existing detection methods cannot effectively segment the defect objects from complex background and have poor real-time performance. To address these issues, we propose a novel saliency detection method based on Encoder-Decoder Residual network (EDRNet). In the encoder stage, we use a fully convolutional neural network to extract rich multilevel defect features and fuse the attention mechanism to accelerate the convergence of the model. Then in the decoder stage, we adopt the channels weighted block (CWB) and the residual decoder block (RDB) alternatively to integrate the spatial features of shallower layers and semantic features of deep layers and recover the predicted spatial saliency values step by step. Finally, we design the residual refinement structure with 1D filters (RRS_1D) to further optimize the coarse saliency map. Compared with the existing saliency detection methods, the deeply supervised EDRNet can accurately segment the complete defect objects with well-defined boundary and effectively filter out irrelevant background noise. The extensive experimental results prove that our method is consistently superior to the state-of-the-art methods with large margins and strong robustness, and the detection efficiency is at over 27 fps on a single GPU.",,,,,,,,,31,1,0,0,0,0,32,,,0018-9456,1557-9662,,WOS:000589255800038,,
J,"Zhang, Qian; Liu, Yeqi; Gong, Chuanyang; Chen, Yingyi; Yu, Huihui",,,,,,,,Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review,,,,,,,,SENSORS,,,,20,5,,,,,1520,10.3390/s20051520,,,,MAR 2020,2020,"Deep Learning (DL) is the state-of-the-art machine learning technology, which shows superior performance in computer vision, bioinformatics, natural language processing, and other areas. Especially as a modern image processing technology, DL has been successfully applied in various tasks, such as object detection, semantic segmentation, and scene analysis. However, with the increase of dense scenes in reality, due to severe occlusions, and small size of objects, the analysis of dense scenes becomes particularly challenging. To overcome these problems, DL recently has been increasingly applied to dense scenes and has begun to be used in dense agricultural scenes. The purpose of this review is to explore the applications of DL for dense scenes analysis in agriculture. In order to better elaborate the topic, we first describe the types of dense scenes in agriculture, as well as the challenges. Next, we introduce various popular deep neural networks used in these dense scenes. Then, the applications of these structures in various agricultural tasks are comprehensively introduced in this review, including recognition and classification, detection, counting and yield estimation. Finally, the surveyed DL applications, limitations and the future work for analysis of dense images in agriculture are summarized.",,,,,,,,,31,1,0,0,6,0,32,,,,1424-8220,,WOS:000525271500282,32164200,
J,"Wickstrom, Kristoffer; Kampffmeyer, Michael; Jenssen, Robert",,,,"Wickstrøm, Kristoffer/AAR-5964-2020","Wickstrøm, Kristoffer/0000-0003-1395-7154; Kampffmeyer, Michael/0000-0002-7699-0405",,,Uncertainty and interpretability in convolutional neural networks for semantic segmentation of colorectal polyps,,,,,,,,MEDICAL IMAGE ANALYSIS,,,,60,,,,,,101619,10.1016/j.media.2019.101619,,,,FEB 2020,2020,"Colorectal polyps are known to be potential precursors to colorectal cancer, which is one of the leading causes of cancer-related deaths on a global scale. Early detection and prevention of colorectal cancer is primarily enabled through manual screenings, where the intestines of a patient is visually examined. Such a procedure can be challenging and exhausting for the person performing the screening. This has resulted in numerous studies on designing automatic systems aimed at supporting physicians during the examination. Recently, such automatic systems have seen a significant improvement as a result of an increasing amount of publicly available colorectal imagery and advances in deep learning research for object image recognition. Specifically, decision support systems based on Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance on both detection and segmentation of colorectal polyps. However, CNN-based models need to not only be precise in order to be helpful in a medical context. In addition, interpretability and uncertainty in predictions must be well understood. In this paper, we develop and evaluate recent advances in uncertainty estimation and model interpretability in the context of semantic segmentation of polyps from colonoscopy images. Furthermore, we propose a novel method for estimating the uncertainty associated with important features in the input and demonstrate how interpretability and uncertainty can be modeled in DSSs for semantic segmentation of colorectal polyps. Results indicate that deep models are utilizing the shape and edge information of polyps to make their prediction. Moreover, inaccurate predictions show a higher degree of uncertainty compared to precise predictions. (C) 2019 The Authors. Published by Elsevier B.V.",,,,,,,,,32,0,0,0,8,0,32,,,1361-8415,1361-8423,,WOS:000527917100026,31810005,
J,"Wang, Caiyong; Muhammad, Jawad; Wang, Yunlong; He, Zhaofeng; Sun, Zhenan",,,,,"Wang, Caiyong/0000-0001-7341-3904",,,Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention Network for Non-Cooperative Iris Recognition,,,,,,,,IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY,,,,15,,,,2944,2959,,10.1109/TIFS.2020.2980791,,,,2020,2020,"Iris images captured in non-cooperative environments often suffer from adverse noise, which challenges many existing iris segmentation methods. To address this problem, this paper proposes a high-efficiency deep learning based iris segmentation approach, named IrisParseNet. Different from many previous CNN-based iris segmentation methods, which only focus on predicting accurate iris masks by following popular semantic segmentation frameworks, the proposed approach is a complete iris segmentation solution, i.e., iris mask and parameterized inner and outer iris boundaries are jointly achieved by actively modeling them into a unified multi-task network. Moreover, an elaborately designed attention module is incorporated into it to improve the segmentation performance. To train and evaluate the proposed approach, we manually label three representative and challenging iris databases, i.e., CASIA.v4-distance, UBIRIS.v2, and MICHE-I, which involve multiple illumination (NIR, VIS) and imaging sensors (long-range and mobile iris cameras), along with various types of noises. Additionally, several unified evaluation protocols are built for fair comparisons. Extensive experiments are conducted on these newly annotated databases, and results show that the proposed approach achieves state-of-the-art performance on various benchmarks. Further, as a general drop-in replacement, the proposed iris segmentation method can be used for any iris recognition methodology, and would significantly improve the performance of non-cooperative iris recognition.",,,,,,,,,31,1,0,0,3,0,32,,,1556-6013,1556-6021,,WOS:000524505300006,,
J,"Wen, Shiping; Dong, Minghui; Yang, Yin; Zhou, Pan; Huang, Tingwen; Chen, Yiran",,,,"Huang, Tingwen/K-1367-2013","Huang, Tingwen/0000-0001-9610-846X; Yang, Yin/0000-0002-0549-3882; Wen, Shiping/0000-0001-8077-7001",,,End-to-End Detection-Segmentation System for Face Labeling,,,,,,,,IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE,,,,5,3,,,457,467,,10.1109/TETCI.2019.2947319,,,,JUN 2021,2021,"In this paper, we propose an end-to-end detection-segmentation system to implement detailed face labeling. Fully convolutional networks (FCN) has become the mainstream algorithm in the field of semantic segmentation due to the state-of-the-art performance. However, a general FCN usually produces smooth and homogeneous results. Moreover, when semantic category is extremely unbalanced in samples such as face labeling problem, features for some categories cannot be well explored by FCN. To alleviate these problems, a face image is firstly encoded to multi-level feature maps by a pyramid FCN, then features of different facial components are extracted separately according to the bounding box provided by a one-stage detection head. Three class-specific sub-networks are employed to process the extracted features to obtain the respective segmentation results. The skin-hair region can be decoded directly from the back end of the pyramid FCN. Finally, the overall segmentation result is obtained by combining different branches. Moreover, the proposed method trained on a single-face labeled dataset, can be directly used to implement detailed multi-face labeling tasks without any network modification and additional module or data. The overall structure can be trained in an end-to-end manner while maintaining a small network size (12 MB). Experiments show that the proposed method can generate more accurate (single or multi) face labeling results comparing with previous works and gets the state-of-the-art results in HELEN face dataset.",,,,,,,,,31,0,0,0,2,0,31,,,2471-285X,,,WOS:000658320500011,,
J,"Wei, Yao; Zhang, Kai; Ji, Shunping",,,,,"Ji, Shunping/0000-0002-3088-1481; zhang, kai/0000-0003-1129-9944; Wei, Yao/0000-0003-4672-4584",,,Simultaneous Road Surface and Centerline Extraction From Large-Scale Remote Sensing Images Using CNN-Based Segmentation and Tracing,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,12,,,8919,8931,,10.1109/TGRS.2020.2991733,,,,DEC 2020,2020,"Accurate and up-to-date road maps are of great importance in a wide range of applications. Unfortunately, automatic road extraction from high-resolution remote sensing images remains challenging due to the occlusion of trees and buildings, discriminability of roads, and complex backgrounds. To address these problems, especially road connectivity and completeness, in this article, we introduce a novel deep learning-based multistage framework to accurately extract the road surface and road centerline simultaneously. Our framework consists of three steps: boosting segmentation, multiple starting points tracing, and fusion. The initial road surface segmentation is achieved with a fully convolutional network (FCN), after which another lighter FCN is applied several times to boost the accuracy and connectivity of the initial segmentation. In the multiple starting points tracing step, the starting points are automatically generated by extracting the road intersections of the segmentation results, which then are utilized to track consecutive and complete road networks through an iterative search strategy embedded in a convolutional neural network (CNN). The fusion step aggregates the semantic and topological information of road networks by combining the segmentation and tracing results to produce the final and refined road segmentation and centerline maps. We evaluated our method utilizing three data sets covering various road situations in more than 40 cities around the world. The results demonstrate the superior performance of our proposed framework. Specifically, our method's performance exceeded the other methods by 7% and 40% for the connectivity indicator for road surface segmentation and for the completeness indicator for centerline extraction, respectively.",,,,,,,,,30,1,0,0,1,0,31,,,0196-2892,1558-0644,,WOS:000594389800050,,
J,"Feng, Mingtao; Zhang, Liang; Lin, Xuefei; Gilani, Syed Zulqarnain; Mian, Ajmal",,,,"Gilani, Syed Zulqarnain/AAJ-8482-2021","Gilani, Syed Zulqarnain/0000-0002-7448-2327; Mian, Ajmal/0000-0002-5206-3842; Zhang, Liang/0000-0003-4331-5830; Feng, Mingtao/0000-0003-0384-3743",,,Point attention network for semantic segmentation of 3D point clouds,,,,,,,,PATTERN RECOGNITION,,,,107,,,,,,107446,10.1016/j.patcog.2020.107446,,,,NOV 2020,2020,"Convolutional Neural Networks (CNNs) have performed extremely well on data represented by regularly arranged grids such as images. However, directly leveraging the classic convolution kernels or parameter sharing mechanisms on sparse 3D point clouds is inefficient due to their irregular and unordered nature. We propose a point attention network that learns rich local shape features and their contextual correlations for 3D point cloud semantic segmentation. Since the geometric distribution of the neighboring points is invariant to the point ordering, we propose a Local Attention-Edge Convolution (LAE-Conv) to construct a local graph based on the neighborhood points searched in multi-directions. We assign attention coefficients to each edge and then aggregate the point features as a weighted sum of its neighbors. The learned LAE-Conv layer features are then given to a point-wise spatial attention module to generate an interdependency matrix of all points regardless of their distances, which captures long-range spatial contextual features contributing to more precise semantic information. The proposed point attention network consists of an encoder and decoder which, together with the LAE-Conv layers and the point-wise spatial attention modules, make it an end-to-end trainable network for predicting dense labels for 3D point cloud segmentation. Experiments on challenging benchmarks of 3D point clouds show that our algorithm can perform at par or better than the existing state of the art methods. (C) 2020 Elsevier Ltd. All rights reserved.",,,,,,,,,30,2,0,0,1,0,31,,,0031-3203,1873-5142,,WOS:000552866000016,,
J,"Hoeser, Thorsten; Bachofer, Felix; Kuenzer, Claudia",,,,"Bachofer, Felix/AAH-1648-2020","Bachofer, Felix/0000-0001-6181-0187; Hoser, Thorsten/0000-0002-7179-3664",,,Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part II: Applications,,,,,,,,REMOTE SENSING,,,,12,18,,,,,3053,10.3390/rs12183053,,,,SEP 2020,2020,"In Earth observation (EO), large-scale land-surface dynamics are traditionally analyzed by investigating aggregated classes. The increase in data with a very high spatial resolution enables investigations on a fine-grained feature level which can help us to better understand the dynamics of land surfaces by taking object dynamics into account. To extract fine-grained features and objects, the most popular deep-learning model for image analysis is commonly used: the convolutional neural network (CNN). In this review, we provide a comprehensive overview of the impact of deep learning on EO applications by reviewing 429 studies on image segmentation and object detection with CNNs. We extensively examine the spatial distribution of study sites, employed sensors, used datasets and CNN architectures, and give a thorough overview of applications in EO which used CNNs. Our main finding is that CNNs are in an advanced transition phase from computer vision to EO. Upon this, we argue that in the near future, investigations which analyze object dynamics with CNNs will have a significant impact on EO research. With a focus on EO applications in thisPart II, we complete the methodological review provided inPart I.",,,,,,,,,31,0,1,0,3,0,31,,,,2072-4292,,WOS:000580006700001,,
J,"Hasan, Md Kamrul; Dahal, Lavsen; Samarakoon, Prasad N.; Tushar, Fakrul Islam; Marti, Robert",,,,"Hasan, Kamrul/AAP-2259-2021","Hasan, Kamrul/0000-0003-1292-4350; TUSHAR, FAKRUL ISLAM/0000-0001-7180-563X",,,DSNet: Automatic dermoscopic skin lesion segmentation,,,,,,,,COMPUTERS IN BIOLOGY AND MEDICINE,,,,120,,,,,,103738,10.1016/j.compbiomed.2020.103738,,,,MAY 2020,2020,"Background and Objective: Automatic segmentation of skin lesions is considered a crucial step in Computer-aided Diagnosis (CAD) systems for melanoma detection. Despite its significance, skin lesion segmentation remains an unsolved challenge due to their variability in color, texture, and shapes and indistinguishable boundaries.Methods: Through this study, we present a new and automatic semantic segmentation network for robust skin lesion segmentation named Dermoscopic Skin Network (DSNet). In order to reduce the number of parameters to make the network lightweight, we used a depth-wise separable convolution in lieu of standard convolution to project the learned discriminating features onto the pixel space at different stages of the encoder. Additionally, we implemented both a U-Net and a Fully Convolutional Network (FCN8s) to compare against the proposed DSNet.Results: We evaluate our proposed model on two publicly available datasets, namely ISIC-2017(1) and PH2(2). The obtained mean Intersection over Union (mIoU) is 77.5% and 87.0% respectively for ISIC-2017 and PH2 datasets which outperformed the ISIC-2017 challenge winner by 1.0% with respect to mIoU. Our proposed network also outperformed U-Net and FCN8s respectively by 3.6% and 6.8% with respect to mIoU on the ISIC-2017 dataset.Conclusion: Our network for skin lesion segmentation outperforms the other methods discussed in the article and is able to provide better-segmented masks on two different test datasets which can lead to better performance in melanoma detection. Our trained model along with the source code and predicted masks are made publicly available(3).",,,,,,,,,31,0,0,0,0,0,31,,,0010-4825,1879-0534,,WOS:000532824300026,32421644,
J,"Lin, Guosheng; Liu, Fayao; Milan, Anton; Shen, Chunhua; Reid, Ian",,,,"Lin, Guosheng/Q-4024-2017","Lin, Guosheng/0000-0002-0329-7458; Reid, Ian/0000-0001-7790-6423",,,RefineNet: Multi-Path Refinement Networks for Dense Prediction,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,42,5,,,1228,1242,,10.1109/TPAMI.2019.2893630,,,,MAY 1 2020,2020,"Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense prediction problems such as semantic segmentation and depth estimation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments on semantic segmentation which is a dense classification problem and achieve good performance on seven public datasets. We further apply our method for depth estimation and demonstrate the effectiveness of our method on dense regression problems.",,,,,,,,,30,2,0,0,0,0,31,,,0162-8828,1939-3539,,WOS:000523685800016,30668461,
J,"Cao, Kaili; Zhang, Xiaoli",,,,,"Zhang, Xiaoli/0000-0001-7443-1557",,,An Improved Res-UNet Model for Tree Species Classification Using Airborne High-Resolution Images,,,,,,,,REMOTE SENSING,,,,12,7,,,,,1128,10.3390/rs12071128,,,,APR 2020,2020,"Tree species classification is important for the management and sustainable development of forest resources. Traditional object-oriented tree species classification methods, such as support vector machines, require manual feature selection and generally low accuracy, whereas deep learning technology can automatically extract image features to achieve end-to-end classification. Therefore, a tree classification method based on deep learning is proposed in this study. This method combines the semantic segmentation network U-Net and the feature extraction network ResNet into an improved Res-UNet network, where the convolutional layer of the U-Net network is represented by the residual unit of ResNet, and linear interpolation is used instead of deconvolution in each upsampling layer. At the output of the network, conditional random fields are used for post-processing. This network model is used to perform classification experiments on airborne orthophotos of Nanning Gaofeng Forest Farm in Guangxi, China. The results are then compared with those of U-Net and ResNet networks. The proposed method exhibits higher classification accuracy with an overall classification accuracy of 87%. Thus, the proposed model can effectively implement forest tree species classification and provide new opportunities for tree species classification in southern China.",,,,,,,,,31,0,0,0,4,0,31,,,,2072-4292,,WOS:000537709600078,,
B,Myeongjin Kim; Hyeran Byun,,,,,,,,Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12972,81,,10.1109/CVPR42600.2020.01299,,,,2020,2020,"Since annotating pixel-level labels for semantic segmentation is laborious, leveraging synthetic data is an attractive solution. However, due to the domain gap between synthetic domain and real domain, it is challenging for a model trained with synthetic data to generalize to real data. In this paper, considering the fundamental difference between the two domains as the texture, we propose a method to adapt to the target domain's texture. First, we diversity the texture of synthetic images using a style transfer algorithm. The various textures of generated images prevent a segmentation model from overfitting to one specific (synthetic) texture. Then, we fine-tune the model with self-training to get direct supervision of the target texture. Our results achieve state-of-the-art performance and we analyze the properties of the model trained on the stylized dataset with extensive experiments.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",31,0,0,0,2,0,31,,,,,978-1-7281-7168-5,INSPEC:19874168,,
J,"Saood, Adnan; Hatem, Iyad",,,,"hatem, iyad/ABD-4255-2021","hatem, iyad/0000-0002-3815-166X; Saood, Adnan/0000-0003-1152-4531",,,COVID-19 lung CT image segmentation using deep learning methods: U-Net versus SegNet,,,,,,,,BMC MEDICAL IMAGING,,,,21,1,,,,,19,10.1186/s12880-020-00529-5,,,,FEB 9 2021,2021,"Background Currently, there is an urgent need for efficient tools to assess the diagnosis of COVID-19 patients. In this paper, we present feasible solutions for detecting and labeling infected tissues on CT lung images of such patients. Two structurally-different deep learning techniques, SegNet and U-NET, are investigated for semantically segmenting infected tissue regions in CT lung images. Methods We propose to use two known deep learning networks, SegNet and U-NET, for image tissue classification. SegNet is characterized as a scene segmentation network and U-NET as a medical segmentation tool. Both networks were exploited as binary segmentors to discriminate between infected and healthy lung tissue, also as multi-class segmentors to learn the infection type on the lung. Each network is trained using seventy-two data images, validated on ten images, and tested against the left eighteen images. Several statistical scores are calculated for the results and tabulated accordingly. Results The results show the superior ability of SegNet in classifying infected/non-infected tissues compared to the other methods (with 0.95 mean accuracy), while the U-NET shows better results as a multi-class segmentor (with 0.91 mean accuracy). Conclusion Semantically segmenting CT scan images of COVID-19 patients is a crucial goal because it would not only assist in disease diagnosis, also help in quantifying the severity of the illness, and hence, prioritize the population treatment accordingly. We propose computer-based techniques that prove to be reliable as detectors for infected tissue in lung CT scans. The availability of such a method in today's pandemic would help automate, prioritize, fasten, and broaden the treatment of COVID-19 patients globally.",,,,,,,,,29,1,0,0,5,0,30,,,1471-2342,,,WOS:000616328400001,33557772,
J,"Almotairi, Sultan; Kareem, Ghada; Aouf, Mohamed; Almutairi, Badr; Salem, Mohammed A-M",,,,"kareem, ghada/AAD-8734-2021; Almotairi/AAX-8922-2020; Salem, Mohammed A.-M/H-1922-2012","Salem, Mohammed A.-M/0000-0003-1489-9830; Almutairi, Badr/0000-0003-1187-6908; gameel, ghada/0000-0003-3954-1456",,,Liver Tumor Segmentation in CT Scans Using Modified SegNet,,,,,,,,SENSORS,,,,20,5,,,,,1516,10.3390/s20051516,,,,MAR 2020,2020,"The main cause of death related to cancer worldwide is from hepatic cancer. Detection of hepatic cancer early using computed tomography (CT) could prevent millions of patients' death every year. However, reading hundreds or even tens of those CT scans is an enormous burden for radiologists. Therefore, there is an immediate need is to read, detect, and evaluate CT scans automatically, quickly, and accurately. However, liver segmentation and extraction from the CT scans is a bottleneck for any system, and is still a challenging problem. In this work, a deep learning-based technique that was proposed for semantic pixel-wise classification of road scenes is adopted and modified to fit liver CT segmentation and classification. The architecture of the deep convolutional encoder-decoder is named SegNet, and consists of a hierarchical correspondence of encode-decoder layers. The proposed architecture was tested on a standard dataset for liver CT scans and achieved tumor accuracy of up to 99.9% in the training phase.",,,,,,,,,29,2,0,0,2,0,30,,,,1424-8220,,WOS:000525271500278,32164153,
J,"Bi, Haixia; Xu, Lin; Cao, Xiangyong; Xue, Yong; Xu, Zongben",,,,"Xu, Lin/D-1991-2015","Xu, Lin/0000-0003-4373-0591",,,Polarimetric SAR Image Semantic Segmentation With 3D Discrete Wavelet Transform and Markov Random Field,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,6601,6614,,10.1109/TIP.2020.2992177,,,,2020,2020,"Polarimetric synthetic aperture radar (PolSAR) image segmentation is currently of great importance in image processing for remote sensing applications. However, it is a challenging task due to two main reasons. Firstly, the label information is difficult to acquire due to high annotation costs. Secondly, the speckle effect embedded in the PolSAR imaging process remarkably degrades the segmentation performance. To address these two issues, we present a contextual PolSAR image semantic segmentation method in this paper. With a newly defined channel-wise consistent feature set as input, the three-dimensional discrete wavelet transform (3D-DWT) technique is employed to extract discriminative multi-scale features that are robust to speckle noise. Then Markov random field (MRF) is further applied to enforce label smoothness spatially during segmentation. By simultaneously utilizing 3D-DWT features and MRF priors for the first time, contextual information is fully integrated during the segmentation to ensure accurate and smooth segmentation. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on three real benchmark PolSAR image data sets. Experimental results indicate that the proposed method achieves promising segmentation accuracy and preferable spatial consistency using a minimal number of labeled pixels.",,,,,,,,,29,1,0,0,2,0,30,,,1057-7149,1941-0042,,WOS:000545739000002,,
J,"Jing, Longlong; Chen, Yucheng; Tian, Yingli",,,,,,,,Coarse-to-Fine Semantic Segmentation From Image-Level Labels,,,,,,,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,29,,,,225,236,,10.1109/TIP.2019.2926748,,,,2020,2020,"Deep neural network-based semantic segmentation generally requires large-scale cost extensive annotations for training to obtain better performance. To avoid pixel-wise segmentation annotations that are needed for most methods, recently some researchers attempted to use object-level labels (e.g., bounding boxes) or image-level labels (e.g., image categories). In this paper, we propose a novel recursive coarse-to-fine semantic segmentation framework based on only image-level category labels. For each image, an initial coarse mask is first generated by a convolutional neural network-based unsupervised foreground segmentation model and then is enhanced by a graph model. The enhanced coarse mask is fed to a fully convolutional neural network to be recursively refined. Unlike the existing image-level label-based semantic segmentation methods, which require labeling of all categories for images that contain multiple types of objects, our framework only needs one label for each image and can handle images that contain multi-category objects. Only trained on ImageNet, our framework achieves comparable performance on the PASCAL VOC dataset with other image-level label-based state-of-the-art methods of semantic segmentation. Furthermore, our framework can be easily extended to foreground object segmentation task and achieves comparable performance with the state-of-the-art supervised methods on the Internet object dataset.",,,,,,,,,29,1,0,0,2,0,30,,,1057-7149,1941-0042,,WOS:000487069300017,31329556,
J,"Zheng, Zhedong; Yang, Yi",,,,"Yang, Yi/B-9273-2017; Zheng, Zhedong/R-5314-2019","Yang, Yi/0000-0002-0512-880X; Zheng, Zhedong/0000-0002-2434-9050",,,Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation,,,,,,,,INTERNATIONAL JOURNAL OF COMPUTER VISION,,,,129,4,,,1106,1120,,10.1007/s11263-020-01395-y,,JAN 2021,,APR 2021,2021,"This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes -> Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.",,,,,,,,,29,0,0,0,0,0,29,,,0920-5691,1573-1405,,WOS:000605541100005,,
J,"Huang, Li; He, Meiling; Tan, Chong; Jiang, Du; Li, Gongfa; Yu, Hui",,,,"cao, xiaoxiang/AAR-9291-2021","li, gongfa/0000-0002-2695-2742; Yu, Hui/0000-0002-7655-9228",,,Jointly network image processing: multi-task image semantic segmentation of indoor scene based on CNN,,,,,,,,IET IMAGE PROCESSING,,,,14,15,,,3689,3697,,10.1049/iet-ipr.2020.0088,,,,DEC 15 2020,2020,"Image semantic segmentation has always been a research hotspot in the field of robots. Its purpose is to assign different semantic category labels to objects by segmenting different objects. However, in practical applications, in addition to knowing the semantic category information of objects, robots also need to know the position information of objects to complete more complex visual tasks. Aiming at a complex indoor environment, this study designs an image semantic segmentation network framework of joint target detection. Using the parallel operation of adding semantic segmentation branches to the target detection network, it innovatively implements multi-vision task combining object classification, detection and semantic segmentation. By designing a new loss function, adjusting the training using the idea of transfer learning, and finally verifying it on the self-built indoor scene data set, the experiment proves that the method in this study is feasible and effective, and has good robustness.",,,,,,,,,29,0,0,0,12,0,29,,,1751-9659,1751-9667,,WOS:000626224500009,,
J,"Martins, Vitor S.; Kaleita, Amy L.; Gelder, Brian K.; da Silveira, Hilton L. F.; Abe, Camila A.",,,,,"Martins, Vitor Souza/0000-0003-3802-0368",,,Exploring multiscale object-based convolutional neural network (multi-OCNN) for remote sensing image classification at high spatial resolution,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,168,,,,56,73,,10.1016/j.isprsjprs.2020.08.004,,,,OCT 2020,2020,"Convolutional Neural Network (CNN) has been increasingly used for land cover mapping of remotely sensed imagery. However, large-area classification using traditional CNN is computationally expensive and produces coarse maps using a sliding window approach. To address this problem, object-based CNN (OCNN) becomes an alternative solution to improve classification performance. However, previous studies were mainly focused on urban areas or small scenes, and implementation of OCNN method is still needed for large-area classification over heterogeneous landscape. Additionally, the massive labeling of segmented objects requires a practical approach for less computation, including object analysis and multiple CNNs. This study presents a new multiscale OCNN (multi-OCNN) framework for large-scale land cover classification at 1-m resolution over 145,740 km(2). Our approach consists of three main steps: (i) image segmentation, (ii) object analysis with skeleton-based algorithm, and (iii) application of multiple CNNs for final classification. Also, we developed a large benchmark dataset, called IowaNet, with 1 million labeled images and 10 classes. In our approach, multiscale CNNs were trained to capture the best contextual information during the semantic labeling of objects. Meanwhile, skeletonization algorithm provided morphological representation (medial axis) of objects to support the selection of convolutional locations for CNN predictions. In general, proposed multi-OCNN presented better classification accuracy (overall accuracy similar to 87.2%) compared to traditional patch-based CNN (81.6%) and fixed-input OCNN (82%). In addition, the results showed that this framework is 8.1 and 111.5 times faster than traditional pixel-wise CNN16 or CNN256, respectively. Multiple CNNs and object analysis have proved to be essential for accurate and fast classification. While multi-OCNN produced a high-level of spatial details in the land cover product, misclassification was observed for some classes, such as road versus buildings or shadow versus lake. Despite these minor drawbacks, our results also demonstrated the benefits of IowaNet training dataset in the model performance; overfitting process reduces as the number of samples increases. The limitations of multi-OCNN are partially explained by segmentation quality and limited number of spectral bands in the aerial data. With the advance of deep learning methods, this study supports the claim of multi-OCNN benefits for operational large-scale land cover product at 1-m resolution.",,,,,,,,,29,0,0,0,3,0,29,,,0924-2716,1872-8235,,WOS:000567932300005,,
J,"Shang, Ronghua; Zhang, Jiyu; Jiao, Licheng; Li, Yangyang; Marturi, Naresh; Stolkin, Rustam",,,,"Marturi, Naresh/AAS-6956-2021","Marturi, Naresh/0000-0002-0159-167X; Jiao, Licheng/0000-0003-3354-9617",,,Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images,,,,,,,,REMOTE SENSING,,,,12,5,,,,,872,10.3390/rs12050872,,,,MAR 2020,2020,"Semantic segmentation of high-resolution remote sensing images is highly challenging due to the presence of a complicated background, irregular target shapes, and similarities in the appearance of multiple target categories. Most of the existing segmentation methods that rely only on simple fusion of the extracted multi-scale features often fail to provide satisfactory results when there is a large difference in the target sizes. Handling this problem through multi-scale context extraction and efficient fusion of multi-scale features, in this paper we present an end-to-end multi-scale adaptive feature fusion network (MANet) for semantic segmentation in remote sensing images. It is a coding and decoding structure that includes a multi-scale context extraction module (MCM) and an adaptive fusion module (AFM). The MCM employs two layers of atrous convolutions with different dilatation rates and global average pooling to extract context information at multiple scales in parallel. MANet embeds the channel attention mechanism to fuse semantic features. The high- and low-level semantic information are concatenated to generate global features via global average pooling. These global features are used as channel weights to acquire adaptive weight information of each channel by the fully connected layer. To accomplish an efficient fusion, these tuned weights are applied to the fused features. Performance of the proposed method has been evaluated by comparing it with six other state-of-the-art networks: fully convolutional networks (FCN), U-net, UZ1, Light-weight RefineNet, DeepLabv3+, and APPD. Experiments performed using the publicly available Potsdam and Vaihingen datasets show that the proposed MANet significantly outperforms the other existing networks, with overall accuracy reaching 89.4% and 88.2%, respectively and with average of F1 reaching 90.4% and 86.7% respectively.",,,,,,,,,28,1,0,0,1,0,29,,,,2072-4292,,WOS:000531559300127,,
J,"Stan, Tiberiu; Thompson, Zachary T.; Voorhees, Peter W.",,,,"Voorhees, Peter/B-6700-2009",,,,Optimizing convolutional neural networks to perform semantic segmentation on large materials imaging datasets: X-ray tomography and serial sectioning,,,,,,,,MATERIALS CHARACTERIZATION,,,,160,,,,,,110119,10.1016/j.matchar.2020.110119,,,,FEB 2020,2020,"Machine learning was used to segment large materials science datasets resulting from synchrotron-based x-ray computed tomography (XCT) images of dendrite growth, and serial sectioning (SS) images of dendrite coarsening. Both neural networks (NNs) yielded quantitatively more accurate outputs than conventional segmentation techniques using only 30 XCT or 6 SS training images. We show that performance can be improved if NNs are trained using a large number of small images that are sampled from the fixed amount of training data. The optimal image size and number of training images was identified for the XCT and SS datasets. NN transferability was also tested by applying the highest performing XCT and SS NNs to related datasets. While the initial segmentations were successful, applying simple transformations to the raw images further improved NN performance. These results show the great predictive ability and promising future of using machine learning for segmentation of large materials science datasets.",,,,,,,,,29,0,0,0,2,0,29,,,1044-5803,1873-4189,,WOS:000519655300044,,
J,"Bosilj, Petra; Aptoula, Erchan; Duckett, Tom; Cielniak, Grzegorz",,,,"Aptoula, Erchan/AAI-1070-2020","Aptoula, Erchan/0000-0001-6168-2883; Cielniak, Grzegorz/0000-0002-6299-8465; Bosilj, Petra/0000-0001-9640-9828",,,Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture,,,,,,,,JOURNAL OF FIELD ROBOTICS,,,,37,1,,,7,19,,10.1002/rob.21869,,,,JAN 2020,2020,"Agricultural robots rely on semantic segmentation for distinguishing between crops and weeds to perform selective treatments and increase yield and crop health while reducing the amount of chemicals used. Deep-learning approaches have recently achieved both excellent classification performance and real-time execution. However, these techniques also rely on a large amount of training data, requiring a substantial labeling effort, both of which are scarce in precision agriculture. Additional design efforts are required to achieve commercially viable performance levels under varying environmental conditions and crop growth stages. In this paper, we explore the role of knowledge transfer between deep-learning-based classifiers for different crop types, with the goal of reducing the retraining time and labeling efforts required for a new crop. We examine the classification performance on three datasets with different crop types and containing a variety of weeds and compare the performance and retraining efforts required when using data labeled at pixel level with partially labeled data obtained through a less time-consuming procedure of annotating the segmentation output. We show that transfer learning between different crop types is possible and reduces training times for up to 80%. Furthermore, we show that even when the data used for retraining are imperfectly annotated, the classification performance is within 2% of that of networks trained with laboriously annotated pixel-precision data.",,,,,,,,,28,1,0,0,13,0,29,,,1556-4959,1556-4967,,WOS:000573005200002,,
C,"Dat Huynh; Elhamifar, Ehsan",,,IEEE,"huynh, dat/AAB-7746-2022","huynh, dat/0000-0002-6349-2597",,,Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4482,4492,,10.1109/CVPR42600.2020.00454,,,,2020,2020,"We address the problem of fine-grained generalized zero-shot recognition of visually similar classes without training images for some classes. We propose a dense attribute-based attention mechanism that for each attribute focuses on the most relevant image regions, obtaining attribute-based features. Instead of aligning a global feature vector of an image with its associated class semantic vector, we propose an attribute embedding technique that aligns each attribute-based feature with its attribute semantic vector. Hence, we compute a vector of attribute scores, for the presence of each attribute in an image, whose similarity with the true class semantic vector is maximized. Moreover, we adjust each attribute score using an attention mechanism over attributes to better capture the discriminative power of different attributes. To tackle the challenge of bias towards seen classes during testing, we propose a new self-calibration loss that adjusts the probability of unseen classes to account for the training bias. We conduct experiments on three popular datasets of CUB, SUN and AWA2 as well as the large-scale DeepFashion dataset, showing that our model significantly improves the state of the art.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,28,1,0,0,2,0,29,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504076,,
B,Hui Chen; Guiguang Ding; Xudong Liu; Zijia Lin; Ji Liu; Jungong Han,,,,,,,,IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12652,60,,10.1109/CVPR42600.2020.01267,,,,2020,2020,"Enabling bi-directional retrieval of images and texts is important for understanding the correspondence between vision and language. Existing methods leverage the attention mechanism to explore such correspondence in a fine-grained manner. However, most of them consider all semantics equally and thus align them uniformly, regardless of their diverse complexities. In fact, semantics are diverse (i.e. involving different kinds of semantic concepts), and humans usually follow a latent structure to combine them into understandable languages. It may be difficult to optimally capture such sophisticated correspondences in existing methods. In this paper, to address such a deficiency, we propose an Iterative Matching with Recurrent Attention Memory (IMRAM) method, in which correspondences between images and texts are captured with multiple steps of alignments. Specifically, we introduce an iterative matching scheme to explore such fine-grained correspondence progressively. A memory distillation unit is used to refine alignment knowledge from early steps to later ones. Experiment results on three benchmark datasets, i.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves state-of-the-art performance, well demonstrating its effectiveness. Experiments on a practical business advertisement dataset, named KWAI-AD, further validates the applicability of our method in practical scenarios.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",29,0,0,0,0,0,29,,,,,978-1-7281-7168-5,INSPEC:19874454,,
J,"Liu, Junfu; Chen, Keming; Xu, Guangluan; Sun, Xian; Yan, Menglong; Diao, Wenhui; Han, Hongzhe",,,,,"Liu, Junfu/0000-0002-7316-4562; Sun, Xian/0000-0002-0038-9816",,,Convolutional Neural Network-Based Transfer Learning for Optical Aerial Images Change Detection,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,17,1,,,127,131,,10.1109/LGRS.2019.2916601,,,,JAN 2020,2020,"Considering the lack of labeled training data sets for the supervised change detection task, in this letter, we try to relieve this problem by proposing a convolutional neural network (CNN)-based change detection method with a newly designed loss function to achieve transfer learning among different data sets. To reach this goal, we first pretrain a U-Net model on an open source data set by taking advantages of the relatively sufficient training data used for the supervised semantic segmentation task. Then, we minimize a skillfully designed loss function to combine the high-level features extracted from the pretrained model and the semantic information contained in the change detection data set, by which a transfer learning is achieved. Third, we compute the distance between the feature vectors obtained from the above step and produce a difference map. Finally, a simple clustering method used on the difference map can even obtain satisfied change map. Experiments carried out on typical optical aerial image data sets validate that the proposed approach compares favorably to the state-of-the-art unsupervised methods.",,,,,,,,,28,1,0,0,0,0,29,,,1545-598X,1558-0571,,WOS:000505528400026,,
J,"Ruiz-Santaquiteria, Jesus; Bueno, Gloria; Deniz, Oscar; Vallez, Noelia; Cristobal, Gabriel",,,,"Vallez, Noelia/L-3106-2015; Ruiz-Santaquiteria Alegre, Jesus/B-2180-2019; Bueno, Gloria/K-6286-2014","Vallez, Noelia/0000-0002-5092-8275; Ruiz-Santaquiteria Alegre, Jesus/0000-0003-1454-7624; Deniz, Oscar/0000-0002-0841-4131; Bueno, Gloria/0000-0002-7345-4869",,,Semantic versus instance segmentation in microscopic algae detection,,,,,,,,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,,,,87,,,,,,103271,10.1016/j.engappai.2019.103271,,,,JAN 2020,2020,"Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading.",,,,,,,,,28,1,0,0,6,0,29,,,0952-1976,1873-6769,,WOS:000506715100016,,
B,Shaobo Min; Hantao Yao; Hongtao Xie; Chaoqun Wang; Zheng-Jun Zha; Yongdong Zhang,,,,,,,,Domain-Aware Visual Bias Eliminating for Generalized Zero-Shot Learning,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings,,,,,,,,12661,70,,10.1109/CVPR42600.2020.01268,,,,2020,2020,"Generalized zero-shot learning aims to recognize images from seen and unseen domains. Recent methods focus on learning a unified semantic-aligned visual representation to transfer knowledge between two domains, while ignoring the effect of semantic-free visual representation in alleviating the biased recognition problem. In this paper, we propose a novel Domain-aware Visual Bias Eliminating (DVBE) network that constructs two complementary visual representations, i.e., semantic-free and semantic-aligned, to treat seen and unseen domains separately. Specifically, we explore cross-attentive second-order visual statistics to compact the semantic-free representation, and design an adaptive margin Softmax to maximize inter-class divergences. Thus, the semantic-free representation becomes discriminative enough to not only predict seen class accurately but also filter out unseen images, i.e., domain detection, based on the predicted class entropy. For unseen images, we automatically search an optimal semantic-visual alignment architecture, rather than manual designs, to predict unseen classes. With accurate domain detection, the biased recognition problem towards the seen domain is significantly reduced. Experiments on five benchmarks for classification and segmentation show that DVBE outperforms existing methods by averaged 5.7% improvement.",,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 202013-19 June 2020,,"Seattle, WA, USASeattle, WA, USA",29,0,0,0,2,0,29,,,,,978-1-7281-7168-5,INSPEC:19874437,,
J,"Li, Ying; Ma, Lingfei; Zhong, Zilong; Liu, Fei; Chapman, Michael A.; Cao, Dongpu; Li, Jonathan",,,,"Li, Jonathan/AAA-7712-2021","Zhong, Zilong/0000-0003-0104-9116; Ma, Lingfei/0000-0001-8893-9693; LI, Jonathan/0000-0001-7899-0049",,,Deep Learning for LiDAR Point Clouds in Autonomous Driving: A Review,,,,,,,,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,,,32,8,,,3412,3432,,10.1109/TNNLS.2020.3015992,,,,AUG 2021,2021,"Recently, the advancement of deep learning (DL) in discriminative feature learning from 3-D LiDAR data has led to rapid development in the field of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3-D point clouds are a challenging and tedious task. In this article, we provide a systematic review of existing compelling DL architectures applied in LiDAR point clouds, detailing for specific tasks in autonomous driving, such as segmentation, detection, and classification. Although several published research articles focus on specific topics in computer vision for autonomous vehicles, to date, no general survey on DL applied in LiDAR point clouds for autonomous vehicles exists. Thus, the goal of this article is to narrow the gap in this topic. More than 140 key contributions in the recent five years are summarized in this survey, including the milestone 3-D deep architectures, the remarkable DL applications in 3-D semantic segmentation, object detection, and classification; specific data sets, evaluation metrics, and the state-of-the-art performance. Finally, we conclude the remaining challenges and future researches.",,,,,,,,,28,0,0,0,1,0,28,,,2162-237X,2162-2388,,WOS:000681169500016,32822311,
J,"Sun, Xian; Wang, Bing; Wang, Zhirui; Li, Hao; Li, Hengchao; Fu, Kun",,,,,"Wang, Bing/0000-0002-0604-3728; Sun, Xian/0000-0002-0038-9816; fu, kun/0000-0002-0450-6469",,,Research Progress on Few-Shot Learning for Remote Sensing Image Interpretation,,,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,14,,,,2387,2402,,10.1109/JSTARS.2021.3052869,,,,2021,2021,"The rapid development of deep learning brings effective solutions for remote sensing image interpretation. Training deep neural network models usually require a large number of manually labeled samples. However, there is a limitation to obtain sufficient labeled samples in remote sensing field to satisfy the data requirement. Therefore, it is of great significance to conduct the research on few-shot learning for remote sensing image interpretation. First, this article provides a bibliometric analysis of the existing works for remote sensing interpretation related to few-shot learning. Second, two categories of few-shot learning methods, i.e., the data-augmentation-based and the prior-knowledge-based, are introduced for the interpretation of remote sensing images. Then, three typical remote sensing interpretation applications are listed, including scene classification, semantic segmentation, and object detection, together with the corresponding public datasets and the evaluation criteria. Finally, the research status is summarized, and some possible research directions are provided. This article gives a reference for scholars working on few-shot learning research in the remote sensing field.",,,,,,,,,27,1,0,0,1,0,28,,,1939-1404,2151-1535,,WOS:000621403900003,,
J,"Li, Ying; Ma, Lingfei; Zhong, Zilong; Cao, Dongpu; Li, Jonathan",,,,"Li, Jonathan/AAA-7712-2021","LI, Jonathan/0000-0001-7899-0049; Zhong, Zilong/0000-0003-0104-9116; Ma, Lingfei/0000-0001-8893-9693",,,TGNet: Geometric Graph CNN on 3-D Point Cloud Segmentation,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,5,,,3588,3600,,10.1109/TGRS.2019.2958517,,,,MAY 2020,2020,"Recent geometric deep learning works define convolution operations in local regions and have enjoyed remarkable success on non-Euclidean data, including graph and point clouds. However, the high-level geometric correlations between the input and its neighboring coordinates or features are not fully exploited, resulting in suboptimal segmentation performance. In this article, we propose a novel graph convolution architecture, which we term as Taylor Gaussian mixture model (GMM) network (TGNet), to efficiently learn expressive and compositional local geometric features from point clouds. The TGNet is composed of basic geometric units, TGConv, that conduct local convolution on irregular point sets and are parametrized by a family of filters. Specifically, these filters are defined as the products of the local point features and the neighboring geometric features extracted from local coordinates. These geometric features are expressed by Gaussian weighted Taylor kernels. Then, a parametric pooling layer aggregates TGConv features to generate new feature vectors for each point. TGNet employs TGConv on multiscale neighborhoods to extract coarse-to-fine semantic deep features while improving its scale invariance. Additionally, a conditional random field (CRF) is adopted within the output layer to further improve the segmentation results. Using three point cloud data sets, qualitative and quantitative experimental results demonstrate that the proposed method achieves 62.2% average accuracy on ScanNet, 57.8% and 68.17% mean intersection over union (mIoU) on Stanford Large-Scale 3D Indoor Spaces (S3DIS) and Paris-Lille-3D data sets, respectively.",,,,,,,,,28,0,0,0,0,0,28,,,0196-2892,1558-0644,,WOS:000529868700046,,
J,"Zhang, Lili; Wu, Jisen; Fan, Yu; Gao, Hongmin; Shao, Yehong",,,,,"Gao, Hongmin/0000-0002-8404-2464",,,An Efficient Building Extraction Method from High Spatial Resolution Remote Sensing Images Based on Improved Mask R-CNN,,,,,,,,SENSORS,,,,20,5,,,,,1465,10.3390/s20051465,,,,MAR 2020,2020,"In this paper, we consider building extraction from high spatial resolution remote sensing images. At present, most building extraction methods are based on artificial features. However, the diversity and complexity of buildings mean that building extraction methods still face great challenges, so methods based on deep learning have recently been proposed. In this paper, a building extraction framework based on a convolution neural network and edge detection algorithm is proposed. The method is called Mask R-CNN Fusion Sobel. Because of the outstanding achievement of Mask R-CNN in the field of image segmentation, this paper improves it and then applies it in remote sensing image building extraction. Our method consists of three parts. First, the convolutional neural network is used for rough location and pixel level classification, and the problem of false and missed extraction is solved by automatically discovering semantic features. Second, Sobel edge detection algorithm is used to segment building edges accurately so as to solve the problem of edge extraction and the integrity of the object of deep convolutional neural networks in semantic segmentation. Third, buildings are extracted by the fusion algorithm. We utilize the proposed framework to extract the building in high-resolution remote sensing images from Chinese satellite GF-2, and the experiments show that the average value of IOU (intersection over union) of the proposed method was 88.7% and the average value of Kappa was 87.8%, respectively. Therefore, our method can be applied to the recognition and segmentation of complex buildings and is superior to the classical method in accuracy.",,,,,,,,,26,2,0,0,0,0,28,,,,1424-8220,,WOS:000525271500227,32155935,
J,"Huang, Lingcao; Luo, Jing; Lin, Zhanju; Niu, Fujun; Liu, Lin",,,,"Liu, Lin/Q-4237-2018","Liu, Lin/0000-0002-9581-1337",,,Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan Plateau) from CubeSat images,,,,,,,,REMOTE SENSING OF ENVIRONMENT,,,,237,,,,,,111534,10.1016/j.rse.2019.111534,,,,FEB 2020,2020,"Retrogressive thaw slumps (RTSs) are among the most dynamic landforms in permafrost areas, and their formation can be attributed to the thawing of ice-rich permafrost. The spatial distribution and impacts of RTSs on the Tibetan Plateau are poorly understood due to their remote location and the technical challenges of automatic mapping. In this study, we innovatively applied DeepLabv3 +, a cutting-edge deep learning algorithm for semantic segmentation, to Planet CubeSat images, which are satellite images with high spatial and temporal resolution. Our method allows us to automatically delineate 220 RTSs within an area of 5200 km(2) with an average precision of 0.541. The corresponding precision, recall, and F1 score are 0.863, 0.833, and 0.848 respectively, when the threshold of intersection over union is 0.5. Moreover, approximately 100 experiments on k-fold cross-validation (k = 3, 5, and 10) and data augmentation show that our method is robust. And a test in a different geographic area shows that the generalization of the trained model is very good. We find that (1) most of the RTSs are small (areas < eight ha and perimeters < 2000 m) and (2) RTSs preferentially develop at locations with gentle slopes (four to eight degrees), and in areas lower than the surroundings (the mean topographic position index is - 0.17) and receiving less solar radiation (i.e., north-facing slopes). The results show that the method can map RTSs automatically from Planet CubeSat images and can potentially be applied to larger areas.",,,,,,,,,26,1,1,0,6,0,28,,,0034-4257,1879-0704,,WOS:000509819300007,,
J,"Qiao, Lifeng; Zhu, Ying; Zhou, Hui",,,,,,,,Diabetic Retinopathy Detection Using Prognosis of Microaneurysm and Early Diagnosis System for Non-Proliferative Diabetic Retinopathy Based on Deep Learning Algorithms,,,,,,,,IEEE ACCESS,,,,8,,,,104292,104302,,10.1109/ACCESS.2020.2993937,,,,2020,2020,"Predicting the presence of Microaneurysms in the fundus images and the identification of diabetic retinopathy in early-stage has always been a major challenge for decades. Diabetic Retinopathy (DR) is affected by prolonged high blood glucose level which leads to microvascular complications and irreversible vision loss. Microaneurysms formation and macular edema in the retinal is the initial sign of DR and diagnosis at the right time can reduce the risk of non proliferated diabetic retinopathy. The rapid improvement of deep learning makes it gradually become an efficient technique to provide an interesting solution for medical image analysis problems. The proposed system analysis the presence of microaneurysm in fundus image using convolutional neural network algorithms that embeds deep learning as a core component accelerated with GPU(Graphics Processing Unit) which will perform medical image detection and segmentation with high-performance and low-latency inference. The semantic segmentation algorithm is utilized to classify the fundus picture as normal or infected. Semantic segmentation divides the image pixels based on their common semantic to identify the feature of microaneurysm. This provides an automated system that will assist ophthalmologists to grade the fundus images as early NPDR, moderate NPDR, and severe NPDR. The Prognosis of Microaneurysm and early diagnosis system for non - proliferative diabetic retinopathy system has been proposed that is capable to train effectively a deep convolution neural network for semantic segmentation of fundus images which can increase the efficiency and accuracy of NPDR (non proliferated diabetic retinopathy) prediction.",,,,,,,,,28,0,0,0,3,0,28,,,2169-3536,,,WOS:000546426500003,,
J,"Wang, Shengsheng; Hou, Xiaowei; Zhao, Xin",,,,,"Hou, Xiaowei/0000-0002-3313-3972; zhao, xin/0000-0003-2176-7537",,,Automatic Building Extraction From High-Resolution Aerial Imagery via Fully Convolutional Encoder-Decoder Network With Non-Local Block,,,,,,,,IEEE ACCESS,,,,8,,,,7313,7322,,10.1109/ACCESS.2020.2964043,,,,2020,2020,"Extracting buildings automatically from high-resolution aerial images is a significant and fundamental task for various practical applications, such as land-use statistics and urban planning. Recently, various methods based on deep learning, especially the fully convolution networks, achieve impressive scores in this challenging semantic segmentation task. However, the lack of global contextual information and the careless upsampling method limit the further improvement of the performance for building extraction task. To simultaneously address these problems, we propose a novel network named Efficient Non-local Residual U-shape Network(ENRU-Net), which is composed of a well designed U-shape encoder-decoder structure and an improved non-local block named asymmetric pyramid non-local block (APNB). The encoder-decoder structure is adopted to extract and restore the feature maps carefully, and APNB could capture global contextual information by utilizing self-attention mechanism. We evaluate the proposed ENRU-Net and compare it with other state-of-the-art models on two widely-used public aerial building imagery datasets: the Massachusetts Buildings Dataset and the WHU Aerial Imagery Dataset. The experiments show that the accuracy of ENRU-Net on these datasets has remarkable improvement against previous state-of-the-art semantic segmentation models, including FCN-8s, U-Net, SegNet and Deeplab v3. The subsequent analysis also indicates that our ENRU-Net has advantages in efficiency for building extraction from high-resolution aerial images.",,,,,,,,,28,1,0,0,0,0,28,,,2169-3536,,,WOS:000525422700047,,
B,Xiangtai Li; Xia Li; Li Zhang; Guangliang Cheng; Jianping Shi; Zhouchen Lin; Shaohua Tan; Yunhai Tong,,,,,,"Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.",,Improving Semantic Segmentation via Decoupled Body and Edge Supervision,,,,,,,,Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362),,,,,,,,435,52,,10.1007/978-3-030-58520-4_26,,,,2020,2020,"Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, CamVid, KIITI and BDD show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (https://github.com/lxtGH/DecoupleSegNets).",,,,,Computer Vision - ECCV 2020. 16th European ConferenceComputer Vision - ECCV 2020. 16th European Conference,23-28 Aug. 202023-28 Aug. 2020,,"Glasgow, UKGlasgow, UK",28,0,0,0,1,0,28,,,,,978-3-030-58519-8,INSPEC:20220943,,
C,"Zheng, Zhuo; Zhong, Yanfei; Wang, Junjue; Ma, Ailong",,,IEEE,"Wang, Junjue/AAE-5698-2022","Wang, Junjue/0000-0002-9500-3399",,,Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4095,4104,,10.1109/CVPR42600.2020.00415,,,,2020,2020,"Geospatial object segmentation, as a particular semantic segmentation task, always faces with larger-scale variation, larger intra-class variance of background, and foreground-background imbalance in the high spatial resolution (HSR) remote sensing imagery. However, general semantic segmentation methods mainly focus on scale variation in the natural scene, with inadequate consideration of the other two problems that usually happen in the large area earth observation scene. In this paper, we argue that the problems lie on the lack of foreground modeling and propose a foreground-aware relation network (FarSeg) from the perspectives of relation-based and optimization-based foreground modeling, to alleviate the above two problems. From perspective of relation, FarSeg enhances the discrimination of foreground features via foreground-correlated contexts associated by learning foreground-scene relation. Meanwhile, from perspective of optimization, a foreground-aware optimization is proposed to focus on foreground examples and hard examples of background during training for a balanced optimization. The experimental results obtained using a large scale dataset suggest that the proposed method is superior to the state-of-the-art general semantic segmentation methods and achieves a better trade-off between speed and accuracy.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,27,1,0,0,3,0,28,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504037,,
J,"Zhou, Wujie; Yuan, Jianzhong; Lei, Jingsheng; Luo, Ting",,,,,,,,TSNet: Three-Stream Self-Attention Network for RGB-D Indoor Semantic Segmentation,,,,,,,,IEEE INTELLIGENT SYSTEMS,,,,36,4,,,73,78,,10.1109/MIS.2020.2999462,,,,JUL-AUG 2021,2021,"This article proposes a three-stream self-attention network (TSNet) for indoor semantic segmentation comprising two asymmetric input streams (asymmetric encoder structure) and a cross-modal distillation stream with a self-attention module. The two asymmetric input streams are ResNet34 for the red-green-blue (RGB) stream and VGGNet16 for the depth stream. Accompanying the RGB and depth streams, a cross-modal distillation stream with a self- attention module extracts new RGB plus depth features in each level in the bottom-up path. In addition, while using bilinear upsampling to recover the spatial resolution of the feature map, we incorporated the feature information of both the RGB flow and the depth flow through the self-attention module. We constructed the NYU Depth V2 dataset to evaluate the TSNet and achieved results comparable to those of current state-of-the-art methods.",,,,,,,,,27,0,0,0,0,0,27,,,1541-1672,1941-1294,,WOS:000696077900008,,
J,"Hong, Danfeng; Yao, Jing; Meng, Deyu; Xu, Zongben; Chanussot, Jocelyn",,,,"Hong, Danfeng/U-6082-2019; Yao, Jing/AAA-5843-2021","Hong, Danfeng/0000-0002-3212-9584; Yao, Jing/0000-0003-1301-9758; Chanussot, Jocelyn/0000-0003-4817-2875",,,Multimodal GANs: Toward Crossmodal HyperspectralMultispectral Image Segmentation,,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,59,6,,,5103,5113,,10.1109/TGRS.2020.3020823,,,,JUN 2021,2021,"This article addresses the problem of semantic segmentation with limited cross-modality data in large-scale urban scenes. Most prior works have attempted to address this issue by using multimodal deep neural networks (DNNs). However, their ability to effectively blending different properties across multimodalities and robustly learning representations from complex scenes remains limited, particularly in the absence of sufficient and well-annotated training images. This leads to a challenge related to cross-modality learning with multimodal DNNs. To this end, we introduce two novel plug-and-play units in the network: self-generative adversarial networks (GANs) module and mutual-GANs module, to learn perturbation-insensitive feature representations and to eliminate the gap between multimodalities, respectively, yielding more effective and robust information transfer. Furthermore, a patchwise progressive training strategy is devised to enable effective network learning with limited samples. We evaluate the proposed network on two multimodal (hyperspectral and multispectral) overhead image data sets and achieve a significant improvement in comparison with several state-of-the-art methods.",,,,,,,,,27,0,0,0,0,0,27,,,0196-2892,1558-0644,,WOS:000652834200045,,
J,"Su, Jinya; Yi, Dewei; Su, Baofeng; Mi, Zhiwen; Liu, Cunjia; Hu, Xiaoping; Xu, Xiangming; Guo, Lei; Chen, Wen-Hua",,,,"Xu, Xiangming/I-3050-2014; 于, 于增臣/AAH-4657-2021; SU, JINYA/AAG-6205-2019","Xu, Xiangming/0000-0002-4567-7117; SU, JINYA/0000-0002-3121-7208; Yi, Dewei/0000-0003-1702-9136; Hu, Xiaoping/0000-0002-8155-7040; Guo, Lei/0000-0002-3061-2337; Liu, Cunjia/0000-0003-2829-9369",,,Aerial Visual Perception in Smart Farming: Field Study of Wheat Yellow Rust Monitoring,,,,,,,,IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS,,,,17,3,,,2242,2249,,10.1109/TII.2020.2979237,,,,MAR 2021,2021,"Agriculture is facing severe challenges from crop stresses, threatening its sustainable development and food security. This article exploits aerial visual perception for yellow rust disease monitoring, which seamlessly integrates state-of-the-art techniques and algorithms, including unmanned aerial vehicle sensing, multispectral imaging, vegetation segmentation, and deep learning U-Net. A field experiment is designed by infecting winter wheat with yellow rust inoculum, on top of which multispectral aerial images are captured by DJI Matrice 100 equipped with RedEdge camera. After image calibration and stitching, multispectral orthomosaic is labeled for system evaluation by inspecting high-resolution RGB images taken by Parrot Anafi Drone. The merits of the developed framework drawing spectral-spatial information concurrently are demonstrated by showing improved performance over purely spectral-based classifier by the classical random forest algorithm. Moreover, various network input band combinations are tested, including three RGB bands and five selected spectral vegetation indices, by sequential forward selection strategy of wrapper algorithm.",,,,,,,,,25,1,0,0,6,1,27,,,1551-3203,1941-0050,,WOS:000597195500068,,
J,"Mehrtash, Alireza; Wells, William M., III; Tempany, Clare M.; Abolmaesumi, Purang; Kapur, Tina",,,,,"Abolmaesumi, Purang/0000-0002-7259-8609; Mehrtash, Alireza/0000-0003-3703-5330",,,Confidence Calibration and Predictive Uncertainty Estimation for Deep Medical Image Segmentation,,,,,,,,IEEE TRANSACTIONS ON MEDICAL IMAGING,,,,39,12,,,3868,3878,,10.1109/TMI.2020.3006437,,,,DEC 2020,2020,"Fully convolutional neural networks (FCNs), and in particular U-Nets, have achieved state-of-the-art results in semantic segmentation for numerous medical imaging applications. Moreover, batch normalization and Dice loss have been used successfully to stabilize and accelerate training. However, these networks are poorly calibrated i.e. they tend to produce overconfident predictions for both correct and erroneous classifications, making them unreliable and hard to interpret. In this paper, we study predictive uncertainty estimation in FCNs for medical image segmentation. We make the following contributions: 1) We systematically compare cross-entropy loss with Dice loss in terms of segmentation quality and uncertainty estimation of FCNs; 2) We propose model ensembling for confidence calibration of the FCNs trained with batch normalization and Dice loss; 3) We assess the ability of calibrated FCNs to predict segmentation quality of structures and detect out-of-distribution test examples. We conduct extensive experiments across three medical image segmentation applications of the brain, the heart, and the prostate to evaluate our contributions. The results of this study offer considerable insight into the predictive uncertainty estimation and out-of-distribution detection in medical image segmentation and provide practical recipes for confidence calibration. Moreover, we consistently demonstrate that model ensembling improves confidence calibration.",,,,,,,,,27,0,0,0,4,0,27,,,0278-0062,1558-254X,,WOS:000595547500010,32746129,
J,"Pan, Bin; Xu, Xia; Shi, Zhenwei; Zhang, Ning; Luo, Huanlin; Lan, Xianchao",,,,,,,,DSSNet: A Simple Dilated Semantic Segmentation Network for Hyperspectral Imagery Classification,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,17,11,,,1968,1972,,10.1109/LGRS.2019.2960528,,,,NOV 2020,2020,"Deep learning-based methods have presented a promising performance in the task of hyperspectral imagery classification (HSIC). However, recent methods usually are considered HSIC as a patchwise image classification problem and addressed it by giving a single label to the patch surrounding a pixel. In this letter, we propose a new semantic segmentation network that can directly label each pixel in an end-to-end manner. Compared with patchwise models, our method can significantly improve training effectiveness and reduce some manual parameters. Another challenge in HSIC is that the spatial resolution of hyperspectral imagery is relatively low; in that case, the pooling operation may result in resolution and coverage loss. To address this issue, we introduce dilated convolution to our model and construct a dilated semantic segmentation network (DSSNet). Different from some existing works, DSSNet is specially designed for HSIC without complicated architecture, and no pretrained models are required. The joint spatial-spectral information can be extracted via an end-to-end manner and, thus, avoid various preprocessing or postprocessing operations. Experiments on two public data sets have demonstrated the effectiveness of our improvements compared with some of the latest deep learning-based HSIC models.",,,,,,,,,27,0,0,0,2,0,27,,,1545-598X,1558-0571,,WOS:000583714200026,,
J,"Yang, Kailun; Hu, Xinxin; Bergasa, Luis M.; Romera, Eduardo; Wang, Kaiwei",,,,"Bergasa, Luis M./H-9810-2013; Yang, Kailun/U-2491-2019","Bergasa, Luis M./0000-0002-0087-3077; Yang, Kailun/0000-0002-1090-667X",,,PASS: Panoramic Annular Semantic Segmentation,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,21,10,,,4171,4185,,10.1109/TITS.2019.2938965,,,,OCT 2020,2020,"Pixel-wise semantic segmentation is capable of unifying most of driving scene perception tasks, and has enabled striking progress in the context of navigation assistance, where an entire surrounding sensing is vital. However, current mainstream semantic segmenters are predominantly benchmarked against datasets featuring narrow Field of View (FoV), and a large part of vision-based intelligent vehicles use only a forward-facing camera. In this paper, we propose a Panoramic Annular Semantic Segmentation (PASS) framework to perceive the whole surrounding based on a compact panoramic annular lens system and an online panorama unfolding process. To facilitate the training of PASS models, we leverage conventional FoV imaging datasets, bypassing the efforts entailed to create fully dense panoramic annotations. To consistently exploit the rich contextual cues in the unfolded panorama, we adapt our real-time ERF-PSPNet to predict semantically meaningful feature maps in different segments, and fuse them to fulfill panoramic scene parsing. The innovation lies in the network adaptation to enable smooth and seamless segmentation, combined with an extended set of heterogeneous data augmentations to attain robustness in panoramic imagery. A comprehensive variety of experiments demonstrates the effectiveness for real-world surrounding perception in a single PASS, while the adaptation proposal is exceptionally positive for state-of-the-art efficient networks.",,,,,,,,,26,1,0,0,0,0,27,,,1524-9050,1558-0016,,WOS:000576271400012,,
J,"Pan, Suoyan; Guan, Haiyan; Chen, Yating; Yu, Yongtao; Goncalves, Wesley Nunes; Marcato Junior, Jose; Li, Jonathan",,,,"Li, Jonathan/AAA-7712-2021","LI, Jonathan/0000-0001-7899-0049",,,Land-cover classification of multispectral LiDAR data using CNN with optimized hyper-parameters,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,166,,,,241,254,,10.1016/j.isprsjprs.2020.05.022,,,,AUG 2020,2020,"Multispectral LiDAR (Light Detection And Ranging) is characterized of the completeness and consistency of its spectrum and spatial geometric data, which provides a new data source for land-cover classification. In recent years, the convolutional neural network (CNN), compared with traditional machine learning methods, has made a series of breakthroughs in image classification, object detection, and image semantic segmentation due to its stronger feature learning and feature expression abilities. However, traditional CNN models suffer from some issues, such as a large number of layers, leading to higher computational cost. To address this problem, we propose a CNN-based multi-spectral LiDAR land-cover classification framework and analyze its optimal parameters to improve classification accuracy. This framework starts with the preprocessing of multi-spectral 3D LiDAR data into 2D images. Next, a CNN model is constructed with seven fundamental functional layers, and its hyper-parameters are comprehensively discussed and optimized. The constructed CNN model with the optimized hyper-parameters was tested on the Titan multi-spectral LiDAR data, which include three wavelengths of 532 nm, 1064 nm, and 1550 nm. Extensive experiments demonstrated that the constructed CNN with the optimized hyper-parameters is feasible for multi-spectral LiDAR land-cover classification tasks. Compared with the classical CNN models (i.e., AlexNet, VGG16 and ResNet50) and our previous studies, our constructed CNN model with the optimized hyper-parameters is superior in computational performance and classification accuracies.",,,,,,,,,27,0,0,0,1,0,27,,,0924-2716,1872-8235,,WOS:000551268300019,,
J,"Hausen, Ryan; Robertson, Brant E.",,,,"Robertson, Brant/AAA-6124-2022","Robertson, Brant/0000-0002-4271-0364; Hausen, Ryan/0000-0002-8543-761X",,,Morpheus: A Deep Learning Framework for the Pixel-level Analysis of Astronomical Image Data,,,,,,,,ASTROPHYSICAL JOURNAL SUPPLEMENT SERIES,,,,248,1,,,,,20,10.3847/1538-4365/ab8868,,,,MAY 2020,2020,"We present Morpheus, a new model for generating pixel-level morphological classifications of astronomical sources. Morpheus leverages advances in deep learning to perform source detection, source segmentation, and morphological classification pixel-by-pixel via a semantic segmentation algorithm adopted from the field of computer vision. By utilizing morphological information about the flux of real astronomical sources during object detection, Morpheus shows resiliency to false-positive identifications of sources. We evaluate Morpheus by performing source detection, source segmentation, morphological classification on the Hubble Space Telescope data in the five CANDELS fields with a focus on the GOODS South field, and demonstrate a high completeness in recovering known GOODS South 3D-HST sources with H < 26 AB. We release the code publicly, provide online demonstrations, and present an interactive visualization of the Morpheus results in GOODS South.",,,,,,,,,27,0,0,0,0,0,27,,,0067-0049,1538-4365,,WOS:000535553400001,,
J,"Huang, Rong; Xu, Yusheng; Hong, Danfeng; Yao, Wei; Ghamisi, Pedram; Stilla, Uwe",,,,"Stilla, Uwe/H-1534-2011; Hong, Danfeng/U-6082-2019; Ghamisi, Pedram/ABD-5419-2021; Huang, Rong/AAW-3077-2020; Yao, Wei/E-8520-2017","Stilla, Uwe/0000-0002-1184-0924; Hong, Danfeng/0000-0002-3212-9584; Huang, Rong/0000-0002-4491-6059; Yao, Wei/0000-0001-7704-0615",,,Deep point embedding for urban classification using ALS point clouds: A new perspective from local to global,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,163,,,,62,81,,10.1016/j.isprsjprs.2020.02.020,,,,MAY 2020,2020,"Semantic interpretation of the 3D scene is one of the most challenging problems in point cloud processing, which also deems as an essential task in a wide variety of point cloud applications. The core task of semantic interpretation is semantic labeling, namely, obtaining a unique semantic label for each point in the point cloud. Despite several reported approaches, semantic labeling continues to be a challenge owing to the complexity of scenes, objects of various scales, and the non-homogeneity of unevenly distributed points. In this paper, we propose a novel method for obtaining semantic labels of airborne laser scanning (ALS) point clouds involving the embedding of local context information for each point with multi-scale deep learning, nonlinear manifold learning for feature dimension reduction, and global graph-based optimization for refining the classification results. Specifically, we address the tasks of learning discriminative features and global labeling smoothing. The key contribution of our study is threefold. First, a hierarchical data augmentation strategy is applied to enhance the learning of deep features based on the PointNet + + network and simultaneously eliminate the artifacts caused by division and sampling while dealing with large-scale datasets. Subsequently, the learned hierarchical deep features are globally optimized and embedded into a low-dimensional space with a nonlinear manifold-based joint learning method with the removal of redundant and disturbing information. Finally, a graphstructured optimization based on the Markov random fields algorithm is performed to achieve global optimization of the initial classification results that are obtained using the embedded deep features by constructing a weighted indirect graph and solving the optimization problem with graph-cuts. We conducted thorough experiments on ALS point cloud datasets to assess the performance of our framework. Results indicate that compared to other commonly used advanced classification methods, our method can achieve high classification accuracy. The overall accuracy (OA) of our approach on the ISPRS benchmark dataset can scale up to 83.2% for classifying nine semantic classes, thereby outperforming other compared point-based strategies. Additionally, we evaluated our framework on a selected portion of the AHN3 dataset, which provided OA up to 91.2%.",,,,,,,,,27,0,0,0,0,0,27,,,0924-2716,1872-8235,,WOS:000527712500005,,
J,"Guo, Hongxiang; He, Guojin; Jiang, Wei; Yin, Ranyu; Yan, Lei; Leng, Wanchun",,,,,"Leng, Wanchun/0000-0001-9355-2952; yin, ranyu/0000-0001-5594-0815",,,A Multi-Scale Water Extraction Convolutional Neural Network (MWEN) Method for GaoFen-1 Remote Sensing Images,,,,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,9,4,,,,,189,10.3390/ijgi9040189,,,,APR 2020,2020,"Automatic water body extraction method is important for monitoring floods, droughts, and water resources. In this study, a new semantic segmentation convolutional neural network named the multi-scale water extraction convolutional neural network (MWEN) is proposed to automatically extract water bodies from GaoFen-1 (GF-1) remote sensing images. Three convolutional neural networks for semantic segmentation (fully convolutional network (FCN), Unet, and Deeplab V3+) are employed to compare with the water bodies extraction performance of MWEN. Visual comparison and five evaluation metrics are used to evaluate the performance of these convolutional neural networks (CNNs). The results show the following. (1) The results of water body extraction in multiple scenes using the MWEN are better than those of the other comparison methods based on the indicators. (2) The MWEN method has the capability to accurately extract various types of water bodies, such as urban water bodies, open ponds, and plateau lakes. (3) By fusing features extracted at different scales, the MWEN has the capability to extract water bodies with different sizes and suppress noise, such as building shadows and highways. Therefore, MWEN is a robust water extraction algorithm for GaoFen-1 satellite images and has the potential to conduct water body mapping with multisource high-resolution satellite remote sensing data.",,,,,International Conference on Geo-Information Technology and its Applications (ICGITA)International Conference on Geo-Information Technology and its Applications (ICGITA),"OCT 11-13, 2019OCT 11-13, 2019",,"Nanchang, PEOPLES R CHINANanchang, PEOPLES R CHINA",26,1,0,0,1,0,27,,,,2220-9964,,WOS:000539535700004,,
J,"Kong, Bin; Wang, Xin; Bai, Junjie; Lu, Yi; Gao, Feng; Cao, Kunlin; Xia, Jun; Song, Qi; Yin, Youbing",,,,,,,,Learning tree-structured representation for 3D coronary artery segmentation,,,,,,,,COMPUTERIZED MEDICAL IMAGING AND GRAPHICS,,,,80,,,,,,101688,10.1016/j.compmedimag.2019.101688,,,,MAR 2020,2020,"Extensive research has been devoted to the segmentation of the coronary artery. However, owing to its complex anatomical structure, it is extremely challenging to automatically segment the coronary artery from 3D coronary computed tomography angiography (CCTA). Inspired by recent ideas to use tree-structured long short-term memory (LSTM) to model the underlying tree structures for NLP tasks, we propose a novel tree-structured convolutional gated recurrent unit (ConvGRU) model to learn the anatomical structure of the coronary artery. However, unlike tree-structured LSTM proposed for semantic relatedness as well as sentiment classification in natural language processing, our tree-structured ConvGRU model considers the local spatial correlations in the input data as the convolutions are used for input-to-state as well as state-to-state transitions, thus more suitable for image analysis. To conduct voxel-wise segmentation, a tree-structured segmentation framework is presented. It consists of a fully convolutional network (FCN) for multi-scale discriminative feature extraction and the final prediction, and a tree-structured ConvGRU layer for anatomical structure modeling. The proposed framework is extensively evaluated on four large-scale 3D CCTA dataset (the largest to the best of our knowledge), and experiments show that our method is more accurate as well as efficient, compared with other coronary artery segmentation approaches. (C) 2019 Elsevier Ltd. All rights reserved.",,,,,,,,,26,1,0,0,3,0,27,,,0895-6111,1879-0771,,WOS:000517850500001,31926366,
J,"Pashaei, Mohammad; Kamangir, Hamid; Starek, Michael J.; Tissot, Philippe",,,,"Kamangir, Hamid/AAB-7042-2021; Pashaei, Mohammad/ABC-9236-2020; Tissot, Philippe/I-5609-2017","Tissot, Philippe/0000-0002-2954-2378; kamangir, hamid/0000-0001-9718-7518; Starek, Michael/0000-0002-7996-0594; Pashaei, Mohammad/0000-0002-1427-6265",,,Review and Evaluation of Deep Learning Architectures for Efficient Land Cover Mapping with UAS Hyper-Spatial Imagery: A Case Study Over a Wetland,,,,,,,,REMOTE SENSING,,,,12,6,,,,,959,10.3390/rs12060959,,,,MAR 2020,2020,"Deep learning has already been proved as a powerful state-of-the-art technique for many image understanding tasks in computer vision and other applications including remote sensing (RS) image analysis. Unmanned aircraft systems (UASs) offer a viable and economical alternative to a conventional sensor and platform for acquiring high spatial and high temporal resolution data with high operational flexibility. Coastal wetlands are among some of the most challenging and complex ecosystems for land cover prediction and mapping tasks because land cover targets often show high intra-class and low inter-class variances. In recent years, several deep convolutional neural network (CNN) architectures have been proposed for pixel-wise image labeling, commonly called semantic image segmentation. In this paper, some of the more recent deep CNN architectures proposed for semantic image segmentation are reviewed, and each model's training efficiency and classification performance are evaluated by training it on a limited labeled image set. Training samples are provided using the hyper-spatial resolution UAS imagery over a wetland area and the required ground truth images are prepared by manual image labeling. Experimental results demonstrate that deep CNNs have a great potential for accurate land cover prediction task using UAS hyper-spatial resolution images. Some simple deep learning architectures perform comparable or even better than complex and very deep architectures with remarkably fewer training epochs. This performance is especially valuable when limited training samples are available, which is a common case in most RS applications.",,,,,,,,,27,0,0,0,5,0,27,,,,2072-4292,,WOS:000526820600061,,
J,"Adegun, Adekanmi A.; Viriri, Serestina",,,,"ADEGUN, ADEKANMI/ABF-6533-2020; Viriri, Serestina/AAN-3882-2020","ADEGUN, ADEKANMI/0000-0001-7244-9665; Viriri, Serestina/0000-0002-2850-8645",,,Deep Learning-Based System for Automatic Melanoma Detection,,,,,,,,IEEE ACCESS,,,,8,,,,7160,7172,,10.1109/ACCESS.2019.2962812,,,,2020,2020,"Melanoma is the deadliest form of skin cancer. Distinguishing melanoma lesions from non-melanoma lesions has however been a challenging task. Many Computer Aided Diagnosis and Detection Systems have been developed in the past for this task. They have been limited in performance due to the complex visual characteristics of the skin lesion images which consists of inhomogeneous features and fuzzy boundaries. In this paper, we propose a deep learning-based method that overcomes these limitations for automatic melanoma lesion detection and segmentation. An enhanced encoder-decoder network with encoder and decoder sub-networks connected through a series of skip pathways which brings the semantic level of the encoder feature maps closer to that of the decoder feature maps is proposed for efficient learning and feature extraction. The system employs multi-stage and multi-scale approach and utilizes softmax classifier for pixel-wise classification of melanoma lesions. We devise a new method called Lesion-classifier that performs the classification of skin lesions into melanoma and non-melanoma based on results derived from pixel-wise classification. Our experiments on two well-established public benchmark skin lesion datasets, International Symposium on Biomedical Imaging(ISBI)2017 and Hospital Pedro Hispano (PH2), demonstrate that our method is more effective than some state-of-the-art methods. We achieved accuracy and dice coefficient of 95 & x0025; and 92 & x0025; on ISIC 2017 dataset and accuracy and dice coefficient of 95 & x0025; and 93 & x0025; on PH2 datasets.",,,,,,,,,27,0,0,0,1,0,27,,,2169-3536,,,WOS:000525422700033,,
C,"Dong, Jiahua; Cong, Yang; Sun, Gan; Zhong, Bineng; Xu, Xiaowei",,,IEEE,"Sun, Gan/ABD-6793-2021",,,,What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4022,4031,,10.1109/CVPR42600.2020.00408,,,,2020,2020,"Unsupervised domain adaptation has attracted growing research attention on semantic segmentation. However, 1) most existing models cannot be directly applied into lesions transfer of medical images, due to the diverse appearances of same lesion among different datasets; 2) equal attention has been paid into all semantic representations instead of neglecting irrelevant knowledge, which leads to negative transfer of untransferable knowledge. To address these challenges, we develop a new unsupervised semantic transfer model including two complementary modules (i.e., T-D and T-F) for endoscopic lesions segmentation, which can alternatively determine where and how to explore transferable domain-invariant knowledge between labeled source lesions dataset (e.g., gastroscope) and unlabeled target diseases dataset (e.g., enteroscopy). Specifically, T-D focuses on where to translate transferable visual information of medical lesions via residual transferability-aware bottleneck, while neglecting untransferable visual characterizations. Furthermore, T-F highlights how to augment transferable semantic features of various lesions and automatically ignore untransferable representations, which explores domain-invariant knowledge and in return improves the performance of T-D. To the end, theoretical analysis and extensive experiments on medical endoscopic dataset and several non-medical public datasets well demonstrate the superiority of our proposed model.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,27,0,0,0,0,0,27,,,1063-6919,,978-1-7281-7168-5,WOS:000620679504030,,
J,"Li, Xiang; Li, Jun; Hu, Xiaolin; Yang, Jian",,,,,"Li, Xiang/0000-0002-4996-7365",,,Line-CNN: End-to-End Traffic Line Detection With Line Proposal Unit,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,21,1,,,248,258,,10.1109/TITS.2019.2890870,,,,JAN 2020,2020,"The task of traffic line detection is a fundamental yet challenging problem. Previous approaches usually conduct traffic line detection via a two-stage way, namely the line segment detection followed by a segment clustering, which is very likely to ignore the global semantic information of an entire line. To address the problem, we propose an end-to-end system called Line-CNN (L-CNN), in which the key component is a novel line proposal unit (LPU). The LPU utilizes line proposals as references to locate accurate traffic curves, which forces the system to learn the global feature representation of the entire traffic lines. We benchmark the proposed L-CNN on two public datasets including MIKKI and TuSimple, and the results suggest that L-CNN outperforms the state-of-the-art methods. In addition, L-CNN can run at approximately 30 f/s on a Titan X GPU, which indicates the practicability and effectiveness of L-CNN for real-time intelligent self-driving systems.",,,,,,,,,25,2,0,0,0,0,27,,,1524-9050,1558-0016,,WOS:000506619900019,,
J,"Tian, Zhi; Shen, Chunhua; Chen, Hao; He, Tong",,,,,,,,FCOS: A Simple and Strong Anchor-Free Object Detector,,,,,,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,,,44,4,,,1922,1933,,10.1109/TPAMI.2020.3032166,,,,APR 1 2022,2022,"In computer vision, object detection is one of most important tasks, which underpins a few instance-level recognition tasks and many downstream applications. Recently one-stage methods have gained much attention over two-stage approaches due to their simpler design and competitive performance. Here we propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to other dense prediction problems such as semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating the intersection over union (IoU) scores during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: git. io/AdelaiDet",,,,,,,,,26,0,0,0,0,0,26,,,0162-8828,1939-3539,,WOS:000764815300020,33074804,
J,"Liu, Yi; Pang, Chao; Zhan, Zongqian; Zhang, Xiaomeng; Yang, Xue",,,,,,,,Building Change Detection for Remote Sensing Images Using a Dual-Task Constrained Deep Siamese Convolutional Network Model,,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,18,5,,,811,815,,10.1109/LGRS.2020.2988032,,,,MAY 2021,2021,"In recent years, building change detection methods have made great progress by introducing deep learning, but they still suffer from the problem of the extracted features not being discriminative enough, resulting in incomplete regions and irregular boundaries. To tackle this problem, we propose a dual-task constrained deep Siamese convolutional network (DTCDSCN) model, which contains three subnetworks: a change detection network and two semantic segmentation networks. DTCDSCN can accomplish both change detection and semantic segmentation at the same time, which can help to learn more discriminative object-level features and obtain a complete change detection map. Furthermore, we introduce a dual attention module (DAM) to exploit the interdependencies between channels and spatial positions, which improves the feature representation. We also improve the focal loss function to suppress the sample imbalance problem. The experimental results obtained with the WHU building data set show that the proposed method is effective for building change detection and achieves state-of-the-art performance in terms of four metrics on the WHU building data set: precision, recall, F1-score, and intersection over union.",,,,,,,,,26,0,0,0,0,0,26,,,1545-598X,1558-0571,,WOS:000642766600012,,
J,"Zhang, Kaige; Zhang, Yingtao; Cheng, Heng-Da",,,,,"Zhang, Kaige/0000-0003-0935-2734",,,CrackGAN: Pavement Crack Detection Using Partially Accurate Ground Truths Based on Generative Adversarial Learning,,,,,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,22,2,,,1306,1319,,10.1109/TITS.2020.2990703,,,,FEB 2021,2021,"Fully convolutional network is a powerful tool for per-pixel semantic segmentation/detection. However, it is problematic when coping with crack detection using partially accurate ground truths (GTs): the network may easily converge to the status that treats all the pixels as background (BG) and still achieves a very good loss, named All Black phenomenon, due to the unavailability of accurate GTs and the data imbalance. To tackle this problem, we propose crack-patch-only (CPO) supervised generative adversarial learning for end-to-end training, which forces the network to always produce crack-GT images while reserves both crack and BG-image translation abilities by feeding a larger-size crack image into an asymmetric U-shape generator to overcome the All Black issue. The proposed approach is validated using four crack datasets; and achieves state-of-the-art performance comparing with that of the recently published works in efficiency and accuracy.",,,,,,,,,25,2,1,0,0,0,26,,,1524-9050,1558-0016,,WOS:000615045000049,,
J,"Li, Qingyu; Shi, Yilei; Huang, Xin; Zhu, Xiao Xiang",,,,"Zhu, Xiao Xiang/ABE-7138-2020","Zhu, Xiao Xiang/0000-0001-5530-3613",,,Building Footprint Generation by Integrating Convolution Neural Network With Feature Pairwise Conditional Random Field (FPCRF),,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,58,11,,,7502,7519,,10.1109/TGRS.2020.2973720,,,,NOV 2020,2020,"Building footprint maps are vital to many remote sensing (RS) applications, such as 3-D building modeling, urban planning, and disaster management. Due to the complexity of buildings, the accurate and reliable generation of the building footprint from RS imagery is still a challenging task. In this article, an end-to-end building footprint generation approach that integrates convolution neural network (CNN) and graph model is proposed. CNN serves as the feature extractor, while the graph model can take spatial correlation into consideration. Moreover, we propose to implement the feature pairwise conditional random field (FPCRF) as a graph model to preserve sharp boundaries and fine-grained segmentation. Experiments are conducted on four different data sets: 1) Planetscope satellite imagery of the cities of Munich, Paris, Rome, and Zurich; 2) ISPRS Benchmark data from the city of Potsdam; 3) Dstl Kaggle data set; and 4) Inria Aerial Image Labeling data of Austin, Chicago, Kitsap County, Western Tyrol, and Vienna. It is found that the proposed end-to-end building footprint generation framework with the FPCRF as the graph model can further improve the accuracy of building footprint generation by using only CNN, which is the current state of the art.",,,,,,,,,25,1,0,0,0,0,26,,,0196-2892,1558-0644,,WOS:000583850500001,,
J,"Lu, Yuzhen; Young, Sierra",,,,,"Lu, Yuzhen/0000-0002-3636-1137; Young, Sierra/0000-0001-9146-1088",,,A survey of public datasets for computer vision tasks in precision agriculture,,,,,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,178,,,,,,105760,10.1016/j.compag.2020.105760,,,,NOV 2020,2020,"Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture.",,,,,,,,,26,0,0,0,11,1,26,,,0168-1699,1872-7107,,WOS:000596392400007,,
J,"Lee, Haeyun; Park, Jinhyoung; Hwang, Jae Youn",,,,,"Lee, Haeyun/0000-0002-7572-1705",,,Channel Attention Module With Multiscale Grid Average Pooling for Breast Cancer Segmentation in an Ultrasound Image,,,,,,,,IEEE TRANSACTIONS ON ULTRASONICS FERROELECTRICS AND FREQUENCY CONTROL,,,,67,7,,,1344,1353,,10.1109/TUFFC.2020.2972573,,,,JUL 2020,2020,"Breast cancer accounts for the second-largest number of deaths in women around the world, and more than 8% of women will suffer from the disease in their lifetime. Mortality due to breast cancer can be reduced by its early and precise diagnosis. Many studies have investigated methods for segmentation, and computer-aided diagnosis based on deep learning techniques, in particular, has recently gained attention. However, recently proposed methods such as fully convolutional network (FCN), SegNet, and U-Net still need to be further improved to provide better semantic segmentation when diagnosing breast cancer by ultrasound imaging, because of their low performance. In this article, we propose a channel attention module with multiscale grid average pooling (MSGRAP) for the precise segmentation of breast cancer regions in ultrasound images. We demonstrate the effectiveness of the channel attention module with MSGRAP for semantic segmentation and develop a novel semantic segmentation network with the proposed attention module for the precise segmentation of breast cancer regions in ultrasound images. While a conventional convolutional operation cannot use global spatial information on input images and only use the small local information in a kernel of a convolution filter, the proposed attention module allows using both global and local spatial information. In addition, through ablation studies, we come up with a network architecture for precise breast cancer segmentation in an ultrasound image. The proposed network was constructed with an open-source breast cancer ultrasound image data set, and its performance was compared with those of other state-of-the-art deep-learning models for the segmentation of breast cancer. The experimental results showed that our network outperformed other segmentation methods, and the proposed channel attention module improved the performance of the network for breast cancer segmentation in ultrasound images.",,,,,,,,,26,0,0,0,5,0,26,,,0885-3010,1525-8955,,WOS:000545430200005,32054578,
J,"Chen, Shuhan; Wang, Ben; Tan, Xiuli; Hu, Xuelong",,,,,"Chen, Shuhan/0000-0002-0094-5157",,,Embedding Attention and Residual Network for Accurate Salient Object Detection,,,,,,,,IEEE TRANSACTIONS ON CYBERNETICS,,,,50,5,,,2050,2062,,10.1109/TCYB.2018.2879859,,,,MAY 2020,2020,"Salient object detection is usually used as a preprocessing step to facilitate a variety of subsequent applications which should take little time cost. With the quick development of deep learning recently, profound progresses have been made to achieve a new state-of-the-art performance. However, the learned features of the existing deep learning-based methods are not accurate enough thus leading to unsatisfactory detection in complex scenes, such as low contrast or very similar between salient object and background region and multiple (small) salient objects with diverse characteristics. In addition, some post-processing techniques are usually needed for refinement, which is time consuming. To address these issues, this paper presents an efficient fully convolutional salient object detection network. Specifically, we first introduce a visual attention mechanism to guide feature learning in side output layers. In detail, attention weight is employed in a top-down manner which can bridge high level semantic information to help shallow layers better locate salient objects and also filter out noisy response in the background region. Second, we propose a residual refinement network to fuse the learned multilevel features gradually. Not to simply add or concatenate them step by step as previous works, we introduce a second-order term into element-wise addition to learn stage-wise residual features for refinement. Such a second-order term not only benefits efficient gradient propagation but also increases network nonlinearity. Extensive experiments on seven standard benchmarks demonstrate that the proposed approach achieves consistently superior performance and performs well on small salient object detection in comparison with the very recent state-of-the-arts, especially in the metric of structure-measure.",,,,,,,,,24,2,0,0,0,0,26,,,2168-2267,2168-2275,,WOS:000528622000024,30507520,
J,"Chai, Dengfeng; Newsam, Shawn; Huang, Jingfeng",,,,,"Newsam, Shawn/0000-0001-6803-5291",,,Aerial image semantic segmentation using DCNN predicted distance maps,,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,161,,,,309,322,,10.1016/j.isprsjprs.2020.01.023,,,,MAR 2020,2020,"This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them.",,,,,,,,,25,1,0,0,0,0,26,,,0924-2716,1872-8235,,WOS:000517849600024,,
J,"Muresan, Mircea Paul; Giosan, Ion; Nedevschi, Sergiu",,,,"Muresan, Mircea Paul/L-2356-2016; Nedevschi, Sergiu S/G-5262-2016; Giosan, Ion/AAO-7462-2021","Muresan, Mircea Paul/0000-0003-0315-3507; Nedevschi, Sergiu S/0000-0003-2018-4647; Giosan, Ion/0000-0003-2925-3194",,,Stabilization and Validation of 3D Object Position Using Multimodal Sensor Fusion and Semantic Segmentation,,,,,,,,SENSORS,,,,20,4,,,,,1110,10.3390/s20041110,,,,FEB 2020,2020,"The stabilization and validation process of the measured position of objects is an important step for high-level perception functions and for the correct processing of sensory data. The goal of this process is to detect and handle inconsistencies between different sensor measurements, which result from the perception system. The aggregation of the detections from different sensors consists in the combination of the sensorial data in one common reference frame for each identified object, leading to the creation of a super-sensor. The result of the data aggregation may end up with errors such as false detections, misplaced object cuboids or an incorrect number of objects in the scene. The stabilization and validation process is focused on mitigating these problems. The current paper proposes four contributions for solving the stabilization and validation task, for autonomous vehicles, using the following sensors: trifocal camera, fisheye camera, long-range RADAR (Radio detection and ranging), and 4-layer and 16-layer LIDARs (Light Detection and Ranging). We propose two original data association methods used in the sensor fusion and tracking processes. The first data association algorithm is created for tracking LIDAR objects and combines multiple appearance and motion features in order to exploit the available information for road objects. The second novel data association algorithm is designed for trifocal camera objects and has the objective of finding measurement correspondences to sensor fused objects such that the super-sensor data are enriched by adding the semantic class information. The implemented trifocal object association solution uses a novel polar association scheme combined with a decision tree to find the best hypothesis-measurement correlations. Another contribution we propose for stabilizing object position and unpredictable behavior of road objects, provided by multiple types of complementary sensors, is the use of a fusion approach based on the Unscented Kalman Filter and a single-layer perceptron. The last novel contribution is related to the validation of the 3D object position, which is solved using a fuzzy logic technique combined with a semantic segmentation image. The proposed algorithms have a real-time performance, achieving a cumulative running time of 90 ms, and have been evaluated using ground truth data extracted from a high-precision GPS (global positioning system) with 2 cm accuracy, obtaining an average error of 0.8 m.",,,,,,,,,25,0,0,0,0,1,26,,,,1424-8220,,WOS:000522448600161,32085608,
J,"Zhang, Xin; Han, Liangxiu; Han, Lianghao; Zhu, Liang",,,,,"han, lianghao/0000-0001-8672-1017; Zhang, Xin/0000-0001-7844-593X",,,How Well Do Deep Learning-Based Methods for Land Cover Classification and Object Detection Perform on High Resolution Remote Sensing Imagery?,,,,,,,,REMOTE SENSING,,,,12,3,,,,,417,10.3390/rs12030417,,,,FEB 2020,2020,"Land cover information plays an important role in mapping ecological and environmental changes in Earth's diverse landscapes for ecosystem monitoring. Remote sensing data have been widely used for the study of land cover, enabling efficient mapping of changes of the Earth surface from Space. Although the availability of high-resolution remote sensing imagery increases significantly every year, traditional land cover analysis approaches based on pixel and object levels are not optimal. Recent advancement in deep learning has achieved remarkable success on image recognition field and has shown potential in high spatial resolution remote sensing applications, including classification and object detection. In this paper, a comprehensive review on land cover classification and object detection approaches using high resolution imagery is provided. Through two case studies, we demonstrated the applications of the state-of-the-art deep learning models to high spatial resolution remote sensing data for land cover classification and object detection and evaluated their performances against traditional approaches. For a land cover classification task, the deep-learning-based methods provide an end-to-end solution by using both spatial and spectral information. They have shown better performance than the traditional pixel-based method, especially for the categories of different vegetation. For an objective detection task, the deep-learning-based object detection method achieved more than 98% accuracy in a large area; its high accuracy and efficiency could relieve the burden of the traditional, labour-intensive method. However, considering the diversity of remote sensing data, more training datasets are required in order to improve the generalisation and the robustness of deep learning-based models.",,,,,,,,,26,0,0,0,1,0,26,,,,2072-4292,,WOS:000515393800076,,
C,"Chiu, Mang Tik; Xu, Xingqian; Wei, Yunchao; Huang, Zilong; Schwing, Alexander G.; Brunner, Robert; Khachatrian, Hrant; Karapetyan, Hovnatan; Dozier, Ivan; Rose, Greg; Wilson, David; Tudor, Adrian; Hovakimyan, Naira; Huang, Thomas S.; Shi, Honghui",,,IEEE,"Huang, Zilong/AAW-6071-2021",,,,Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,2825,2835,,10.1109/CVPR42600.2020.00290,,,,2020,2020,"The success of deep learning in visual recognition tasks has driven advancements in multiple fields of research. Particularly, increasing attention has been drawn towards its application in agriculture. Nevertheless, while visual pattern recognition on farmlands carries enormous economic values, little progress has been made to merge computer vision and crop sciences due to the lack of suitable agricultural image datasets. Meanwhile, problems in agriculture also pose new challenges in computer vision. For example, semantic segmentation of aerial farmland images requires inference over extremely large-size images with extreme annotation sparsity. These challenges are not present in most of the common object datasets, and we show that they are more challenging than many other aerial image datasets. To encourage research in computer vision for agriculture, we present Agriculture-Vision: a large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. We collected 94, 986 high-quality aerial images from 3, 432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. We annotate nine types of field anomaly patterns that are most important to farmers. As a pilot study of aerial agricultural semantic segmentation, we perform comprehensive experiments using popular semantic segmentation models; we also propose an effective model designed for aerial agricultural pattern recognition. Our experiments demonstrate several challenges Agriculture-Vision poses to both the computer vision and agriculture communities. Future versions of this dataset will include even more aerial images, anomaly patterns and image channels.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,25,0,0,0,3,1,26,,,1063-6919,,978-1-7281-7168-5,WOS:000620679503009,,
B,"Cortinhal, T.; Tzelepis, G.; Erdal Aksoy, E.",,,,"Cortinhal, Tiago/AAL-3154-2021","Cortinhal, Tiago/0000-0002-8067-9521; Aksoy, Eren Erdal/0000-0002-5712-6777","Bebis, G.; Zhaozheng Yin; Kim, E.; Bender, J.; Subr, K.; Bum Chul Kwon; Jian Zhao; Kalkofen, D.; Baciu, G.",,"SalsaNext: Fast, Uncertainty-Aware Semantic Segmentation of LiDAR Point Clouds",,,,,,,,"Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510)",,,,,,,,207,22,,10.1007/978-3-030-64559-5_16,,,,2020,2020,"In this paper, we introduce SalsaNext for the uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet [1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we introduce a new context module, replace the ResNet encoder blocks with a new residual dilated convolution stack with gradually increasing receptive fields and add the pixel-shuffle layer in the decoder. Additionally, we switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross entropy loss with Lovasz-Softmax loss [4]. We finally inject a Bayesian treatment to compute the epistemic and aleatoric uncertainties for each point in the cloud. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext outperforms other published semantic segmentation networks and achieves 3.6% more accuracy over the previous state-of-the-art method. We also release our source code (https://github.com/TiagoCortinhal/SalsaNext).",,,,,"Advances in Visual Computing. 15th International Symposium, ISVC 2020Advances in Visual Computing. 15th International Symposium, ISVC 2020",5-7 Oct. 20205-7 Oct. 2020,,"San Diego, CA, USASan Diego, CA, USA",26,0,0,0,0,0,26,,,,,978-3-030-64558-8,INSPEC:20301990,,
C,"Jiang, Li; Zhao, Hengshuang; Shi, Shaoshuai; Liu, Shu; Fu, Chi-Wing; Jia, Jiaya",,,IEEE,"Shi, Shaoshuai/AAV-3211-2021",,,,PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation,,,,,,,,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,4866,4875,,10.1109/CVPR42600.2020.00492,,,,2020,2020,"Instance segmentation is an important task for scene understanding. Compared to the fully-developed 2D, 3D instance segmentation for point clouds have much room to improve. In this paper, we present PointGroup, a new end-to-end bottom-up architecture, specifically focused on better grouping the points by exploring the void space between objects. We design a two-branch network to extract point features and predict semantic labels and offsets, for shifting each point towards its respective instance centroid. A clustering component is followed to utilize both the original and offset-shifted point coordinate sets, taking advantage of their complementary strength. Further, we formulate the ScoreNet to evaluate the candidate instances, followed by the Non-Maximum Suppression (NMS) to remove duplicates. We conduct extensive experiments on two challenging datasets, ScanNet v2 and S3DIS, on which our method achieves the highest performance, 63.6% and 64.0%, compared to 54.9% and 54.4% achieved by former best solutions in terms of mAP with IoU threshold 0.5.",,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"JUN 14-19, 2020JUN 14-19, 2020",IEEE; CVFIEEE; CVF,ELECTR NETWORKELECTR NETWORK,24,3,0,0,0,0,26,,,1063-6919,,978-1-7281-7168-5,WOS:000620679505015,,
C,"Li, Xiangtai; Zhao, Houlong; Han, Lei; Tong, Yunhai; Tan, Shaohua; Yang, Kuiyuan",,,Assoc Advancement Artificial Intelligence,,,,,Gated Fully Fusion for Semantic Segmentation,,,,,,,,"THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE",,AAAI Conference on Artificial Intelligence,,34,,,,11418,11425,,,,,,2020,2020,"Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of high-level features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features. Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion(GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lowerlevel features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.",,,,,34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence,"FEB 07-12, 2020FEB 07-12, 2020",Assoc Advancement Artificial IntelligenceAssoc Advancement Artificial Intelligence,"New York, NYNew York, NY",26,0,0,0,0,0,26,,,2159-5399,2374-3468,978-1-57735-835-0,WOS:000668126803107,,
