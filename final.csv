Article Title,"Times Cited, All Databases",Publication Time,Abstract,DOI
MultiResUNet：重新思考用于多模态生物医学图像分割的 U-Net 架构,369,2020 JAN 2020,近年来，深度学习在医学图像分割方面取得了突破。在这方面，U-Net 一直是医学影像界最流行的架构。尽管在分割多模态医学图像方面具有出色的整体性能，但通过对一些具有挑战性的数据集的广泛实验，我们证明了经典的 U-Net 架构似乎在某些方面缺乏。因此，我们提出了一些修改，以改进已经最先进的 U-Net 模型。在这些修改之后，我们开发了一种新颖的架构 MultiResUNet，作为 U-Net 架构的潜在继任者。我们已经在大量多模态医学图像上测试并比较了 MultiResUNet 与经典 U-Net。尽管在理想图像的情况下只注意到了轻微的改进，但对于具有挑战性的图像，性能却取得了显着的进步。我们在五个不同的数据集上评估了我们的模型，每个数据集都有自己独特的挑战，并分别获得了 10.15%、5.07%、2.63%、1.41% 和 0.62% 的性能相对提升。我们还讨论并强调了 MultiResUNet 在质量上优于经典 U-Net 的一些方面，这些方面并未真正反映在定量测量中。 (C) 2019 Elsevier Ltd. 保留所有权利。,10.1016/j.neunet.2019.08.025
用于视觉识别的深度高分辨率表示学习,267,2021 OCT 1 2021,高分辨率表示对于位置敏感的视觉问题至关重要，例如人体姿态估计、语义分割和对象检测。现有的最先进的框架首先通过一个子网络将输入图像编码为低分辨率表示，该子网络通过串联连接高分辨率到低分辨率的卷积（例如，ResNet、VGGNet）形成，然后恢复高分辨率- 来自编码的低分辨率表示的分辨率表示。相反，我们提出的网络，称为高分辨率网络（HRNet），在整个过程中保持高分辨率表示。有两个关键特征：（i）并行连接从高分辨率到低分辨率的卷积流，以及（ii）跨分辨率重复交换信息。好处是生成的表示在语义上更丰富，在空间上更精确。我们展示了所提出的 HRNet 在广泛的应用中的优越性，包括人体姿态估计、语义分割和对象检测，这表明 HRNet 是计算机视觉问题的更强大的主干。所有代码都可以在 https://github.com/HRNet 上找到。,10.1109/TPAMI.2020.2983686
ResUNet-a：遥感数据语义分割的深度学习框架,162,2020 APR 2020,高分辨率航空影像的场景理解对于各种遥感应用中的自动监测任务具有重要意义。由于感兴趣对象的像素值的类内和类间差异较大，这仍然是一项具有挑战性的任务。近年来，深度卷积神经网络已开始用于遥感应用，并展示了对象像素级分类的最先进性能。在这里，我们为单时间极高分辨率航空图像的语义分割任务提出了一个可靠的高性能结果框架。我们的框架由一个新的深度学习架构 ResUNet-a 和一个基于 Dice 损失的新损失函数组成。 ResUNet-a 使用 UNet 编码器/解码器主干，结合残差连接、atrous 卷积、金字塔场景解析池和多任务推理。 ResUNet-a 依次推断对象的边界、分割掩码的距离变换、分割掩码和输入的彩色重建。每个任务都以先前任务的推理为条件，从而在各种任务之间建立条件关系，正如通过架构的计算图描述的那样。我们分析了几种用于语义分割的广义骰子损失的性能，并为对象的语义分割引入了一种新的变体损失函数，该函数具有出色的收敛性，即使在存在高度不平衡的类的情况下也表现良好。我们的建模框架的性能在 ISPRS 2D Potsdam 数据集上进行评估。结果显示，我们的最佳模型在所有类别中的平均 F1 分数为 92.9%，具有最先进的性能。,10.1016/j.isprsjprs.2020.01.013
用于路面裂缝检测的特征金字塔和分层增强网络,134,2020 APR 2020,路面裂缝检测是确保道路安全的一项关键任务。手动裂纹检测非常耗时。因此，需要一种自动道路裂缝检测方法来推动这一进展。然而，由于裂缝的强度不均匀性和背景的复杂性，它仍然是一项具有挑战性的任务，例如，与周围路面的低对比度和可能具有相似强度的阴影。受计算机视觉深度学习最新进展的启发，我们提出了一种新的网络架构，称为特征金字塔和分层增强网络（FPHBN），用于路面裂缝检测。所提出的网络以特征金字塔的方式将上下文信息集成到用于裂缝检测的低级特征中，并通过在训练期间以分层方式嵌套样本重新加权来平衡简单样本和困难样本对损失的贡献。此外，我们提出了一种新的裂纹检测测量方法，称为平均交叉联合（AIU）。为了证明所提出方法的优越性和普遍性，我们在五个裂缝数据集上对其进行了评估，并将其与最先进的裂缝检测、边缘检测和语义分割方法进行了比较。大量实验表明，所提出的方法在准确性和普遍性方面优于这些方法。代码和数据可以在 https://github.com/fyangneil/pavement-crack-detection 中找到。,10.1109/TITS.2019.2910595
语义分割的对象-上下文表示,121,2020 2020,在本文中，我们研究了语义分割中的上下文聚合问题。由于像素的标签是像素所属对象的类别，我们提出了一种简单而有效的方法，即对象上下文表示，通过利用相应对象类别的表示来表征像素。首先，我们在真实分割的监督下学习对象区域。其次，我们通过聚合位于对象区域中的像素的表示来计算对象区域表示。最后，我们计算每个像素和每个对象区域之间的关系，并使用对象上下文表示增强每个像素的表示，该表示是所有对象区域表示的加权聚合。我们凭经验证明我们的方法在各种基准测试中实现了具有竞争力的性能：Cityscapes、ADE20K、LIP、PASCAL-Context 和 COCO-Stuff。我们的提交 ldquoHRNet + OCR + SegFixrdquo 在 ECCV 2020 提交截止日期前在 Cityscapes 排行榜上获得第一名。代码位于：https://git.io/openseg 和 https://git.io/HRNet.OCR。,10.1007/978-3-030-58539-6_11
SOLO：按位置分割对象,98,2020 2020,我们提出了一种新的、非常简单的实例分割方法。与许多其他密集预测任务（例如语义分割）相比，任意数量的实例使实例分割更具挑战性。为了预测每个实例的掩码，主流方法要么遵循 ldquodetect-then-segmentrdquo 策略（例如，Mask R-CNN），要么先预测嵌入向量，然后使用聚类技术将像素分组到单个实例中。我们通过引入 ldquoinstance categoriesrdquo 的概念，从全新的角度看待实例分割的任务，它根据实例的位置和大小为实例中的每个像素分配类别，从而很好地将实例分割转换为单次分类可解决的问题。我们展示了一个更简单灵活的实例分割框架，具有强大的性能，与 Mask R-CNN 实现了同等的准确性，并且在准确性上优于最近的单次实例分割器。我们希望这个简单而强大的框架可以作为除实例分割之外的许多实例级识别任务的基准。代码可在 https://git.io/AdelaiDet 获得。,10.1007/978-3-030-58523-5_38
用于高分辨率双时相遥感图像变化检测的深度监督图像融合网络,88,2020 AUG 2020,高分辨率遥感图像中的变化检测对于了解地表变化至关重要。由于传统的变化检测方法不适合考虑高分辨率图像中传达的精细图像细节和复杂纹理特征带来的挑战，因此提出了许多基于深度学习的变化检测方法来提高变化检测性能。尽管最先进的基于深度特征的方法优于所有其他基于深度学习的变化检测方法，但现有的基于深度特征的方法中的网络大多是从最初为单图像语义分割提出的架构修改而来的。转移这些网络进行变化检测任务仍然存在一些关键问题。在本文中，我们提出了一种深度监督图像融合网络（IFN），用于高分辨率双时相遥感图像的变化检测。具体来说，首先通过完全卷积的双流架构提取具有高度代表性的双时图像深度特征。然后，将提取的深度特征输入深度监督的差异判别网络（DDN）进行变化检测。为了提高输出变化图中对象的边界完整性和内部紧凑性，通过注意力模块将原始图像的多层深度特征与图像差异特征融合，用于变化图重建。通过直接向网络中的中间层引入变化图损失来进一步增强 DDN，并以端到端的方式训练整个网络。干扰素应用于一个公开可用的数据集，以及一个具有挑战性的数据集，该数据集由来自谷歌地球的多源双时相图像组成，覆盖中国不同城市。视觉解释和定量评估都证实，与最先进的方法相比，IFN 通过返回具有完整边界和高内部紧凑性的变化区域，优于文献中的四种基准方法。,10.1016/j.isprsjprs.2020.06.003
使用深度神经网络从连续驾驶场景中进行稳健的车道检测,80,2020 JAN 2020,驾驶场景中的车道检测是自动驾驶汽车和高级驾驶辅助系统的重要模块。近年来，已经提出了许多复杂的车道检测方法。然而，大多数方法都侧重于从一张图像中检测车道，并且在处理一些极其恶劣的情况（如阴影重、标记退化严重、车辆严重遮挡等）时往往会导致性能不理想。事实上，车道是道路上的连续线结构。因此，在当前帧中无法准确检测到的车道可能会通过合并先前帧的信息来推断出来。为此，我们通过使用连续驾驶场景的多帧来研究车道检测，并通过结合卷积神经网络（CNN）和循环神经网络（RNN）提出了一种混合深度架构。具体来说，每一帧的信息被一个CNN块抽象出来，然后将多个连续帧的CNN特征，具有时间序列的特性，输入RNN块进行特征学习和车道预测。在两个大规模数据集上进行的大量实验表明，所提出的方法在车道检测方面优于其他竞争方法，尤其是在处理困难情况方面。,10.1109/TVT.2019.2949603
用于遥感图像中目标检测的旋转感知和多尺度卷积神经网络,75,2020 MAR 2020,目标检测在遥感影像分析领域发挥着重要作用。推进这项任务最具挑战性的问题是物体尺度的巨大变化和物体的任意方向。在本文中，我们在基于区域的卷积神经网络上构建了一个统一的框架，用于遥感图像中的任意方向和多尺度目标检测。为了处理多尺度目标检测问题，提出了一种特征融合架构来生成多尺度特征层次结构，该架构通过自顶向下的路径将浅层的特征与语义表示相结合，并结合顶层的特征图。通过自下而上的路径具有低级信息的层。通过组合不同层次的特征，我们可以为多尺度对象形成强大的特征表示。大多数以前的方法通过轴对齐的框定位具有任意方向和密集空间分布的对象，这些框可能覆盖相邻的实例和背景区域。我们构建了一个旋转感知对象检测器，它使用定向框来定位遥感图像中的对象。区域提议网络使用多个默认角度增强锚点以覆盖定向对象。它利用定向提议框来包围对象，而不是粗略定位定向对象的水平提议。引入定向 RoI 池化操作以提取定向提议的特征图，用于以下 R-CNN 子网络。我们在公共数据集上进行了综合实验，用于遥感图像中的定向目标检测。我们的方法实现了最先进的性能，这证明了所提出方法的有效性。,10.1016/j.isprsjprs.2020.01.025
SDDNet：实时裂缝分割,71,2020 SEP 2020,本文报告了一种用于分割图像中混凝土裂缝的纯深度学习方法的发展。目标是实现实时性能，同时有效地消除各种复杂背景和裂纹状特征。为了实现这些目标，提出了一种原始的卷积神经网络。该模型由标准卷积、密集连接的可分离卷积模块、改进的空洞空间金字塔池化模块和解码器模块组成。语义损伤检测网络 (SDDNet) 在手动创建的裂纹数据集上进行训练，训练后的网络在测试集上记录了 0.846 的平均交并比。分析每个测试图像，并呈现具有代表性的分割结果。结果表明，除非特征太微弱，否则 SDDNet 片段会有效地破裂。所提出的模型还与最近的模型进行了比较，结果表明它返回了更好的评估指标，尽管它的参数数量是比较模型的 88 倍。此外，该模型处理 1025 x 512 像素的实时 (36 FPS) 图像，比最近的工作快 46 倍。,10.1109/TIE.2019.2945265
基于全卷积网络的航空图像道路提取集成方法,67,2020 OCT 2020,这封信提出了一种基于全卷积网络（FCNs）和集成策略的道路提取方法，以解决航拍图像中道路和背景区域的不平衡问题。通过利用 FCN，我们将道路提取视为语义分割问题。在网络中，由于道路和背景的不平衡，损失函数的权重被修改，如果道路被错误地分类为背景，将会有更大的惩罚。由于很难确定给定图像的损失函数的适当权重，因此提出了一种基于空间一致性（SC）的集成方法。从具有不同损失函数的 FCN 获得的结果图融合在我们提出的集成策略中，这也避免了权重的确定。我们的方法使用马萨诸塞州道路数据集进行了测试，根据我们的实验结果，与基础全卷积模型相比，它被证明是有效的。,10.1109/LGRS.2019.2953523
通过自我监督的无监督域内自适应语义分割,65,2020 2020,基于卷积神经网络的方法在语义分割方面取得了显着进展。然而，这些方法严重依赖于劳动密集型的注释数据。为了应对这一限制，从图形引擎生成的自动注释数据用于训练分割模型。然而，从合成数据训练的模型很难转移到真实图像上。为了解决这个问题，以前的工作已经考虑将模型从源数据直接调整到未标记的目标数据（以减少域间差距）。尽管如此，这些技术并未考虑目标数据本身之间的大分布差距（域内差距）。在这项工作中，我们提出了一种两步自监督域自适应方法，以最小化域间和域内的差距。首先，我们对模型进行域间自适应，从这种自适应中，我们使用基于熵的排序函数将目标域分为简单和硬分割。最后，为了减少域内差距，我们建议采用从易到难子域的自我监督适应技术。许多基准数据集的实验结果突出了我们的方法对现有最先进方法的有效性。源代码可在 https://github.com/feipan664/IntraDA.git 获得。,10.1109/CVPR42600.2020.00382
CGNet：用于语义分割的轻量级上下文引导网络,60,2021 2021,在移动设备上应用语义分割模型的需求一直在快速增长。当前最先进的网络具有大量参数，因此不适合移动设备，而其他小内存占用模型遵循分类网络的精神，忽略了语义分割的固有特征。为了解决这个问题，我们提出了一种新颖的上下文引导网络（CGNet），它是一种用于语义分割的轻量级高效网络。我们首先提出了上下文引导（CG）块，它有效地学习了局部特征和周围上下文的联合特征，并进一步改进了与全局上下文的联合特征。基于 CG 块，我们开发了 CGNet，它在网络的所有阶段捕获上下文信息。 CGNet 是专门为利用语义分割的固有属性和提高分割精度而量身定制的。此外，CGNet 经过精心设计，可减少参数数量并节省内存占用。在相同数量的参数下，所提出的 CGNet 显着优于现有的轻量级分割网络。在 Cityscapes 和 CamVid 数据集上的大量实验验证了所提出方法的有效性。具体来说，在没有任何后处理和多尺度测试的情况下，所提出的 CGNet 在 Cityscapes 上实现了 64.8% 的平均 IoU，且参数少于 0.5 M。,10.1109/TIP.2020.3042065
ASIF-Net：用于 RGB-D 显着目标检测的注意力转向交织融合网络,57,2021 JAN 2021,从 RGB-D 图像中检测显着目标是一项重要但具有挑战性的视觉任务，旨在通过结合颜色信息和深度约束来检测场景中最独特的目标。与先前的融合方式不同，我们提出了一种注意力转向交织融合网络（ASIF-Net）来检测显着对象，该网络通过注意力机制的转向逐步整合了来自 RGB 图像和相应深度图的跨模式和跨级别互补性。具体来说，RGB-D 图像的互补特征被联合提取并以密集和交织的方式分层融合。这种方式打破了跨模态数据存在的不一致障碍，也充分捕捉到了互补性。同时，引入了一种注意力机制，以注意力加权的方式定位潜在的显着区域，在突出显着对象和抑制杂乱的背景区域方面取得了进步。我们不仅关注像素显着性，还通过结合为 RGB-D 显着性对象检测提供全局语义约束的对抗性学习来确保检测到的显着性对象具有对象性特征（例如，完整的结构和清晰的边界）。定量和定性实验表明，所提出的方法在四个公开可用的 RGB-D 显着对象检测数据集上优于 17 个最先进的显着性检测器。我们方法的代码和结果可在 https://github.com/Li-Chongyi/ASIF-Net 获得。,10.1109/TCYB.2020.2969255
基于双向词向量的像素级遥感图像识别,56,2020 FEB 2020,在传统的遥感图像识别中，传统的特征（如颜色特征和纹理特征）不能充分描述复杂的图像，图像像素之间的关系也不能很好的捕捉。使用单个模型或传统的顺序联合模型，在特征挖掘过程中很容易丢失深层特征。本文提出了一种新的特征提取方法，该方法利用自然语言处理中的词嵌入方法生成双向真实密集向量，以反映像素之间的上下文关系。将双向独立循环神经网络（BiIndRNN）与卷积神经网络（CNN）相结合，改进切片循环神经网络（SRNN）算法模型，然后在注意力机制下与图卷积网络（GCN）并行构建充分利用图像的深层特征并捕获上下文的语义信息。该模型统称为改进的 SRNN 和基于注意力处理的基于 GCN 的并行 (SAGP) 模型。在胡杨林上进行的实验表明，所提出的方法在识别准确率方面优于传统方法。在公共数据集上进行的验证也证明了这一点。,10.1109/TGRS.2019.2945591
使用 Transformer 从序列到序列的角度重新思考语义分割,50,2021 2021,最近的语义分割方法采用具有编码器-解码器架构的全卷积网络 (FCN)。编码器逐渐降低空间分辨率并学习更多具有更大感受野的抽象/语义视觉概念。由于上下文建模对于分割至关重要，因此最新的努力集中在通过扩张/空洞卷积或插入注意力模块来增加感受野。然而，基于编码器-解码器的 FCN 架构保持不变。在本文中，我们旨在通过将语义分割视为序列到序列的预测任务来提供另一种视角。具体来说，我们部署了一个纯转换器（即没有卷积和分辨率降低）来将图像编码为一系列补丁。借助在转换器的每一层中建模的全局上下文，该编码器可以与简单的解码器相结合，以提供强大的分割模型，称为 SEgmentation TRansformer (SETR)。大量实验表明，SETR 在 ADE20K (50.28% mIoU)、Pascal Context (55.83% mIoU) 和 Cityscapes 上取得了竞争性结果。特别是在提交当天，我们在竞争激烈的 ADE2OK 测试服务器排行榜中获得了第一名。,10.1109/CVPR46437.2021.00681
基于特征融合模型改进语义图像分割的研究,50,,由于特征的低分辨率以及最大池化层和下采样层的重复组合，图像的上下文信息已经丢失。当使用卷积网络执行特征提取过程时，语义图像分割的结果对对象的位置失去了敏感性。提出了基于逐层上下文特征的特征融合模型的语义图像分割。首先，对原始图像进行高斯核函数预处理，生成一系列不同分辨率的图像，形成图像金字塔。其次，将图像金字塔输入到网络结构中，将多个全卷积网络并行组合，通过使用Atrous Convolutions扩展感受野得到一组不同粒度的初始特征，并进行不同层的特征融合初始化- 自上而下方法中的逐层粒度。最后，计算出特征融合模型的得分图并发送给条件随机场，利用全连接条件随机场对原图像的图像像素之间的类相关性以及图像的空间位置信息和颜色矢量信息进行建模。像素被连接以优化并获得结果。在 PASCAL VOC 2012 和 PASCAL Context 数据集上的实验比最先进的作品取得了更好的平均交叉联合。所提出的方法比传统方法改进了约 6.3%。,10.1007/s12652-020-02066-z
Mask-Refined R-CNN：用于在实例分割中细化对象细节的网络,50,2020 FEB 2020,随着柔性视觉传感器和视觉传感器网络的快速发展，目标检测和跟踪等计算机视觉任务正在进入一个新阶段。因此，包括实例分割在内的更具挑战性的综合任务可以迅速发展。大多数最先进的网络框架，例如分割，都是基于 Mask R-CNN（掩模区域卷积神经网络）。然而，实验结果证实，Mask R-CNN 并不总能成功预测实例细节。 Mask R-CNN 的尺度不变全卷积网络结构忽略了不同大小的感受野之间的空间信息差异。大规模感受野更关注细节信息，而小感受野更关注语义信息。所以网络不能考虑物体边缘像素之间的关系，这些像素就会被错误分类。为了克服这个问题，提出了Mask-Refined R-CNN（MR R-CNN），其中对ROIAlign（region of interest align）的stride进行了调整。此外，将原来的全卷积层替换为新的语义分割层，通过构建特征金字塔网络，将相同分辨率的特征图的前向和后向传输相加，实现特征融合。通过结合关注全局和详细信息的特征层，可以显着提高分割精度。在 COCO（Common Objects in Context）和 Cityscapes 数据集上的实验结果表明，MR R-CNN 的分割精度比使用相同主干的 Mask R-CNN 高 2% 左右。大实例的平均精度达到 56.6%，高于所有最先进的方法。此外，所提出的方法所需的时间成本低且易于实施。在 Cityscapes 数据集上的实验也证明了所提出的方法具有很强的泛化能力。,10.3390/s20041010
基于全卷积网络的多标签遥感图像检索,48,2020 2020,传统的遥感图像检索 (RSIR) 系统通常执行单标签检索，其中每个图像都由表示图像最重要语义内容的单个标签进行注释。然而，在这种情况下，遥感图像的场景复杂性被忽略了，其中图像可能有多个类别（即多个标签），导致检索性能不佳。因此，我们提出了一种基于全卷积网络 (FCN) 的新型多标签 RSIR 方法。具体来说，首先训练 FCN 以预测所考虑的图像存档中每个图像的分割图。然后我们获得多标签向量并根据其分割图提取每个图像的区域卷积特征。提取的区域特征最终用于执行基于区域的多标签检索。实验结果表明，与手工和卷积神经网络特征相比，我们的方法实现了最先进的性能。,10.1109/JSTARS.2019.2961634
场景分割的上下文先验,46,2020 2020,最近的工作广泛探索了上下文依赖关系，以实现更准确的分割结果。然而，大多数方法很少区分不同类型的上下文依赖关系，这可能会污染场景理解。在这项工作中，我们直接监督特征聚合以清楚地区分类内和类间上下文。具体来说，我们在 Affinity Loss 的监督下开发了一个 Context Prior。给定输入图像和相应的基本事实，Affinity Loss 构建一个理想的亲和图来监督 Context Prior 的学习。学习的上下文先验提取属于同一类别的像素，而反向先验则关注不同类别的像素。嵌入到传统的深度 CNN 中，所提出的上下文先验层可以选择性地捕获类内和类间的上下文依赖关系，从而实现鲁棒的特征表示。为了验证有效性，我们设计了一个有效的上下文先验网络（CPNet）。广泛的定量和定性评估表明，所提出的模型优于最先进的语义分割方法。更具体地说，我们的算法在 ADE20K 上达到 46.3% mIoU，在 PASCAL-Context 上达到 53.9% mIoU，在 Cityscapes 上达到 81.3% mIoU。代码可在 https://git.io/ContextPrior 获得。,10.1109/CVPR42600.2020.01243
具有双重关系感知注意网络的场景分割,45,2021 JUN 2021,在本文中，我们提出了一个双关系感知注意网络 (DRANet) 来处理场景分割任务。如何有效地利用上下文对于像素级识别至关重要。为了解决这个问题，我们基于关系感知注意机制自适应地捕获上下文信息。特别是，我们在扩张的全卷积网络（FCN）的顶部附加了两种类型的注意力模块，它们分别对空间和通道维度的上下文依赖关系进行建模。在注意力模块中，我们采用自注意力机制来模拟任意两个像素或通道之间的语义关联。每个像素或通道可以根据它们的相关性自适应地聚合来自所有像素或通道的上下文。为了降低上述成对关联计算带来的高计算和内存成本，我们进一步设计了两种紧凑的注意力模块。在紧凑的注意力模块中，每个像素或通道仅与少数几个聚集中心建立关联，并在这些聚集中心上获得相应的上下文聚合。同时，我们添加了一个跨级门控解码器，以选择性地增强空间细节，从而提高网络的性能。我们进行了广泛的实验来验证我们网络的有效性，并在四个具有挑战性的场景分割数据集（即 Cityscapes、ADE20K、PASCAL Context 和 COCO Stuff 数据集）上实现新的最先进的分割性能。特别是，在 Cityscapes 测试集上的平均 IoU 得分为 82.9%，而无需使用额外粗略的注释数据。,10.1109/TNNLS.2020.3006524
关系问题：关系上下文感知全卷积网络用于高分辨率航空图像的语义分割,45,2020 NOV 2020,当前大多数语义分割方法都依赖于深度卷积神经网络（CNN）。然而，他们使用带有局部感受野的卷积运算导致建模上下文空间关系的失败。先前的工作试图通过在网络中使用图形模型或空间传播模块来解决这个问题。但此类模型通常无法捕捉实体之间的远程空间关系，从而导致空间碎片化的预测。此外，最近的工作表明，通道信息在 CNN 中也起着关键作用。在本文中，我们介绍了两个简单而有效的网络单元，空间关系模块和通道关系模块来学习和推理任意两个空间位置或特征图之间的全局关系，然后生成关系增强 (RA) 特征表示.空间和通道关系模块具有通用性和可扩展性，可以与现有的完全卷积网络（FCN）框架以即插即用的方式使用。我们使用两个航拍图像数据集，即国际摄影测量与遥感学会 (ISPRS) Vaihingen 和 Potsdam 数据集，在语义分割任务上评估配备关系模块的网络，它们从根本上依赖于远程空间关系推理。这些网络取得了非常有竞争力的结果，在 Vaihingen 数据集上的平均 F1 得分为 88.54%，在 Potsdam 数据集上的平均 F1 得分为 88.01%，与基线相比有了显着的改进。,10.1109/TGRS.2020.2979552
CCNet：语义分割的交叉注意力。,44,2020 2020-Jul-03,上下文信息在视觉理解问题中至关重要，例如语义分割和对象检测。我们提出了一个纵横交错网络（CCNet），用于以非常有效和高效的方式获取全图像上下文信息。具体来说，对于每个像素，一个新的交叉注意力模块会收集其交叉路径上所有像素的上下文信息。通过进一步的循环操作，每个像素最终都可以捕获全图像的依赖关系。此外，提出了一个类别一致的损失来强制交叉注意力模块产生更多的判别特征。总体而言，CCNet 具有以下优点：1）GPU 内存友好。与非局部块相比，所提出的循环交叉注意力模块需要的 GPU 内存使用量减少 11 倍。 2）计算效率高。反复出现的交叉注意力显着减少了非局部块的 85% 左右的 FLOP。 3) 最先进的性能。我们对语义分割基准进行了广泛的实验，包括 Cityscapes、ADE20K、人类解析基准 LIP、实例分割基准 COCO、视频分割基准 CamVid。特别是，我们的 CCNet 在 Cityscapes 测试集、ADE20K 验证集和 LIP 验证集上分别取得了 81.9%、45.76% 和 55.47% 的 mIoU 分数，这是新的最先进的结果。源代码可在 https://github.com/speedinghzl/CCNet 获得。,10.1109/TPAMI.2020.3007032
边缘深度学习：使用卷积神经网络从卫星图像中提取场边界,43,2020 AUG 2020,数字农业服务的应用通常需要农民或其顾问提供其田间边界的数字记录。从卫星图像中自动提取田界边界将减少对这些记录的人工输入的依赖，因为人工输入是耗时的，并且将支持远程产品和服务的提供。当前字段边界数据集的缺乏似乎表明现有方法的采用率较低，这可能是因为昂贵的图像预处理要求和局部的（通常是任意的）调整。在本文中，我们提出了一种数据驱动、稳健且通用的方法，以促进从卫星图像中提取场边界。我们将此任务表述为多任务语义分割问题。我们使用了 ResUNet-a，这是一种具有完全连接的 UNet 主干的深度卷积神经网络，具有扩张卷积和条件推理的特点，以识别：1）场的范围； 2) 领域边界； 3) 到最近边界的距离。通过要求算法重构三个相关的输出，模型的性能和泛化能力大大提高。然后通过对三个模型输出进行后处理（例如，通过阈值处理或分水岭分割）来实现各个字段的分割。使用来自 Sentinel-2 的单个月度合成图像作为输入，我们的模型在映射字段范围、字段边界以及因此单个字段方面非常准确。用接近合成周期的单日期图像替换每月合成图像会略微降低准确性。然后，我们在一系列实验中表明，无需重新校准，相同的模型在分辨率（10 m 到 30 m）、传感器（Sentinel-2 到 Landsat-8）、空间和时间上都能很好地推广。通过平均从整个季节获得的至少四张图像的模型预测来建立共识对于减少准确性的时间变化至关重要。我们的卷积神经网络能够从图像中学习复杂的分层上下文特征，以准确检测字段边界并丢弃不相关的边界，从而优于传统的边缘滤波器。通过最小化过度拟合和图像预处理要求，并通过用数据驱动的决策替换局部任意决策，我们的方法有望促进大规模提取单个农田。,10.1016/j.rse.2020.111741
遥感图像语义分割的多尺度上下文聚合,42,2020 FEB 2020,遥感图像（RSI）的语义分割在各种应用中都很重要。传统的基于编码器-解码器的卷积神经网络 (CNN) 使用级联池化操作来聚合语义信息，这会导致定位精度的损失和空间细节的保留。为了克服这些限制，我们引入了使用高分辨率网络（HRNet）在没有解码阶段的情况下产生高分辨率特征。此外，我们分别增强了从不同分支中提取的从低到高的特征，以加强与尺度相关的上下文信息的嵌入。低分辨率特征包含更多的语义信息，空间尺寸较小；因此，它们被用来模拟长期空间相关性。通过引入自适应空间池 (ASP) 模块来聚合更多本地上下文，可以增强高分辨率分支。通过在不同层次上结合这些上下文聚合设计，最终的架构能够在全局和局部层次上利用空间上下文。在两个 RSI 数据集上获得的实验结果表明，我们的方法显着提高了常用 CNN 的准确性，并实现了最先进的性能。,10.3390/rs12040701
基于改进Faster-RCNN模型的目标识别多尺度目标语义分割,41,2021 OCT 2021,图像语义分割在计算机视觉中受到了极大的关注，其目的是对不同的对象进行分割，并为其提供不同的语义类别标签，使计算机能够充分获取场景的语义信息。然而，目前的研究主要集中在彩色图像数据作为训练，用于户外场景和单任务语义分割。本文基于改进的 Faster-RCNN 算法，利用 RGB-D 图像信息，在复杂的室内环境中进行多任务语义分割模型联合目标检测，可同时实现室内场景语义分割、目标分类和检测多视觉任务。其中，针对环境中光照不均的影响，改进了RGB图像与深度图像融合的方法。在增强融合图像特征信息的同时，也提高了模型训练的效率。同时，为了满足对多尺度目标对象进行操作的需要，对非极大值抑制算法进行了改进，以提高模型的性能。为了实现模型多任务信息的输出，损失函数也进行了重新设计和优化。本文构建的室内场景语义分割模型不仅性能好、效率高，而且能够清晰地分割不同尺度物体的轮廓，适应室内光照不均匀的环境。 (C) 2021 年 Elsevier BV 出版,10.1016/j.future.2021.04.019
具有上下文编码和多路径解码的语义分割,40,2020 2020,语义图像分割旨在将场景图像的每个像素分类为多个类别之一。它隐含地涉及对象识别、定位和边界描绘。在本文中，我们提出了一种称为 CGBNet 的分割网络，通过上下文编码和多路径解码来提高分割性能。我们首先提出了一个上下文编码模块，该模块生成上下文对比的局部特征，以利用信息上下文和有区别的局部信息。这个上下文编码模块极大地提高了分割性能，特别是对于不显眼的对象。此外，我们提出了一种尺度选择方案，可以选择性地融合每个空间位置不同尺度特征的分割结果。它自适应地从丰富的特征尺度中选择合适的分数图。为了提高边界处的分割性能结果，我们进一步提出了一个边界描绘模块，该模块鼓励边界附近的特定位置的非常低级特征参与最终预测，并在远离边界的地方抑制它们。所提出的分割网络在六个流行的场景分割数据集、Pascal Context、SUN-RGBD、Sift Flow、COCO Stuff、ADE20K 和 Cityscapes 上，在所有三个不同的评估指标方面都取得了非常有竞争力的性能。,10.1109/TIP.2019.2962685
基于具有注意块和多个损失的 U-Net 的建筑物提取,39,2020 MAY 2020,高分辨率遥感图像的语义分割在建筑物提取的应用中起着重要作用。然而，目前的算法存在一些语义信息提取的局限性，这些都会导致分割结果不佳。为了以高精度提取建筑物，我们提出了一种基于注意力的多损失神经网络。所设计的网络，基于 U-Net，可以通过注意力块提高模型的敏感性，抑制不相关特征区域的背景影响。为了提高模型的能力，在训练网络时提出了一种多损失方法。实验结果表明，所提出的模型比其他最先进的方法有了很大的改进。对于公开的 Inria Aerial Image Labeling 数据集，F1 得分达到 76.96%，在 Aerial Imagery for Roof Segmentation 数据集上表现出良好的性能。,10.3390/rs12091400
